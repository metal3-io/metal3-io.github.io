<!doctype html>
<html class="no-js" lang="en">

<head>
    <script id="dpal" src="//www.redhat.com/ma/dpal.js" type="text/javascript"></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="/assets/css/styles.css">
    <link rel="shortcut icon" type="image/png" href="/favicon.png">
    <meta name="theme-color" content="#008585">
    
    <title>Metal³ - Metal Kubed</title>
    <!-- # Opengraph protocol properties: https://ogp.me/ -->
    <meta name="author" content="The Metal³ - Metal Kubed website team, " >
    
    <meta name="twitter:card" content="summary">
    <meta name="description" content="Metal3.io aims to build on baremetal host provisioning technologies to provide a Kubernetes native API for managing bare metal hosts via a provisioning stack that is also running on Kubernetes.">
    <meta name="keywords" content="hybrid, cloud, metal3, baremetal, stack, edge, openstack, ironic, openshift, kubernetes, openstack, operator, summit, kubecon, shiftdev, metal3-dev-env, documentation, development, talk, conference, meetup, cluster-api, provider, raw-image, image-streaming, ipam, ip-address-manager, pivoting, move, scaling, cncf, community, announcement, ipa, oci, deployment, bare-metal, " >
    <meta property="og:title" content="Metal³ - Metal Kubed">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://metal3.io/search.html" >
    <meta property="og:image" content="https://metal3.io/assets/images/metal3logo.png">
    <meta property="og:description" content="Metal3.io aims to build on baremetal host provisioning technologies to provide a Kubernetes native API for managing bare metal hosts via a provisioning stack that is also running on Kubernetes." >
    <meta property="og:site_name" content="Metal³ - Metal Kubed" >
    <meta property="og:article:author" content="The Metal³ - Metal Kubed website team" >
    <meta property="og:article:published_time" content="2026-02-27 18:29:37 -0600" >
    <meta name="twitter:title" content="Metal³ - Metal Kubed">
    <meta name="twitter:description" content="Metal3.io aims to build on baremetal host provisioning technologies to provide a Kubernetes native API for managing bare metal hosts via a provisioning stack that is also running on Kubernetes.">

    <link type="application/atom+xml" rel="alternate" href="https://metal3.io/feed.xml" title="Metal³ - Metal Kubed" />
    <meta name="google-site-verification" content="HCdbGknTOCTKQVt7m-VxTG4BEYXxSqm-sDb-iklqrB0" />
  <link href="https://fonts.googleapis.com/css?family=Nunito:200,400&display=swap" rel="stylesheet">
  <script defer src="https://use.fontawesome.com/releases/v5.1.0/js/all.js" integrity="sha384-3LK/3kTpDE/Pkp8gTNp2gR/2gOiwQ6QaO7Td0zV76UFJVhqLl4Vl3KL1We6q6wR9" crossorigin="anonymous"></script>
  <!-- Photoswipe.com gallery-->

  <!-- Core CSS file -->
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe.min.css">

  <!-- Skin CSS file (styling of UI - buttons, caption, etc.)
      In the folder of skin CSS file there are also:
      - .png and .svg icons sprite,
      - preloader.gif (for browsers that do not support CSS animations) -->
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/default-skin/default-skin.min.css">
</head>
<body>
    <!--[if IE]>
      <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
    <![endif]-->

<div class="mk-wrapper">
    <section class="mk-masthead mk-masthead--sub">
<header class="mk-main-header">
    <a href="/" class="mk-main-header__brand">
        <svg version="1.1" viewBox="0 0 557 540" xmlns="http://www.w3.org/2000/svg">
          <g fill="none" fill-rule="evenodd">
            <g transform="translate(-1)" fill-rule="nonzero">
            <path d="m181.91 539.68h-0.7c-0.76 0-1.44-0.11-2-0.17h-0.14l-1.62-0.2c-15.204-1.867-29.364-8.7129-40.27-19.47l-1.07-1.06-49.46-61.26-73.34-90.59-0.5-0.72c-2.8927-4.0899-5.2989-8.503-7.17-13.15-1.0257-2.532-1.8875-5.1274-2.58-7.77v-0.11c-0.22-0.85-0.43-1.69-0.62-2.56v-0.14c-0.8042-3.5966-1.2861-7.2578-1.44-10.94v-0.47-0.48c-0.067687-4.136 0.26722-8.2688 1-12.34l0.11-0.61 14.51-63.64 28.72-126c4.0017-17.442 15.802-32.074 32-39.68l178.2-85.93 3.34-0.67c5.6926-1.1418 11.484-1.7201 17.29-1.7201h2.83 0.57c8.4518-0.016309 16.808 1.7879 24.5 5.2901l0.47 0.22 175.82 84.2 0.48 0.25c7.1101 3.7526 13.491 8.7481 18.84 14.75 2.7886 3.1018 5.2639 6.4715 7.39 10.06l0.17 0.29c2.1776 3.7964 3.9314 7.8205 5.23 12l0.3 1 44.23 190.25 0.17 1.42c2.0399 16.443-2.1677 33.052-11.79 46.54l-0.48 0.68-121.46 150.24c-7.2792 9.604-17.475 16.59-29.06 19.91-0.93 0.27-1.87 0.52-2.81 0.75l-0.3 0.07c-5.0328 1.1701-10.183 1.76-15.35 1.76h-194.01z" fill="#fff"/>
            <path d="m492 131.65c-0.75221-2.3458-1.7582-4.6025-3-6.73-1.2507-2.1148-2.7114-4.0982-4.36-5.92-3.3032-3.7145-7.2456-6.8067-11.64-9.13l-179.82-86c-4.3569-1.9816-9.0938-2.9883-13.88-2.95h-0.77c-4.9428-0.19294-9.8909 0.20318-14.74 1.18l-179.72 86.6c-8.9642 4.1124-15.498 12.17-17.67 21.79l-3.69 16.16 216.29 117.67 0.34-0.18 217.22-112.72-4.56-19.77z" fill="#00E0C1"/>
            <path d="m279 264.32l-216.29-117.67-25.77 113.1-14.73 64.63c-0.44744 2.4671-0.64178 4.9734-0.58 7.48v0.29c0.072639 2.1493 0.33702 4.2878 0.79 6.39 0.12 0.56 0.26 1.1 0.4 1.65 0.41228 1.5719 0.92671 3.1151 1.54 4.62 1.1152 2.7748 2.5517 5.4095 4.28 7.85l23.69 29.27 51 63 49.67 61.55c6.7982 6.7311 15.643 11.009 25.14 12.16 0.67 0 1.31 0.18 2 0.21h99.17v-254.34l-0.31-0.19z" fill="#00EEC4"/>
            <path d="m536.75 324.38l-40.19-173-217.23 112.76v254.71h98.82c3.2616 0.017 6.5139-0.34884 9.69-1.0906 0.62-0.15 1.23-0.31 1.84-0.49 6.2438-1.7629 11.72-5.5604 15.56-10.79l66.09-81.75 31.31-38.73 26.94-33.33c5.8432-8.2018 8.4013-18.295 7.17-28.29z" fill="#00D1BD"/>
            <path d="m120.94 369l137 75.89c1.3702 0.76284 3.0421 0.74251 4.3933-0.05344s2.1796-2.2483 2.1767-3.8166v-161.02c0-5.718-3.1489-10.971-8.19-13.67l-1.64-0.87-134.68-71.99c-0.8041-0.43178-1.7757-0.41032-2.56 0.056543-0.78426 0.46687-1.2663 1.3108-1.27 2.2235l-0.94 163.17c0.02 3.63 2.77 8.44 5.71 10.08z" fill="#fff"/>
            <path d="m282.61 103.85c-4.0372-0.033083-8.0333 0.81323-11.71 2.481l-134.2 60.47c-0.91184 0.40637-1.512 1.2973-1.5476 2.295-0.032512 0.99771 0.50554 1.9274 1.3876 2.395l135.72 72.51c0.15 0.09 0.31 0.16 0.47 0.24l0.59 0.29 0.26 0.11 0.8 0.34h0.09c4.9879 1.8704 10.539 1.5061 15.24-1l139.14-73.94c1.1079-0.5822 1.7814-1.7504 1.7328-3.0009-0.054039-1.2505-0.82096-2.3596-1.9728-2.8491l-135.06-58.05c-3.4545-1.4945-7.1761-2.2735-10.94-2.291z" fill="#fff"/>
            <path d="m442.82 192.61c-1.08-0.49333-2.4133-0.29667-4 0.59l-24.52 13.54c-3.6117 1.9922-6.3845 5.2194-7.81 9.09l-37.49 87.55-37.31-46.2c-1.59-2.29-4.2-2.45-7.81-0.45l-24.51 13.54c-1.6358 0.9266-3.0116 2.2508-4 3.85-1.0039 1.4454-1.5667 3.151-1.62 4.91v166.83c0 1.59 0.55 2.59 1.63 3s2.42 0.19 4-0.69l27.34-15.1c1.6143-0.90735 2.9864-2.1902 4-3.74 1.0178-1.3976 1.5863-3.0717 1.63-4.8v-105l23.21 30.12c2.1667 2.1267 4.6967 2.3933 7.59 0.8l11.67-6.45c3.18-1.7467 5.71-4.8067 7.59-9.18l23.43-55.9 0.25 97.1 0.17 8.31c-0.10795 1.217 0.57186 2.3675 1.69 2.86 1.2747 0.44018 2.6836 0.23517 3.78-0.55l27.27-15.83c1.5869-0.9387 2.9245-2.2455 3.9-3.81 0.9881-1.4207 1.5184-3.1095 1.5212-4.84v-166.43c0.028815-1.6-0.51118-2.64-1.6012-3.12z" fill="#fff"/>
            </g>
          </g>
        </svg>
      </a>
      <div role="navigation" class="mk-main-header__nav-wrapper">
        <button class="mk-main-header__toggle" id="toggle" aria-controls="main_nav" aria-expanded="false" aria-label="navigation toggle" >
          <svg version="1.1" viewBox="0 0 512 448" xmlns="http://www.w3.org/2000/svg">
          <g>
          <path d="m296 0h192c13.255 0 24 10.745 24 24v160c0 13.255-10.745 24-24 24h-192c-13.255 0-24-10.745-24-24v-160c0-13.255 10.745-24 24-24zm-80 0h-192c-13.255 0-24 10.745-24 24v160c0 13.255 10.745 24 24 24h192c13.255 0 24-10.745 24-24v-160c0-13.255-10.745-24-24-24zm-216 264v160c0 13.255 10.745 24 24 24h192c13.255 0 24-10.745 24-24v-160c0-13.255-10.745-24-24-24h-192c-13.255 0-24 10.745-24 24zm296 184h192c13.255 0 24-10.745 24-24v-160c0-13.255-10.745-24-24-24h-192c-13.255 0-24 10.745-24 24v160c0 13.255 10.745 24 24 24z"/>
          </g>
          </svg>
          <span class="mk-main-header__toggle-text">menu</span>
        </button>
        </div>
        <ul id="main_nav" class="mk-main-nav">
          <li ><a class="mk-main-nav__item" href="/blog/index.html">Blog</a></li>
          <li ><a class="mk-main-nav__item" href="/community-resources.html">Community Resources</a></li>
          <li ><a class="mk-main-nav__item" href="https://book.metal3.io">Documentation</a></li>
          <li ><a class="mk-main-nav__item" href="/contribute.html">Contribute</a></li>
          <li ><a class="mk-main-nav__item" href="/faqs.html">FAQs</a></li>
          <li ><a class="mk-main-nav__item" href="https://book.metal3.io/developer_environment/tryit">Try It!</a></li>
          <li  class="active"  id="mk-main-nav__search">
            <form action="/search.html" method="get" autocomplete="off" class="mk-search-form">
              <div class="autocomplete" style="width:150px;">
                <input type="text" id="search-input" class="docs-search--input" placeholder="Search Term" name="query">
              </div>
              <button type="submit" id = "search-button" class = "search-button" disabled = 'true' >
                <img src="/assets/images/search.png" style="height: 20px;" alt="">
              </button>
                <div id="mode-toggle">
                  <img src="/assets/images/moon-outline.png" id="mode-icon" style="height: 20px; margin-left: 10px;"/>
                </div>
            </form>
          </li>

        </ul>
  </header>
  
<script>
function autocomplete(inp, arr) {
  /*the autocomplete function takes two arguments,
  the text field element and an array of possible autocompleted values:*/
  var currentFocus;
  /*execute a function when someone writes in the text field:*/
  inp.addEventListener("input", function(e) {
      var a, b, i, val = this.value;
      /*close any already open lists of autocompleted values*/
      closeAllLists();
      if (!val) { return false;}
      currentFocus = -1;
      /*create a DIV element that will contain the items (values):*/
      a = document.createElement("DIV");
      a.setAttribute("id", this.id + "autocomplete-list");
      a.setAttribute("class", "autocomplete-items");
      /*append the DIV element as a child of the autocomplete container:*/
      this.parentNode.appendChild(a);
      /*for each item in the array...*/
      for (i = 0; i < arr.length; i++) {
        /*check if the item starts with the same letters as the text field value:*/
        if (arr[i].substr(0, val.length).toUpperCase() == val.toUpperCase()) {
          /*create a DIV element for each matching element:*/
          b = document.createElement("DIV");
          /*make the matching letters bold:*/
          b.innerHTML = "<strong>" + arr[i].substr(0, val.length) + "</strong>";
          b.innerHTML += arr[i].substr(val.length);
          /*insert a input field that will hold the current array item's value:*/
          b.innerHTML += "<input type='hidden' value='" + arr[i] + "'>";
          /*execute a function when someone clicks on the item value (DIV element):*/
              b.addEventListener("click", function(e) {
              /*insert the value for the autocomplete text field:*/
              inp.value = this.getElementsByTagName("input")[0].value;
              /*close the list of autocompleted values,
              (or any other open lists of autocompleted values:*/
              closeAllLists();
          });
          a.appendChild(b);
        }
      }
  });
  /*execute a function presses a key on the keyboard:*/
  inp.addEventListener("keydown", function(e) {
      document.getElementById("search-button").disabled= undefined;
      var x = document.getElementById(this.id + "autocomplete-list");
      if (x) x = x.getElementsByTagName("div");
      if (e.keyCode == 40) {
        /*If the arrow DOWN key is pressed,
        increase the currentFocus variable:*/
        currentFocus++;
        /*and and make the current item more visible:*/
        addActive(x);
      } else if (e.keyCode == 38) { //up
        /*If the arrow UP key is pressed,
        decrease the currentFocus variable:*/
        currentFocus--;
        /*and and make the current item more visible:*/
        addActive(x);
      } else if (e.keyCode == 13) {
        /*If the ENTER key is pressed, prevent the form from being submitted,*/
        if (currentFocus > -1) {
          /*and simulate a click on the "active" item:*/
          if (x) {
            x[currentFocus].click();
            e.preventDefault();
          }
        }
        if (document.getElementById("search-input").value == "") {
          e.preventDefault();
        }
      }
  });
  function addActive(x) {
    /*a function to classify an item as "active":*/
    if (!x) return false;
    /*start by removing the "active" class on all items:*/
    removeActive(x);
    if (currentFocus >= x.length) currentFocus = 0;
    if (currentFocus < 0) currentFocus = (x.length - 1);
    /*add class "autocomplete-active":*/
    x[currentFocus].classList.add("autocomplete-active");
  }
  function removeActive(x) {
    /*a function to remove the "active" class from all autocomplete items:*/
    for (var i = 0; i < x.length; i++) {
      x[i].classList.remove("autocomplete-active");
    }
  }
  function closeAllLists(elmnt) {
    /*close all autocomplete lists in the document,
    except the one passed as an argument:*/
    var x = document.getElementsByClassName("autocomplete-items");
    for (var i = 0; i < x.length; i++) {
      if (elmnt != x[i] && elmnt != inp) {
      x[i].parentNode.removeChild(x[i]);
    }
  }
}
/*execute a function when someone clicks in the document:*/
document.addEventListener("click", function (e) {
    closeAllLists(e.target);
});
}
</script>
<script>
  document.addEventListener("DOMContentLoaded", function(){
  let iconMode = document.getElementById("mode-icon")
  let toggleMode = document.getElementById("mode-toggle")
  let cncfImage = document.getElementById("cncf-image")
let isToggled = localStorage.getItem("currentMode") === "true";
updateMode();
toggleMode.addEventListener("click", () => {
  isToggled = !isToggled;
  localStorage.setItem("currentMode", isToggled);
  updateMode();
});
function updateMode() {
  let mastHead = document.querySelector(".mk-masthead");
  let h1 = document.querySelectorAll("h1")
  let h2 = document.querySelectorAll("h2")
  let h3 = document.querySelectorAll("h3")
  let li = document.querySelectorAll("li")
  let sections = document.querySelectorAll(".mk-main__section")
  let body = document.querySelector("body")
  let whyCards = document.querySelectorAll(".mk-why-baremetal__card")
  let blogCards = document.querySelectorAll(".mk-blog-meta__card")
  let questions = document.querySelectorAll(".mk-faqs__question")
  let subHeadings = document.querySelectorAll(".mk-sub-heading")
  let p = document.querySelectorAll("p")
  if (isToggled) {

    iconMode.src = "/assets/images/moon-outline.png";
    cncfImage.src = "/assets/images/cncf-white.svg";
    mastHead.style.backgroundColor = "var(--mk--BackgroundColor--500)";
    body.style.backgroundColor = "var(--mk--BackgroundColor--500)";
    body.style.color = "var(--mk--Color--200)";
    h1.forEach((eachH1)=>{
      eachH1.style.color = "var(--mk--Color--200)"
    })
    h2.forEach((eachH2)=>{
      eachH2.style.color = "var(--mk--Color--200)"
    })
    h3.forEach((eachH3)=>{
      eachH3.style.color = "var(--mk--Color--200)"
    })
    li.forEach((eachLi)=>{
      eachLi.style.color = "var(--mk--Color--200)"
    })
    sections.forEach((section)=>{
      section.style.backgroundColor = "var(--mk--BackgroundColor--500)";
    })
    p.forEach((eachP)=>{
      eachP.style.color = "var(--mk--Color--200)"
    })
    whyCards.forEach((whyCard)=>{
    whyCard.querySelector("h3").style.color = "var(--mk--BackgroundColor--150)"
      whyCard.style.backgroundColor = "var(--mk--BackgroundColor--175)"
    })
    blogCards.forEach((blogCard)=>{
      blogCard.style.backgroundColor = "var(--mk--color-brand--400)";
    })
    questions.forEach((question)=>{
      question.style.color = "var(--mk--Color--200)"
    })
    subHeadings.forEach((subHeading)=>{
      subHeading.style.color = "var(--mk--Color--500)"
    })
  } else {
    iconMode.src = "/assets/images/moon.png";
    cncfImage.src = "/assets/images/cncf-color.svg";
    mastHead.style.backgroundColor = "";
    body.style.backgroundColor = "";
    body.style.color = "var(--mk--Color--400)";


    h1.forEach((eachH1)=>{
      eachH1.style.color = ""
    })
    h2.forEach((eachH2)=>{
      eachH2.style.color = ""
    })
    h3.forEach((eachH3)=>{
      eachH3.style.color = ""
    })
    sections.forEach((section)=>{
      section.style.backgroundColor = "var(--mk--BackgroundColor--250)";
    })
    li.forEach((eachLi)=>{
      eachLi.style.color = ""
    })
    p.forEach((eachP)=>{
      eachP.style.color = ""
    })
    whyCards.forEach((whyCard)=>{
      whyCard.querySelector("h3").style.color = ""
      whyCard.style.backgroundColor = "white"
    })
    blogCards.forEach((blogCard)=>{
      blogCard.style.backgroundColor = ""
    })
    questions.forEach((question)=>{
      question.style.color = ""
    })
    subHeadings.forEach((subHeading)=>{
      subHeading.style.color = "var(--mk--Color--500)"
    })


  }
}
})
</script>
<script>
var mykeywords = ["hybrid", "cloud", "metal3", "baremetal", "stack", "edge", "openstack", "ironic", "openshift", "kubernetes", "OpenStack", "operator", "summit", "kubecon", "shiftdev", "metal3-dev-env", "documentation", "development", "talk", "conference", "meetup", "cluster API", "provider", "raw image", "image streaming", "IPAM", "ip address manager", "Pivoting", "Move", "scaling", "cncf", "community", "announcement", "IPA", "OCI", "deployment", "bare metal", ]
autocomplete(document.getElementById("search-input"), mykeywords);
</script>
<script src="/assets/js/clipboard.min.js"></script>
<!-- Photoswipe -->
<!-- Core JS file -->
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe.min.js"></script>
<!-- UI JS file -->
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>

<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="/assets/js/lunr.min.js"></script>

<div class="mk-masthead__content--sub">
        <h1 class="mk-masthead__content--sub__title">Search results</h1>
</div>
</section>
<main class="mk-main mk-blog">
            <article class="mk-main__section mk-main__content mk-main__section__content">
    <div class="container post">
      <h1 class="page-title"></h1>
      <article class="post-content">
        <div id="lunrsearchresults">
            <ul></ul>
        </div>
      </article>
    </div>

</article>
<nav class="mk-pagination">
        
        
</nav>

    </main>



<script>

var documents = [{
    "id": 0,
    "url": "/blog/2026/02/01/Deploying_OCI_Images_with_Custom_IPA_Hardware_Manager.html",
    "title": "Deploying OCI Container Images to Bare Metal with a Custom IPA Hardware Manager",
    "author" : "Serhii Ivanov",
    "tags" : "metal3, ironic, IPA, OCI, deployment, bare metal",
    "body": "What if you could deploy any OCI container image directly to bare metal,without building traditional disk images? Back in 2021, Dmitry Tantsurimplemented custom deploy stepsfor Ironic, enabling alternative deployment methods beyond the standardimage-based approach. This feature powers OpenShift’s bare metalprovisioning with CoreOS, yet it remains surprisingly unknown to thebroader Metal3 community. This post aims to change that by providing anexample implementation of a custom IPA hardware manager that deploysDebian-based container images with EFI boot, LVM root filesystem, andoptional RAID1 mirroring. The Problem with Traditional Image-Based Deployments: Traditional bare metal provisioning with Metal3 and Ironic typicallyrequires pre-built disk images. You need to maintain these images,update them regularly, and ensure they contain all necessary driversand configurations. This approach has some drawbacks:  Image building complexity - Building and maintaining OS diskimages is not as trivial as creating container images Software RAID limitations - Image-based deployments with mdadmRAID and EFI boot require workaroundsWhat if we could leverage the container ecosystem instead? Containerregistries already solve the distribution problem, and OCI images areversioned, layered, simple to build and widely available. This approachallows you to:  Use standard container images from any registry Avoid maintaining custom disk images Easily switch between OS versions by updating spec. image. url Get RAID1 redundancy with minimal configurationIntroducing the deb_oci_efi_lvm Hardware Manager: The DebOCIEFILVMHardwareManageris a custom IPA hardware manager that deploys Debian-based OCI containerimages directly to bare metal. Itprovides:  EFI boot support - UEFI boot with GRUB, which unlike systemd-boot,supports booting from LVM on top of mdadm software RAID LVM root filesystem - Flexible volume management for the rootpartition Optional RAID1 - Software mirroring across two disks forredundancy Cloud-init integration - Ironic configdrivedata is written directly to the root filesystem, no separate configdrivepartition Multi-architecture - Supports x86_64 and ARM64 via OCI multi-archimagesHow It Works: The deployment process extracts an OCI image using Google’s crane tool,then installs the necessary boot infrastructure on top. The hardwaremanager supports three methods for specifying the OCI image (in priorityorder):  spec. image. url with oci:// prefix (e. g. , oci://debian:12) Configdrive metadata annotation bmh. metal3. io/oci_image Default fallback: ubuntu:24. 04Root device hints can be specified using either standard BareMetalHostrootDeviceHints fields or a simplified format via thebmh. metal3. io/root_device_hints annotation (e. g. , serial=ABC123 orwwn=0x123456). For RAID1 configurations, provide two space-separatedvalues (e. g. , serial=ABC123 DEF456).  Note: Alternatively, podman can be used instead of crane for OCIimage extraction, as it is readily available in CentOS Stream 9 and alsohas an export command. This would require code modifications to thehardware manager. The hardware manager performs these steps during deployment:  Resolve OCI image - Check image_source, configdrive, or use default Resolve target disks - Parse root device hints (serial or WWN) Clean existing data - Wipe partitions, RAID arrays, and LVM based ondisk wipe mode (all for RAID1, target for single disk by default) Partition disks - Create 2GB EFI partition and LVM partition(with RAID1 if two disks are specified) Create filesystems - FAT32 for EFI, ext4 for root LV Extract OCI image - Use crane export piped to tar for rootfs Install packages - Add cloud-init, GRUB, kernel, mdadm, lvm2 Configure boot - Set up GRUB, initramfs, and fstab Install bootloader - GRUB to both EFI partitions for RAID1Disk Layout: The hardware manager creates the following partition layout:       Partition   Size   Filesystem   Label   Mount Point         1 (EFI)   2 GB   FAT32   EFI   /boot/efi       2 (LVM/RAID)   Remaining   -   -   -   The LVM configuration:       Component   Name   Description         Volume Group   vg_root   Contains all logical volumes       Logical Volume   lv_root   Root filesystem (100% of VG)       Filesystem   ext4   Label: ROOTFS   For RAID1 configurations, both disks get identical partition tables,with partition 2 forming a RAID1 array that serves as the LVM physicalvolume. Configuration: Basic Single-Disk Deployment: For a simple single-disk deployment, configure your BareMetalHost andMetal3MachineTemplate as follows: apiVersion: metal3. io/v1alpha1kind: BareMetalHostmetadata: name: my-server namespace: metal3spec: online: true bootMode: UEFI # Preferred method: Use spec. image. url with oci:// prefix image:  url:  oci://debian:12  rootDeviceHints:  serialNumber:  DISK_SERIAL_NUMBER Alternatively, you can use annotations or simplified hint formats: apiVersion: metal3. io/v1alpha1kind: BareMetalHostmetadata: name: my-server-alt namespace: metal3 annotations:  # Alternative: Override default ubuntu:24. 04 via annotation  bmh. metal3. io/oci_image:  debian:12   # Alternative: Use simplified hint format  bmh. metal3. io/root_device_hints:  serial=DISK_SERIAL_NUMBER spec: online: true bootMode: UEFIThe hardware manager supports three methods for specifying the OCI image(in priority order):  spec. image. url with oci:// prefix (highest priority, recommended) Annotation bmh. metal3. io/oci_image passed via Metal3DataTemplate Default ubuntu:24. 04 (fallback)Root device hints support both standard format (serialNumber:  ABC123 )and simplified format via annotation (bmh. metal3. io/root_device_hints:  serial=ABC123 ). apiVersion: infrastructure. cluster. x-k8s. io/v1beta1kind: Metal3MachineTemplatemetadata: name: my-worker-template namespace: metal3spec: template:  spec:   customDeploy:    method:  deb_oci_efi_lvm    dataTemplate:    name: my-data-templateRAID1 Configuration: For production deployments requiring disk redundancy, specify two diskserial numbers. The hardware manager supports multiple formats: Method 1: Standard format with space-separated values: apiVersion: metal3. io/v1alpha1kind: BareMetalHostmetadata: name: my-ha-server namespace: metal3spec: online: true bootMode: UEFI image:  url:  oci://debian:13  rootDeviceHints:  # Two space-separated serial numbers enable RAID1  serialNumber:  DISK1_SERIAL DISK2_SERIAL Method 2: Simplified format via annotation: apiVersion: metal3. io/v1alpha1kind: BareMetalHostmetadata: name: my-ha-server-alt namespace: metal3 annotations:  bmh. metal3. io/oci_image:  debian:13   # Simplified RAID1 hint format  bmh. metal3. io/root_device_hints:  serial=DISK1_SERIAL DISK2_SERIAL spec: online: true bootMode: UEFIWith RAID1 enabled, the hardware manager will:  Clean both disks (remove existing partitions, RAID arrays, and LVM) Create identical partition layouts on both disks Set up a RAID1 array (/dev/md0) for the LVM physical volume Install GRUB to both EFI partitions Configure a GRUB update hook to sync EFI partitions via rsyncDisk Wipe Mode Configuration: By default, the hardware manager wipes all block devices for RAID1configurations (to prevent stray RAID/LVM metadata issues) and only targetdisks for single-disk setups. You can override this behavior: apiVersion: metal3. io/v1alpha1kind: BareMetalHostmetadata: name: my-server namespace: metal3 annotations:  # Control disk cleaning behavior  #  all  - Wipe all block devices (recommended for RAID1)  #  target  - Wipe only target disk(s) from root device hints  bmh. metal3. io/disk_wipe_mode:  all spec: online: true bootMode: UEFI image:  url:  oci://ubuntu:24. 04  rootDeviceHints:  serialNumber:  DISK_SERIAL_NUMBER The disk_wipe_mode annotation is useful when:  You have multiple disks and want to ensure clean RAID/LVM state (all) You want to preserve data on non-target disks (target) You’re migrating from a previous RAID configurationMetal3DataTemplate Configuration: When using annotations (instead of spec. image. url), configure yourMetal3DataTemplate to pass them to the configdrive: apiVersion: infrastructure. cluster. x-k8s. io/v1beta1kind: Metal3DataTemplatemetadata: name: my-data-template namespace: metal3spec: clusterName: my-cluster metaData:  fromAnnotations:  # Optional: Pass OCI image annotation (only if not using spec. image. url)  - key: oci_image   object: baremetalhost   annotation:  bmh. metal3. io/oci_image   # Optional: Pass simplified root device hint  - key: root_device_hints   object: baremetalhost   annotation:  bmh. metal3. io/root_device_hints   # Optional: Pass disk wipe mode  - key: disk_wipe_mode   object: baremetalhost   annotation:  bmh. metal3. io/disk_wipe_mode   objectNames:  - key: name   object: machine  - key: local-hostname   object: machine  - key: local_hostname   object: machine  - key: metal3-name   object: baremetalhost  - key: metal3-namespace   object: baremetalhost networkData:  links:   ethernets:   - id: enp1s0    macAddress:     fromHostInterface: enp1s0    type: phy  networks:   ipv4:   - id: baremetalv4    ipAddressFromIPPool: my-ip-pool    link: enp1s0    routes:    - gateway:      fromIPPool: my-ip-pool     network: 0. 0. 0. 0     prefix: 0  services:   dns:   - 8. 8. 8. 8 Note: When using spec. image. url with the oci:// prefix, you don’tneed to pass the oci_image annotation through Metal3DataTemplate. Thehardware manager reads directly from instance_info. image_source. This isthe recommended approach for newer deployments. Building an IPA Image with the Hardware Manager: To use this hardware manager, you need to build a custom IPA ramdiskimage usingironic-python-agent-builder. This tool uses diskimage-builder(DIB) to create bootable ramdisk images containing the IPA and anycustom elements you need. Required Packages: The hardware manager requires several packages to be present in theIPA ramdisk:       Package   Purpose         crane   OCI image extraction from container registries       mdadm   Software RAID array management       lvm2   Logical Volume Manager for root filesystem       parted   Disk partitioning       dosfstools   FAT32 filesystem creation for EFI partition       grub2-efi-*   UEFI bootloader installation       curl   Downloading files during deployment       rsync   EFI partition synchronization for RAID   Custom DIB Elements: DIB elements are modular components that customize the image build. Each element is a directory containing scripts that run at differentphases of the build:       Directory   Phase   Description         extra-data. d/   Pre-build   Copy files into build environment       install. d/   Chroot   Run inside chroot during build       post-install. d/   Post-install   Run after package installation       finalise. d/   Finalize   Run at end of build process   Scripts are named with a numeric prefix (e. g. , 50-crane) to controlexecution order.  DIB element: crane (OCI image tool)     Create a DIB element to install Google’s crane tool for OCI imageextraction. Create the following directory structure:   crane/├── element-deps└── install. d/  └── 50-crane    The element-deps file can be empty or list dependencies. The installscript (install. d/50-crane):   #!/bin/bash# https://docs. openstack. org/diskimage-builder/latest/developer/developing_elements. htmlif [  ${DIB_DEBUG_TRACE:-0}  -gt 0 ]; then  set -xfiset -euset -o pipefailCRANE_VERSION= ${DIB_CRANE_VERSION:-latest} # Detect architectureARCH=$(uname -m)case  ${ARCH}  in  x86_64)    CRANE_ARCH= x86_64     ;;  aarch64)    CRANE_ARCH= arm64     ;;  *)    echo  Unsupported architecture: ${ARCH}     exit 1    ;;esacecho  Installing crane (${CRANE_VERSION}) for ${CRANE_ARCH}. . .  # Get the download URLif [  ${CRANE_VERSION}  =  latest  ]; then  DOWNLOAD_URL=$(curl -s https://api. github. com/repos/google/go-containerregistry/releases/latest |    grep  browser_download_url. *Linux_${CRANE_ARCH}. tar. gz  |    cut -d ' ' -f 4)else  DOWNLOAD_URL= https://github. com/google/go-containerregistry/releases/download/${CRANE_VERSION}/go-containerregistry_Linux_${CRANE_ARCH}. tar. gz fiif [ -z  ${DOWNLOAD_URL}  ]; then  echo  Failed to determine crane download URL   exit 1fiecho  Downloading crane from: ${DOWNLOAD_URL} # Download and extract craneTEMP_DIR=$(mktemp -d)curl -sL  ${DOWNLOAD_URL}  | tar -xz -C  ${TEMP_DIR} # Install crane binaryinstall -m 755  ${TEMP_DIR}/crane  /usr/local/bin/crane# Cleanuprm -rf  ${TEMP_DIR} # Verify installationif crane version; then  echo  crane installed successfully else  echo  crane installation verification failed   exit 1fi    DIB element: packages-install (extra packages)     Create a DIB element that installs packages from the DIB_EXTRA_PACKAGESenvironment variable:   packages-install/├── element-deps└── install. d/  └── 50-packages-install    The install script (install. d/50-packages-install):   #!/bin/bash# https://docs. openstack. org/diskimage-builder/latest/developer/developing_elements. htmlif [  ${DIB_DEBUG_TRACE:-0}  -gt 0 ]; then  set -xfiset -euset -o pipefail# Enable CRB (CodeReady Builder) repository and install EPELecho  Enabling CRB repository. . .  dnf config-manager --set-enabled crb || true# Detect CentOS version and install appropriate EPELif [ -f /etc/os-release ]; then  # shellcheck disable=SC1091  . /etc/os-release  case  ${VERSION_ID%%. *}  in    9)      echo  Installing EPEL for CentOS 9. . .        dnf install -y https://dl. fedoraproject. org/pub/epel/epel-release-latest-9. noarch. rpm || true      ;;    10)      echo  Installing EPEL for CentOS 10. . .        dnf install -y https://dl. fedoraproject. org/pub/epel/epel-release-latest-10. noarch. rpm || true      ;;    *)      echo  Unknown CentOS version: ${VERSION_ID}, skipping EPEL installation       ;;  esacfiif [ -z  ${DIB_EXTRA_PACKAGES:-}  ]; then  echo  No extra packages specified via DIB_EXTRA_PACKAGES, skipping   exit 0fiecho  Updating system packages. . .  dnf update -yecho  Installing extra packages: ${DIB_EXTRA_PACKAGES} # shellcheck disable=SC2086dnf install -y ${DIB_EXTRA_PACKAGES}echo  Cleaning package cache. . .  dnf clean allecho  Extra packages installation complete    Building the Image: Set the ELEMENTS_PATH to include your custom elements directory, thenrun the builder: export ELEMENTS_PATH= /path/to/your/dib-elements export DIB_EXTRA_PACKAGES= jq yq mdadm lvm2 curl parted util-linux \  squashfs-tools xfsprogs dosfstools grub2-efi-x64 grub2-tools rsync ironic-python-agent-builder \  -o ipa-custom \  -e extra-hardware \  -e crane \  -e packages-install \  --release 9-stream centosThis produces two files:  ipa-custom. kernel - The Linux kernel ipa-custom. initramfs - The ramdisk containing IPA and toolsFor ARM64 builds, the grub packages differ: export DIB_EXTRA_PACKAGES= jq yq mdadm lvm2 curl parted util-linux \  squashfs-tools xfsprogs dosfstools grub2-efi-aa64 grub2-tools rsync Installing the Hardware Manager: The hardware manager must be placed in the IPA hardware managers directoryand registered in setup. cfg. File location: ironic_python_agent/hardware_managers/deb_oci_efi_lvm. pysetup. cfg entry point: Add the following entry to the ironic_python_agent. hardware_managerssection in setup. cfg: [entry_points]ironic_python_agent. hardware_managers =  deb_oci_efi_lvm = ironic_python_agent. hardware_managers. deb_oci_efi_lvm:DebOCIEFILVMHardwareManagerThis registers the hardware manager as a plugin, allowing IPA todiscover and load it at runtime. Source Code: The implementation is shown below in expandable sections. Full source:deb_oci_efi_lvm. py.  Note: The code below uses a custom run_command helper functioninstead of IPA’s built-inironic_python_agent. utils. execute. This was a deliberate choice to minimize dependencies on IPA internals,avoiding the need to keep the hardware manager in constant sync withIPA changes. However, reusing IPA’s existing utilities is a validalternative approach. Imports and constantsStandard library and IPA imports, plus configuration constants fordevice paths, filesystem labels, and retry parameters. ```python# SPDX-License-Identifier: Apache-2. 0# SPDX-FileCopyrightText: 2025 s3rj1k   Debian/Ubuntu OCI EFI LVM deployment hardware manager. This hardware manager deploys Debian-based OCI container images with:- EFI boot partition- LVM on root partition- Optional RAID1 support for two-disk configurations   import osimport platformimport reimport shutilimport stat as stat_moduleimport subprocessimport tempfileimport timeimport yamlfrom oslo_log import logfrom ironic_python_agent import device_hintsfrom ironic_python_agent import hardwareLOG = log. getLogger(__name__)# Default OCI image (can be overridden via node metadata 'oci_image')DEFAULT_OCI_IMAGE =  ubuntu:24. 04 # Device/filesystem constantsRAID_DEVICE =  /dev/md0 VG_NAME =  vg_root LV_NAME =  lv_root ROOT_FS_LABEL =  ROOTFS BOOT_FS_LABEL =  EFI BOOT_FS_LABEL2 =  EFI2 DEVICE_PROBE_MAX_ATTEMPTS = 5DEVICE_PROBE_DELAY = 5DEVICE_WAIT_MAX_ATTEMPTS = 5DEVICE_WAIT_DELAY = 5```run_commandWrapper around `subprocess. run` with logging support. ```pythondef run_command(cmd, check=True, capture_output=True, timeout=300):     Run a shell command with logging.   :param cmd: Command as list of strings  :param check: Whether to raise on non-zero exit  :param capture_output: Whether to capture stdout/stderr  :param timeout: Command timeout in seconds  :returns: CompletedProcess object  :raises: subprocess. CalledProcessError on failure if check=True       LOG. debug( Running command: %s ,    . join(cmd))  result = subprocess. run(    cmd, check=check, capture_output=capture_output, text=True, timeout=timeout  )  if result. stdout:    LOG. debug( stdout: %s , result. stdout)  if result. stderr:    LOG. debug( stderr: %s , result. stderr)  return result```is_efi_systemChecks if the system booted in UEFI mode by testing for `/sys/firmware/efi`. ```pythondef is_efi_system():     Check if the system is booted in EFI mode.   :returns: True if running under EFI, False otherwise       return os. path. isdir( /sys/firmware/efi )```probe_deviceRuns `partprobe` and waits for device to appear in the kernel. ```pythondef probe_device(device):     Probe device until it is visible in the kernel.   :param device: Device path (e. g. , /dev/sda)  :raises: RuntimeError if device doesn't appear after max attempts       for attempt in range(DEVICE_PROBE_MAX_ATTEMPTS):    run_command([ partprobe , device], check=False)    time. sleep(DEVICE_PROBE_DELAY)    if os. path. exists(device):      LOG. debug( Device %s visible after %d attempt(s) , device, attempt + 1)      return  raise RuntimeError(    f Device {device} not visible after   f {DEVICE_PROBE_MAX_ATTEMPTS} attempts   )```has_interactive_usersChecks for logged-in users via `who` command, used to pause deploymentfor debugging via BMC console. ```pythondef has_interactive_users():     Check if there are any interactive users logged in.   Uses 'who' command to check for logged-in users, which indicates  someone has connected via BMC console for debugging.   :returns: Boolean indicating if interactive users are logged in       try:    result = run_command([ who ], check=True, timeout=5)    # who returns empty output if no users are logged in    users = result. stdout. strip()    if users:      LOG. debug( Interactive users detected: %s , users)      return True    return False  except (subprocess. CalledProcessError, subprocess. TimeoutExpired, OSError) as e:    LOG. warning( Failed to check for interactive users: %s , e)    return False```get_configdrive_dataExtracts configdrive dictionary from node's `instance_info`. ```pythondef get_configdrive_data(node):     Extract configdrive data from node instance_info.   :param node: Node dictionary containing instance_info  :returns: Dictionary containing configdrive data  :raises: ValueError if node is invalid or configdrive data is missing       if node is None:    raise ValueError( Node cannot be None )  if not isinstance(node, dict):    raise ValueError( Node must be a dictionary )  instance_info = node. get( instance_info , {})  if not isinstance(instance_info, dict):    raise ValueError( instance_info must be a dictionary )  configdrive = instance_info. get( configdrive )  if configdrive is None:    raise ValueError( configdrive not found in instance_info )  if not isinstance(configdrive, dict):    raise ValueError( configdrive must be a dictionary )  LOG. info( Extracted configdrive data: %s , configdrive)  return configdrive```parse_prefixed_hint_stringParses simplified hint format like `serial=ABC123` or `wwn=0x123456` intoIPA hint dictionary format. Supports RAID1 with space-separated values. ```pythondef parse_prefixed_hint_string(hint_string):     Parse a prefixed hint string into a hints dictionary.   Supports simplified format for cloud-init/annotation use cases:  - 'serial=ABC123' -&gt; {'serial': 's== ABC123'}  - 'wwn=0x123456' -&gt; {'wwn': 's== 0x123456'}  - 'serial=ABC123 DEF456' -&gt; {'serial': 's== ABC123 DEF456'} (RAID1)  - 'wwn=0x123 0x456' -&gt; {'wwn': 's== 0x123 0x456'} (RAID1)  :param hint_string: String with format 'hint_type=value1 [value2]'  :returns: Dictionary containing root_device hints  :raises: ValueError if format is invalid       if not hint_string or not isinstance(hint_string, str):    raise ValueError( Hint string must be a non-empty string )  hint_string = hint_string. strip()  if  =  not in hint_string:    raise ValueError(      'Hint string must contain  =  separator. '      'Expected format:  serial=VALUE  or  wwn=VALUE '    )  # Split on first equals only  parts = hint_string. split( = , 1)  if len(parts) != 2:    raise ValueError( Invalid hint string format )  hint_type = parts[0]. strip(). lower()  hint_values = parts[1]. strip()  if hint_type not in ( serial ,  wwn ):    raise ValueError(      f'Unsupported hint type  {hint_type} . '      'Only  serial  and  wwn  are supported. '    )  if not hint_values:    raise ValueError(f No value provided for {hint_type} hint )  # Add s== operator prefix (string equality)  hint_with_operator = f s== {hint_values}   LOG. info(    'Parsed prefixed hint string  %s  -&gt; { %s :  %s }',    hint_string,    hint_type,    hint_with_operator,  )  return {hint_type: hint_with_operator}```get_root_device_hintsExtracts root device hints from configdrive annotation or node's`instance_info`. Supports both simplified string format(`serial=ABC123`) and standard dictionary format. ```pythondef get_root_device_hints(node, configdrive_data):     Extract root_device hints from node instance_info or annotation.   Priority order:  1. configdrive meta_data. root_device_hints (prefixed string format)  1. node. instance_info. root_device (dict format with operators)  :param node: Node dictionary containing instance_info  :param configdrive_data: Configdrive dictionary  :returns: Dictionary containing root_device hints  :raises: ValueError if node is invalid or root_device not found anywhere       if node is None:    raise ValueError( Node cannot be None )  if not isinstance(node, dict):    raise ValueError( Node must be a dictionary )  instance_info = node. get( instance_info , {})  if not isinstance(instance_info, dict):    raise ValueError( instance_info must be a dictionary )  # Check annotation first (via configdrive metadata)  meta_data = configdrive_data. get( meta_data , {})  annotation_hints = meta_data. get( root_device_hints )  if annotation_hints is not None:    # Annotations use prefixed string format only    if not isinstance(annotation_hints, str):      raise ValueError(         root_device_hints from annotation must be a string          'in format  serial=VALUE  or  wwn=VALUE '      )    parsed_hints = parse_prefixed_hint_string(annotation_hints)    LOG. info( Using root_device hints from annotation: %s , parsed_hints)    return parsed_hints  # Fall back to instance_info  root_device = instance_info. get( root_device )  if root_device is not None:    if not isinstance(root_device, dict):      raise ValueError( root_device must be a dictionary )    LOG. info( Using root_device hints from instance_info: %s , root_device)    return root_device  # Neither source provided root_device hints  raise ValueError( root_device hints not found in instance_info or annotation )```find_device_by_hintsUses IPA's `device_hints` module to find a block device by serial or WWN. ```pythondef find_device_by_hints(hints):     Find a single block device matching the given hints.   :param hints: Dictionary containing device hints (serial or wwn)  :returns: Device path (e. g. , /dev/sda)  :raises: ValueError if no device or multiple devices match       devices = hardware. list_all_block_devices()  LOG. debug( list_all_block_devices returned type: %s , type(devices). __name__)  LOG. info( Found %d block devices , len(devices))  serialized_devs = [dev. serialize() for dev in devices]  matched_raw = device_hints. find_devices_by_hints(serialized_devs, hints)  matched = list(matched_raw)  if not matched:    raise ValueError(f No device found matching hints: {hints} )  if len(matched) &gt; 1:    device_names = [dev[ name ] for dev in matched]    raise ValueError(      f Multiple devices match hints: {device_names}.        f Hints must match exactly one device.      )  return matched[0][ name ]```parse_hint_valuesParses hint strings, stripping operator prefixes and splitting multiplevalues for RAID1 configurations. ```pythondef parse_hint_values(hint):     Parse hint value, handling operator prefixes like 's=='.   Returns list of values without the operator prefix.   For RAID1: 's== SERIAL1 SERIAL2' -&gt; ['SERIAL1', 'SERIAL2']  For single: 's== SERIAL1' -&gt; ['SERIAL1']  For plain: 'SERIAL1 SERIAL2' -&gt; ['SERIAL1', 'SERIAL2']  :param hint: Hint string value (may include operator prefix)  :returns: List of values without operator prefix       if not hint:    return []  parts = hint. split()  # Check if first part is an operator (e. g. , 's==', 'int', etc. )  operators = ( s== ,  s!= ,   ,   ,  int ,  float )  if parts and parts[0] in operators:    return parts[1:] # Skip the operator  return parts```&lt;/details&gt;resolve_root_devicesResolves device paths from hints. Returns one device for single-diskor two devices for RAID1 configuration. ```pythondef resolve_root_devices(root_device_hints):     Resolve root device path(s) from hints.   Only serial or wwn hints are supported. If the hint contains two  space-separated values, both devices are resolved for RAID1 setup.   :param root_device_hints: Dictionary containing root device hints  :returns: Tuple of device paths - (primary,) for single device or       (primary, secondary) for RAID1 configuration  :raises: ValueError if device cannot be resolved or hints are invalid       if root_device_hints is None:    raise ValueError( root_device_hints cannot be None )  if not isinstance(root_device_hints, dict):    raise ValueError( root_device_hints must be a dictionary )  # Validate that only serial or wwn hints are present  serial_hint = root_device_hints. get( serial )  wwn_hint = root_device_hints. get( wwn )  if not serial_hint and not wwn_hint:    raise ValueError( root_device_hints must contain serial or wwn hint )  # Check for unsupported hint types  supported_hints = { serial ,  wwn }  provided_hints = set(root_device_hints. keys())  unsupported = provided_hints - supported_hints  if unsupported:    raise ValueError(      f Unsupported root_device hints: {unsupported}.        f Only serial and wwn are supported.      )  LOG. info( Resolving root devices from hints: %s , root_device_hints)  # Parse hints - may contain one or two values (with optional operator)  serial_values = parse_hint_values(serial_hint)  wwn_values = parse_hint_values(wwn_hint)  # Determine if this is a RAID1 configuration  is_raid = len(serial_values) == 2 or len(wwn_values) == 2  if is_raid:    LOG. info( RAID1 configuration detected )  # Resolve primary device  primary_hints = {}  if serial_values:    primary_hints[ serial ] = serial_values[0]  if wwn_values:    primary_hints[ wwn ] = wwn_values[0]  primary_device = find_device_by_hints(primary_hints)  LOG. info( Resolved primary device: %s , primary_device)  if not is_raid:    return (primary_device,)  # Resolve secondary device for RAID1  secondary_hints = {}  if len(serial_values) == 2:    secondary_hints[ serial ] = serial_values[1]  if len(wwn_values) == 2:    secondary_hints[ wwn ] = wwn_values[1]  secondary_device = find_device_by_hints(secondary_hints)  LOG. info( Resolved secondary device: %s , secondary_device)  return (primary_device, secondary_device)```get_oci_imageGets OCI image reference with priority: `spec. image. url` (with `oci://`prefix) &gt; configdrive annotation &gt; default `ubuntu:24. 04`. ```pythondef get_oci_image(node, configdrive_data):     Get OCI image from instance_info, metadata, or use default.   Priority order:  1. node. instance_info. image_source with oci:// prefix  1. configdrive meta_data. oci_image (from annotation)  1. DEFAULT_OCI_IMAGE  :param node: Node dictionary containing instance_info  :param configdrive_data: Configdrive dictionary  :returns: OCI image reference string (without oci:// prefix)       oci_image = None  # Check instance_info first  instance_info = node. get( instance_info , {})  image_source = instance_info. get( image_source ,   ). strip()  if image_source. startswith( oci:// ):    oci_image = image_source. removeprefix( oci:// ). strip()    if not oci_image:      LOG. warning(         Empty OCI image after stripping oci:// prefix,           falling back to annotation/default       )      oci_image = None    else:      LOG. info( Using OCI image from instance_info: %s , oci_image)  else:    # Fall back to annotation (via configdrive metadata)    meta_data = configdrive_data. get( meta_data , {})    annotation_image = (meta_data. get( oci_image ) or   ). strip()    if annotation_image:      oci_image = annotation_image      LOG. info( Using OCI image from annotation: %s , oci_image)    else:      # Fall back to default      oci_image = DEFAULT_OCI_IMAGE      LOG. info( Using default OCI image: %s , oci_image)  return oci_image```get_disk_wipe_modeDetermines disk cleaning behavior based on annotation or setup type. Returns`all` to wipe all block devices (default for RAID1) or `target` to wipe onlyspecified disks (default for single disk). ```pythondef get_disk_wipe_mode(configdrive_data, is_raid):     Get disk wipe mode from configdrive or use default based on setup.   Priority order:  1. configdrive meta_data. disk_wipe_mode (from annotation)  1. Default:  all  for RAID1,  target  for single disk  :param configdrive_data: Configdrive dictionary  :param is_raid: Boolean indicating if this is a RAID setup  :returns: String  all  or  target   :raises: ValueError if disk_wipe_mode has invalid value       meta_data = configdrive_data. get( meta_data , {})  wipe_mode = (meta_data. get( disk_wipe_mode ) or   ). strip(). lower()  if wipe_mode:    if wipe_mode not in ( all ,  target ):      raise ValueError(        f'Invalid disk_wipe_mode  {wipe_mode} . '        'Valid values are:  all ,  target '      )    LOG. info( Using disk wipe mode from annotation: %s , wipe_mode)    return wipe_mode  # Use default based on setup type  default_mode =  all  if is_raid else  target   LOG. info(     Using default disk wipe mode for %s setup: %s ,     RAID1  if is_raid else  single disk ,    default_mode,  )  return default_mode```get_architecture_configReturns architecture-specific settings for x86_64 or ARM64, includingGRUB packages and UEFI target. ```pythondef get_architecture_config(oci_image):     Get architecture-specific configuration.   :param oci_image: OCI image reference to use  :returns: Dictionary with oci_image, oci_platform, uefi_target,       and grub_packages  :raises: RuntimeError if architecture is not supported       machine = platform. machine()  if machine ==  x86_64 :    return {       oci_image : oci_image,       oci_platform :  linux/amd64 ,       uefi_target :  x86_64-efi ,       grub_packages : [ grub-efi-amd64 ,  grub-efi-amd64-signed ,  shim-signed ],    }  elif machine ==  aarch64 :    return {       oci_image : oci_image,       oci_platform :  linux/arm64 ,       uefi_target :  arm64-efi ,       grub_packages : [ grub-efi-arm64 ,  grub-efi-arm64-bin ],    }  else:    raise RuntimeError(f Unsupported architecture: {machine} )```wait_for_deviceWaits for a block device to become available with retries. ```pythondef wait_for_device(device):     Wait for a block device to become available.   :param device: Device path (e. g. , /dev/sda)  :returns: True if device is available  :raises: RuntimeError if device doesn't appear       for attempt in range(DEVICE_WAIT_MAX_ATTEMPTS):    if os. path. exists(device):      try:        mode = os. stat(device). st_mode        if stat_module. S_ISBLK(mode):          LOG. info( Device %s is available , device)          return True      except OSError:        pass    LOG. debug(       Waiting for device %s (attempt %d/%d) ,      device,      attempt + 1,      DEVICE_WAIT_MAX_ATTEMPTS,    )    time. sleep(DEVICE_WAIT_DELAY)  raise RuntimeError(f Device {device} did not become available )```get_partition_pathReturns partition path, handling NVMe and MMC naming conventions. ```pythondef get_partition_path(device, partition_number):     Get the partition path for a device.   :param device: Base device path (e. g. , /dev/sda)  :param partition_number: Partition number  :returns: Partition path (e. g. , /dev/sda1 or /dev/nvme0n1p1)       if re. match(r . */nvme\d+n\d+$ , device) or re. match(r . */mmcblk\d+$ , device):    return f {device}p{partition_number}   return f {device}{partition_number} ```clean_deviceRemoves existing partitions, RAID arrays, LVM structures, and wipesthe device. ```pythondef clean_device(device):     Clean a device of existing partitions, RAID, and LVM.   :param device: Device path to clean       LOG. info( Cleaning device: %s , device)  # Stop any RAID arrays using this device  try:    result = run_command([ lsblk ,  -nlo ,  NAME,TYPE , device], check=False)    for line in result. stdout. strip(). split( \n ):      parts = line. split()      if len(parts) &gt;= 2 and parts[1] in (         raid1 ,         raid0 ,         raid5 ,         raid6 ,         raid10 ,      ):        raid_dev = f /dev/{parts[0]}         run_command([ mdadm ,  --stop , raid_dev], check=False)  except Exception:    pass  # Remove LVM if present (check device and all its partitions)  try:    # Get all block devices (device + partitions)    result = run_command([ lsblk ,  -nlo ,  NAME , device], check=False)    all_devs = []    for line in result. stdout. strip(). split( \n ):      name = line. strip()      if name:        all_devs. append(f /dev/{name} )    # Find all VGs that use any of these devices    vgs_to_remove = set()    for dev in all_devs:      result = run_command([ pvs , dev], check=False)      if result. returncode == 0:        vg_result = run_command(          [ pvs ,  --noheadings ,  -o ,  vg_name , dev], check=False        )        vg_name = vg_result. stdout. strip()        if vg_name:          vgs_to_remove. add(vg_name)    # Deactivate, remove all LVs and VGs    for vg_name in vgs_to_remove:      # Deactivate all LVs in this VG      run_command([ lvchange ,  -an , vg_name], check=False)      lv_result = run_command(        [ lvs ,  --noheadings ,  -o ,  lv_path , vg_name], check=False      )      for lv_path in lv_result. stdout. strip(). split( \n ):        lv_path = lv_path. strip()        if lv_path:          # Try dmsetup remove for stubborn LVs          dm_name = lv_path. replace( /dev/ ,   ). replace( / ,  - )          run_command(            [ dmsetup ,  remove ,  --retry ,  -f , dm_name], check=False          )          run_command([ lvremove ,  -ff , lv_path], check=False)      run_command([ vgremove ,  -ff , vg_name], check=False)    # Remove PVs from all devices    for dev in all_devs:      run_command([ pvremove ,  -ff ,  -y , dev], check=False)  except Exception:    pass  # Zero RAID superblocks  run_command([ mdadm ,  --zero-superblock ,  --force , device], check=False)  # Zero superblocks on partitions  try:    result = run_command([ lsblk ,  -nlo ,  NAME , device], check=False)    base_name = os. path. basename(device)    for line in result. stdout. strip(). split( \n ):      name = line. strip()      if name and name != base_name:        part_dev = f /dev/{name}         run_command(          [ mdadm ,  --zero-superblock ,  --force , part_dev], check=False        )        run_command([ wipefs ,  --all ,  --force , part_dev], check=False)  except Exception:    pass  # Wipe device  run_command([ wipefs ,  --all ,  --force , device], check=False)  run_command([ sgdisk ,  --zap-all , device], check=False)  # Sync filesystem buffers and wait for udev to settle  run_command([ sync ], check=False)  run_command([ udevadm ,  settle ], check=False)  # Probe until device is visible again  probe_device(device)  LOG. info( Device %s cleaned , device)```clean_all_devicesCleans all block devices on the system to remove stray RAID/LVM metadata. Useful when `disk_wipe_mode` is set to `all` (default for RAID1 setups). ```pythondef clean_all_devices():     Clean all block devices to remove stray RAID/LVM metadata.   Useful for nodes that may have multiple disks with old metadata  from previous deployments.        LOG. info( Cleaning all block devices on the system )  try:    devices = hardware. list_all_block_devices()    LOG. info( Found %d block devices to clean , len(devices))    for device_obj in devices:      device = device_obj. name      try:        clean_device(device)      except Exception as e:        LOG. warning( Error cleaning device %s: %s , device, e)    LOG. info( Finished cleaning all block devices )  except Exception as e:    LOG. error( Error listing block devices: %s , e)```clean_partition_signaturesCleans RAID, LVM, and filesystem signatures from a partition withoutremoving the partition itself. Used internally by `partition_disk()` toclean partitions before creating RAID arrays, ensuring no stray metadatacauses issues. ```pythondef clean_partition_signatures(partition):     Clean RAID, LVM, and filesystem signatures from a partition.   Does not remove the partition itself, only metadata/signatures.   :param partition: Partition path to clean       LOG. debug( Cleaning signatures from partition: %s , partition)  run_command([ pvremove ,  -ff ,  -y , partition], check=False)  run_command([ wipefs ,  --all ,  --force , partition], check=False)  run_command([ mdadm ,  --zero-superblock ,  --force , partition], check=False)```partition_diskCreates GPT partition table with EFI and LVM partitions. Sets up RAID1array if second device is provided. Calls `clean_partition_signatures()`before RAID creation to ensure clean metadata. ```pythondef partition_disk(  device, vg_name, lv_name, second_device=None, raid_device=RAID_DEVICE, homehost=None):     Partition disk with EFI and LVM (optionally on RAID).   :param device: Primary device path  :param vg_name: Volume group name  :param lv_name: Logical volume name  :param second_device: Optional second device for RAID  :param raid_device: RAID device path  :param homehost: Hostname for RAID array  :returns: Tuple of (is_raid, pv_device)       LOG. info( Partitioning disk: %s , device)  wait_for_device(device)  # Ensure udev has finished processing before partitioning  run_command([ udevadm ,  settle ], check=False)  # Create GPT partition table  run_command([ parted ,  -s , device,  mklabel ,  gpt ])  # Create EFI partition (2GB)  run_command(    [       parted ,       -s ,       -a ,       optimal ,      device,       mkpart ,       primary ,       fat32 ,       2MiB ,       2050MiB ,    ]  )  run_command([ parted ,  -s , device,  set ,  1 ,  esp ,  on ])  # Create data partition (rest of disk)  run_command(    [ parted ,  -s ,  -a ,  optimal , device,  mkpart ,  primary ,  2050MiB ,  99% ]  )  is_raid = second_device is not None  if is_raid:    run_command([ parted ,  -s , device,  set ,  2 ,  raid ,  on ])  else:    run_command([ parted ,  -s , device,  set ,  2 ,  lvm ,  on ])  # Wipe new partitions  try:    result = run_command([ lsblk ,  -nlo ,  NAME , device], check=False)    base_name = os. path. basename(device)    for line in result. stdout. strip(). split( \n ):      name = line. strip()      if name and name != base_name:        run_command([ wipefs ,  -a , f /dev/{name} ], check=False)  except Exception:    pass  data_partition = get_partition_path(device, 2)  pv_device = data_partition  if is_raid:    probe_device(device)    probe_device(second_device)    # Clone partition table to second device    sfdisk_result = run_command([ sfdisk ,  -d , device])    LOG. debug( Cloning partition table to %s , second_device)    sfdisk_proc = subprocess. run(      [ sfdisk ,  --force , second_device],      input=sfdisk_result. stdout,      capture_output=True,      text=True,      check=False,    )    if sfdisk_proc. stdout:      LOG. debug( sfdisk stdout: %s , sfdisk_proc. stdout)    if sfdisk_proc. stderr:      LOG. debug( sfdisk stderr: %s , sfdisk_proc. stderr)    if sfdisk_proc. returncode != 0:      raise subprocess. CalledProcessError(        sfdisk_proc. returncode,        [ sfdisk ,  --force , second_device],        sfdisk_proc. stdout,        sfdisk_proc. stderr,      )    # Randomize partition GUIDs on second device    run_command([ sgdisk ,  --partition-guid=1:R , second_device])    run_command([ sgdisk ,  --partition-guid=2:R , second_device])    second_data_partition = get_partition_path(second_device, 2)    probe_device(second_data_partition)    if not homehost:      raise RuntimeError( homehost required for RAID configuration )    # Clean new partitions before creating RAID    LOG. info( Cleaning partition signatures before RAID creation )    clean_partition_signatures(data_partition)    clean_partition_signatures(second_data_partition)    # Create RAID array    run_command(      [         mdadm ,         --create ,        raid_device,         --level=1 ,         --raid-devices=2 ,         --metadata=1. 2 ,         --name=root ,         --bitmap=internal ,        f --homehost={homehost} ,         --force ,         --run ,         --assume-clean ,        data_partition,        second_data_partition,      ]    )    # Sync filesystem buffers before continuing    run_command([ sync ], check=False)    time. sleep(5)    pv_device = raid_device  else:    probe_device(device)  # Create LVM  run_command([ pvcreate ,  -ff ,  -y ,  --zero ,  y , pv_device])  run_command([ vgcreate ,  -y , vg_name, pv_device])  run_command([ lvcreate ,  -y ,  -W ,  y ,  -n , lv_name,  -l ,  100%FREE , vg_name])  LOG. info( Disk partitioned successfully, is_raid=%s , is_raid)  return is_raid, pv_device```create_filesystemsCreates FAT32 filesystem on EFI partition and ext4 on root LV. ```pythondef create_filesystems(  efi_partition,  root_lv_path,  boot_label=BOOT_FS_LABEL,  root_label=ROOT_FS_LABEL,  second_efi_partition=None,  boot_label2=BOOT_FS_LABEL2,):     Create filesystems on partitions.   :param efi_partition: EFI partition path  :param root_lv_path: Root LV path  :param boot_label: EFI partition label  :param root_label: Root partition label  :param second_efi_partition: Second EFI partition for RAID  :param boot_label2: Second EFI partition label       LOG. info( Creating filesystems )  run_command([ mkfs. vfat ,  -F ,  32 ,  -n , boot_label, efi_partition])  if second_efi_partition:    run_command(      [ mkfs. vfat ,  -F ,  32 ,  -n , boot_label2, second_efi_partition],      check=False,    )  run_command([ mkfs. ext4 ,  -F ,  -L , root_label, root_lv_path])  LOG. info( Filesystems created )```setup_chrootMounts `/proc`, `/sys`, `/dev` and sets up DNS resolution in chroot. ```pythondef setup_chroot(chroot_dir):     Set up chroot environment with necessary mounts.   :param chroot_dir: Path to chroot directory       LOG. info( Setting up chroot: %s , chroot_dir)  run_command([ mount ,  -t ,  proc ,  proc , f {chroot_dir}/proc ])  run_command([ mount ,  -t ,  sysfs ,  sys , f {chroot_dir}/sys ])  run_command([ mount ,  --bind ,  /dev , f {chroot_dir}/dev ])  run_command([ mount ,  --bind ,  /dev/pts , f {chroot_dir}/dev/pts ])  os. makedirs(f {chroot_dir}/run , exist_ok=True)  # Set up resolv. conf  resolv_link = os. path. join(chroot_dir,  etc ,  resolv. conf )  if os. path. islink(resolv_link):    target = os. readlink(resolv_link)    if target. startswith( / ):      target_path = os. path. join(chroot_dir, target. lstrip( / ))    else:      target_path = os. path. join(chroot_dir,  etc , target)    os. makedirs(os. path. dirname(target_path), exist_ok=True)    shutil. copy( /etc/resolv. conf , target_path)  else:    shutil. copy( /etc/resolv. conf , resolv_link)  LOG. info( Chroot setup complete )```teardown_chrootUnmounts chroot bind mounts in reverse order. ```pythondef teardown_chroot(chroot_dir):     Tear down chroot environment.   :param chroot_dir: Path to chroot directory       LOG. info( Tearing down chroot: %s , chroot_dir)  mounts = [    f {chroot_dir}/run ,    f {chroot_dir}/dev/pts ,    f {chroot_dir}/dev ,    f {chroot_dir}/sys ,    f {chroot_dir}/proc ,  ]  for mount in mounts:    try:      result = run_command([ mountpoint ,  -q , mount], check=False)      if result. returncode == 0:        run_command([ umount ,  -l , mount])    except Exception as e:      LOG. warning( Error unmounting %s: %s , mount, e)  LOG. info( Chroot teardown complete )```extract_oci_imageExtracts OCI image filesystem using `crane export` piped to `tar`. ```pythondef extract_oci_image(image, platform, dest_dir):     Extract OCI image rootfs using crane.   :param image: OCI image reference (e. g. , ubuntu:24. 04)  :param platform: Target platform (e. g. , linux/amd64)  :param dest_dir: Destination directory for rootfs       LOG. info( Extracting OCI image %s (%s) to %s , image, platform, dest_dir)  # Use crane export to extract the image filesystem  # crane export outputs a tar stream, pipe to tar for extraction  crane_cmd = [ crane ,  export ,  --platform , platform, image,  - ]  tar_cmd = [ tar ,  -xf ,  - ,  -C , dest_dir]  LOG. info( Running: %s | %s ,    . join(crane_cmd),    . join(tar_cmd))  # Create pipeline: crane export | tar extract  crane_proc = subprocess. Popen(    crane_cmd, stdout=subprocess. PIPE, stderr=subprocess. PIPE  )  tar_proc = subprocess. Popen(    tar_cmd, stdin=crane_proc. stdout, stdout=subprocess. PIPE, stderr=subprocess. PIPE  )  # Allow crane to receive SIGPIPE if tar exits  crane_proc. stdout. close()  # Wait for tar to complete  tar_stdout, tar_stderr = tar_proc. communicate(timeout=1800)  # Wait for crane to complete  crane_proc. wait()  if crane_proc. returncode != 0:    _, crane_stderr = crane_proc. communicate()    raise RuntimeError(      f crane export failed with code {crane_proc. returncode}:        f {crane_stderr. decode() if crane_stderr else 'unknown error'}     )  if tar_proc. returncode != 0:    raise RuntimeError(      f tar extract failed with code {tar_proc. returncode}:        f {tar_stderr. decode() if tar_stderr else 'unknown error'}     )  if tar_stderr:    LOG. debug( tar stderr: %s , tar_stderr. decode())  LOG. info( OCI image extraction complete )```install_packagesInstalls cloud-init, GRUB, kernel, and other required packages via apt. ```pythondef install_packages(chroot_dir, grub_packages):     Install required packages in chroot.   :param chroot_dir: Path to chroot directory  :param grub_packages: List of GRUB packages to install       LOG. info( Installing packages in chroot )  # Remove snap packages if present  snap_path = os. path. join(chroot_dir,  usr ,  bin ,  snap )  if os. path. exists(snap_path):    snap_patterns = [       !/^Name|^core|^snapd|^lxd/ ,       /^lxd/ ,       /^core/ ,       /^snapd/ ,       !/^Name/ ,    ]    for pattern in snap_patterns:      try:        run_command(          [             chroot ,            chroot_dir,             sh ,             -c ,            f snap list 2&gt;/dev/null | awk '{pattern} ' |               xargs -rI{} snap remove --purge {} ,          ],          check=False,        )      except Exception:        pass  # Update package lists  run_command([ chroot , chroot_dir,  apt-get ,  update ])  # Remove unwanted packages one by one, ignoring errors for missing packages  for pkg in [ lxd ,  lxd-agent-loader ,  lxd-installer ,  snapd ]:    run_command(      [ chroot , chroot_dir,  apt-get ,  --purge ,  remove ,  -y , pkg],      check=False,    )  # Install required packages  packages = [     cloud-init ,     curl ,     efibootmgr ,     grub-common ,     initramfs-tools ,     lvm2 ,     mdadm ,     netplan. io ,     rsync ,     sudo ,     systemd-sysv ,  ] + grub_packages  run_command([ chroot , chroot_dir,  apt-get ,  install ,  -y ] + packages)  # Install kernel based on distro  try:    os_release_path = os. path. join(chroot_dir,  etc ,  os-release )    distro_id = None    version_id = None    if os. path. exists(os_release_path):      with open(os_release_path,  r , encoding= utf-8 ) as f:        for line in f:          if line. startswith( ID= ):            distro_id = line. split( = )[1]. strip(). strip(' ')          elif line. startswith( VERSION_ID= ):            version_id = line. split( = )[1]. strip(). strip(' ')    if distro_id ==  ubuntu  and version_id:      # Ubuntu: install HWE kernel      run_command(        [           chroot ,          chroot_dir,           apt-get ,           install ,           -y ,          f linux-generic-hwe-{version_id} ,        ],        check=False,      )    elif distro_id ==  debian :      # Debian: install standard kernel metapackage      arch = platform. machine()      if arch ==  x86_64 :        kernel_pkg =  linux-image-amd64       elif arch ==  aarch64 :        kernel_pkg =  linux-image-arm64       else:        kernel_pkg =  linux-image-  + arch      run_command(        [ chroot , chroot_dir,  apt-get ,  install ,  -y , kernel_pkg],        check=False,      )  except Exception as e:    LOG. warning( Error installing kernel: %s , e)  # Clean up removed packages  try:    result = run_command([ chroot , chroot_dir,  dpkg ,  -l ], check=False)    rc_packages = []    for line in result. stdout. split( \n ):      if line. startswith( rc  ):        parts = line. split()        if len(parts) &gt;= 2:          rc_packages. append(parts[1])    if rc_packages:      run_command(        [ chroot , chroot_dir,  apt-get ,  purge ,  -y ] + rc_packages,        check=False,      )  except Exception:    pass  run_command(    [ chroot , chroot_dir,  apt-get ,  autoremove ,  --purge ,  -y ], check=False  )  LOG. info( Package installation complete )```write_hosts_fileWrites `/etc/hosts` with localhost and IPv6 entries. ```pythondef write_hosts_file(mount_point, hostname):     Write /etc/hosts file with proper entries.   :param mount_point: Root mount point  :param hostname: System hostname       LOG. info( Writing /etc/hosts file )  hosts_path = os. path. join(mount_point,  etc ,  hosts )  with open(hosts_path,  w , encoding= utf-8 ) as f:    f. write(f 127. 0. 0. 1\tlocalhost\t{hostname}\n )    f. write( \n )    f. write( # The following lines are desirable for IPv6 capable hosts\n )    f. write( ::1\tip6-localhost\tip6-loopback\n )    f. write( fe00::0\tip6-localnet\n )    f. write( ff00::0\tip6-mcastprefix\n )    f. write( ff02::1\tip6-allnodes\n )    f. write( ff02::2\tip6-allrouters\n )    f. write( ff02::3\tip6-allhosts\n )  LOG. info( /etc/hosts written with hostname: %s , hostname)```configure_cloud_initConfigures cloud-init NoCloud datasource with metadata, userdata, andnetwork config from configdrive. ```pythondef configure_cloud_init(mount_point, configdrive_data):     Configure cloud-init with configdrive data.   :param mount_point: Root mount point  :param configdrive_data: Configdrive dictionary       LOG. info( Configuring cloud-init )  cloud_init_cfg_dir = os. path. join(mount_point,  etc ,  cloud ,  cloud. cfg. d )  os. makedirs(cloud_init_cfg_dir, exist_ok=True)  nocloud_seed_dir = os. path. join(    mount_point,  var ,  lib ,  cloud ,  seed ,  nocloud-net   )  os. makedirs(nocloud_seed_dir, exist_ok=True)  # Write datasource config  datasource_cfg = os. path. join(cloud_init_cfg_dir,  99-nocloud-seed. cfg )  with open(datasource_cfg,  w , encoding= utf-8 ) as f:    f. write(         datasource_list: [ NoCloud, None ]datasource: NoCloud:  seedfrom: file:///var/lib/cloud/seed/nocloud-net/       )  # Write meta-data  meta_data = configdrive_data. get( meta_data , {})  meta_data_path = os. path. join(nocloud_seed_dir,  meta-data )  with open(meta_data_path,  w , encoding= utf-8 ) as f:    yaml. safe_dump(meta_data, f, default_flow_style=False)  # Write user-data  user_data = configdrive_data. get( user_data ,   )  user_data_path = os. path. join(nocloud_seed_dir,  user-data )  with open(user_data_path,  w , encoding= utf-8 ) as f:    f. write(user_data if user_data else   )  # Write network-config if present  network_data = configdrive_data. get( network_data , {})  if network_data:    network_config_path = os. path. join(nocloud_seed_dir,  network-config )    with open(network_config_path,  w , encoding= utf-8 ) as f:      yaml. safe_dump(network_data, f, default_flow_style=False)  # Set permissions  for filename in os. listdir(nocloud_seed_dir):    filepath = os. path. join(nocloud_seed_dir, filename)    os. chmod(filepath, 0o600)  LOG. info( Cloud-init configuration complete )```write_fstabWrites `/etc/fstab` with root and EFI entries, plus second EFI for RAID. ```pythondef write_fstab(mount_point, root_label, boot_label, is_raid, boot_label2=None):     Write /etc/fstab.   :param mount_point: Root mount point  :param root_label: Root partition label  :param boot_label: EFI partition label  :param is_raid: Whether RAID is configured  :param boot_label2: Second EFI partition label       LOG. info( Writing fstab )  fstab_path = os. path. join(mount_point,  etc ,  fstab )  with open(fstab_path,  w , encoding= utf-8 ) as f:    f. write(f LABEL={root_label}\t/\text4\terrors=remount-ro\t0\t1\n )    f. write(f LABEL={boot_label}\t/boot/efi\tvfat\tumask=0077,nofail\t0\t1\n )    if is_raid and boot_label2:      f. write(        f LABEL={boot_label2}\t/boot/efi2\tvfat\t         f umask=0077,nofail,noauto\t0\t2\n       )  LOG. info( fstab written )```write_mdadm_confWrites `/etc/mdadm/mdadm. conf` with RAID array configuration. ```pythondef write_mdadm_conf(mount_point):     Write mdadm configuration.   :param mount_point: Root mount point       LOG. info( Writing mdadm. conf )  mdadm_dir = os. path. join(mount_point,  etc ,  mdadm )  os. makedirs(mdadm_dir, exist_ok=True)  mdadm_conf_path = os. path. join(mdadm_dir,  mdadm. conf )  with open(mdadm_conf_path,  w , encoding= utf-8 ) as f:    f. write( HOMEHOST \n )    f. write( MAILADDR root\n )  # Append ARRAY lines from mdadm --detail --scan  result = run_command([ mdadm ,  --detail ,  --scan ,  --verbose ])  with open(mdadm_conf_path,  a , encoding= utf-8 ) as f:    for line in result. stdout. split( \n ):      if line. startswith( ARRAY ):        f. write(line +  \n )  LOG. info( mdadm. conf written )```&lt;/details&gt;configure_initramfsConfigures initramfs-tools to include LVM and RAID modules. ```pythondef configure_initramfs(chroot_dir, is_raid):     Configure initramfs-tools for LVM and optionally RAID.   This ensures initramfs includes LVM modules.   :param chroot_dir: Chroot directory path  :param is_raid: Whether RAID is configured       LOG. info( Configuring initramfs-tools )  initramfs_conf_dir = os. path. join(chroot_dir,  etc ,  initramfs-tools ,  conf. d )  os. makedirs(initramfs_conf_dir, exist_ok=True)  # Disable resume (no swap partition)  resume_conf = os. path. join(initramfs_conf_dir,  resume )  with open(resume_conf,  w , encoding= utf-8 ) as f:    f. write( RESUME=none\n )  # Force LVM inclusion in initramfs  # This is needed because during chroot, LVM volumes may not be  # detected by the initramfs-tools hooks  initramfs_conf = os. path. join(    chroot_dir,  etc ,  initramfs-tools ,  initramfs. conf   )  if os. path. exists(initramfs_conf):    with open(initramfs_conf,  r , encoding= utf-8 ) as f:      content = f. read()    # Set MODULES to  most  to include storage drivers    content = re. sub(r ^MODULES=. *$ ,  MODULES=most , content, flags=re. MULTILINE)    with open(initramfs_conf,  w , encoding= utf-8 ) as f:      f. write(content)  # Add LVM modules explicitly  modules_file = os. path. join(chroot_dir,  etc ,  initramfs-tools ,  modules )  lvm_modules = [ dm-mod ,  dm-snapshot ,  dm-mirror ,  dm-zero ]  if is_raid:    lvm_modules. extend([ raid1 ,  md-mod ])  existing_modules = set()  if os. path. exists(modules_file):    with open(modules_file,  r , encoding= utf-8 ) as f:      for line in f:        line = line. strip()        if line and not line. startswith( # ):          existing_modules. add(line)  with open(modules_file,  a , encoding= utf-8 ) as f:    for module in lvm_modules:      if module not in existing_modules:        f. write(f {module}\n )  LOG. info( initramfs-tools configuration complete )```setup_grub_defaultsConfigures `/etc/default/grub` with root device and RAID options. ```pythondef setup_grub_defaults(chroot_dir, root_label, is_raid):     Configure GRUB defaults.   :param chroot_dir: Chroot directory path  :param root_label: Root partition label  :param is_raid: Whether RAID is configured       LOG. info( Setting up GRUB defaults )  grub_default = os. path. join(chroot_dir,  etc ,  default ,  grub )  with open(grub_default,  r , encoding= utf-8 ) as f:    content = f. read()  # Build GRUB_CMDLINE_LINUX  cmdline = f root=LABEL={root_label}   if is_raid:    cmdline +=   rd. auto=1   # Update GRUB_CMDLINE_LINUX  content = re. sub(    r ^#*\s*GRUB_CMDLINE_LINUX=. *$ ,    f'GRUB_CMDLINE_LINUX= {cmdline} ',    content,    flags=re. MULTILINE,  )  # Update GRUB_DISABLE_LINUX_UUID  if  GRUB_DISABLE_LINUX_UUID=  in content:    content = re. sub(      r ^#*\s*GRUB_DISABLE_LINUX_UUID=. *$ ,       GRUB_DISABLE_LINUX_UUID=true ,      content,      flags=re. MULTILINE,    )  else:    content +=  \nGRUB_DISABLE_LINUX_UUID=true\n   # Add rootdelay for RAID  if is_raid:    if  GRUB_CMDLINE_LINUX_DEFAULT=  in content:      if  rootdelay=  not in content:        content = re. sub(          r'^(#*\s*GRUB_CMDLINE_LINUX_DEFAULT= [^ ]*)',          r \1 rootdelay=10 ,          content,          flags=re. MULTILINE,        )    else:      content += '\nGRUB_CMDLINE_LINUX_DEFAULT= rootdelay=10 \n'  with open(grub_default,  w , encoding= utf-8 ) as f:    f. write(content)  LOG. info( GRUB defaults configured )```setup_grub_efi_syncCreates GRUB hook script to sync EFI partitions for RAID redundancy. ```pythondef setup_grub_efi_sync(chroot_dir, boot_label2):     Set up GRUB hook to sync EFI partitions for RAID.   :param chroot_dir: Chroot directory path  :param boot_label2: Second EFI partition label       LOG. info( Setting up GRUB EFI sync hook )  grub_hook = os. path. join(chroot_dir,  etc ,  grub. d ,  90_copy_to_boot_efi2 )  with open(grub_hook,  w , encoding= utf-8 ) as f:    f. write(      f   #!/bin/sh# Sync GRUB updates to both EFI partitions for RAID redundancyset -eif mountpoint --quiet --nofollow /boot/efi; then  mount LABEL={boot_label2} /boot/efi2 || :  rsync --times --recursive --delete /boot/efi/ /boot/efi2/  umount -l /boot/efi2fiexit 0       )  os. chmod(grub_hook, 0o755) # nosec B103  LOG. info( GRUB EFI sync hook created )```class DebOCIEFILVMHardwareManagerMain hardware manager class implementing the `deb_oci_efi_lvm` deploy step. Orchestrates the full deployment workflow. ```pythonclass DebOCIEFILVMHardwareManager(hardware. HardwareManager):     Hardware manager for OCI EFI LVM RAID deployment.      HARDWARE_MANAGER_NAME =  DebOCIEFILVMHardwareManager   HARDWARE_MANAGER_VERSION =  1. 0   def evaluate_hardware_support(self):    LOG. info( DebOCIEFILVMHardwareManager:    evaluate_hardware_support called )    return hardware. HardwareSupport. SERVICE_PROVIDER  def get_deploy_steps(self, node, ports):    LOG. info( DebOCIEFILVMHardwareManager: get_deploy_steps called )    return [      {         step :  deb_oci_efi_lvm ,         priority : 0,         interface :  deploy ,         reboot_requested : False,         argsinfo : {},      },    ]  def deb_oci_efi_lvm(self, node, ports):       Deploy Debian-based OCI image with EFI, LVM, and optional RAID.     :param node: Node dictionary containing deployment configuration    :param ports: List of port dictionaries for the node    :raises: ValueError if configuration is invalid    :raises: RuntimeError if deployment fails           LOG. info( DebOCIEFILVMHardwareManager:    deb_oci_efi_lvm called )    LOG. info( DebOCIEFILVMHardwareManager: node: %s , node)    LOG. info( DebOCIEFILVMHardwareManager: ports: %s , ports)    if not is_efi_system():      raise RuntimeError(         This deployment requires EFI boot mode.           System is not booted in EFI mode.        )    try:      # Extract configuration from node      configdrive_data = get_configdrive_data(node)      root_device_hints = get_root_device_hints(node, configdrive_data)      resolved_devices = resolve_root_devices(root_device_hints)      meta_data = configdrive_data. get( meta_data , {})      metal3_name = meta_data. get( metal3-name )      root_device_path = resolved_devices[0]      second_device = resolved_devices[1] if len(resolved_devices) &gt; 1 else None      LOG. info(         DebOCIEFILVMHardwareManager:    root_device_path: %s , root_device_path      )      if second_device:        LOG. info(           DebOCIEFILVMHardwareManager:    second_device: %s (RAID1) ,          second_device,        )      # Get OCI image and architecture-specific configuration      oci_image = get_oci_image(node, configdrive_data)      arch_config = get_architecture_config(oci_image)      LOG. info(         DebOCIEFILVMHardwareManager:    architecture config: %s , arch_config      )      # Get disk wipe mode      is_raid_setup = second_device is not None      wipe_mode = get_disk_wipe_mode(configdrive_data, is_raid_setup)      # Clean devices based on wipe mode      if wipe_mode ==  all :        LOG. info( Cleaning all block devices (wipe_mode: all) )        clean_all_devices()        wait_for_device(root_device_path)        if second_device:          wait_for_device(second_device)      else: # wipe_mode == 'target'        LOG. info( Cleaning only target device(s) (wipe_mode: target) )        wait_for_device(root_device_path)        clean_device(root_device_path)        if second_device:          wait_for_device(second_device)          clean_device(second_device)      # Partition disk      is_raid, pv_device = partition_disk(        root_device_path,        VG_NAME,        LV_NAME,        second_device=second_device,        raid_device=RAID_DEVICE,        homehost=metal3_name,      )      # Get partition paths      efi_partition = get_partition_path(root_device_path, 1)      second_efi_partition = None      if is_raid and second_device:        second_efi_partition = get_partition_path(second_device, 1)      root_lv_path = f /dev/{VG_NAME}/{LV_NAME}       # Create filesystems      create_filesystems(        efi_partition,        root_lv_path,        boot_label=BOOT_FS_LABEL,        root_label=ROOT_FS_LABEL,        second_efi_partition=second_efi_partition,        boot_label2=BOOT_FS_LABEL2,      )      # Mount root filesystem      root_mount = tempfile. mkdtemp()      run_command([ mount , root_lv_path, root_mount])      try:        # Extract OCI image rootfs        extract_oci_image(          arch_config[ oci_image ], arch_config[ oci_platform ], root_mount        )        # Mount EFI partition        efi_mount = os. path. join(root_mount,  boot ,  efi )        os. makedirs(efi_mount, exist_ok=True)        run_command([ mount , efi_partition, efi_mount])        try:          # Set up chroot          setup_chroot(root_mount)          try:            # Install packages            install_packages(root_mount, arch_config[ grub_packages ])            # Configure cloud-init            configure_cloud_init(root_mount, configdrive_data)            # Write /etc/hosts            write_hosts_file(root_mount, metal3_name)            # Write fstab            write_fstab(              root_mount,              ROOT_FS_LABEL,              BOOT_FS_LABEL,              is_raid,              BOOT_FS_LABEL2,            )            # Configure GRUB            setup_grub_defaults(root_mount, ROOT_FS_LABEL, is_raid)            # RAID-specific configuration            if is_raid:              write_mdadm_conf(root_mount)              setup_grub_efi_sync(root_mount, BOOT_FS_LABEL2)              efi2_mount = os. path. join(root_mount,  boot ,  efi2 )              os. makedirs(efi2_mount, exist_ok=True)            # Install GRUB to EFI            run_command(              [                 chroot ,                root_mount,                 grub-install ,                f'--target={arch_config[ uefi_target ]}',                 --efi-directory=/boot/efi ,                 --bootloader-id=ubuntu ,                 --recheck ,              ]            )            # Configure initramfs for LVM (required for Debian)            configure_initramfs(root_mount, is_raid)            # Update GRUB config and initramfs            run_command([ chroot , root_mount,  update-grub ])            run_command(              [                 chroot ,                root_mount,                 update-initramfs ,                 -u ,                 -k ,                 all ,              ]            )            # Install GRUB to second EFI partition for RAID            if is_raid and second_efi_partition:              efi2_mount = os. path. join(root_mount,  boot ,  efi2 )              try:                run_command([ mount , second_efi_partition, efi2_mount])                run_command(                  [                     rsync ,                     -a ,                    f {root_mount}/boot/efi/ ,                    f {root_mount}/boot/efi2/ ,                  ]                )                run_command(                  [                     chroot ,                    root_mount,                     grub-install ,                    f'--target={arch_config[ uefi_target ]}',                     --efi-directory=/boot/efi2 ,                     --bootloader-id=ubuntu ,                     --recheck ,                  ]                )              except Exception as e:                LOG. warning(                   Error installing GRUB to second EFI: %s , e                )              finally:                result = run_command(                  [ mountpoint ,  -q , efi2_mount], check=False                )                if result. returncode == 0:                  run_command([ umount ,  -l , efi2_mount])          finally:            teardown_chroot(root_mount)        finally:          # Unmount EFI partition          result = run_command([ mountpoint ,  -q , efi_mount], check=False)          if result. returncode == 0:            run_command([ umount ,  -l , efi_mount])      finally:        # Unmount root filesystem        result = run_command([ mountpoint ,  -q , root_mount], check=False)        if result. returncode == 0:          run_command([ umount ,  -l , root_mount])        # Clean up temporary directories        if root_mount and os. path. exists(root_mount):          try:            os. rmdir(root_mount)            LOG. debug( Cleaned up root mount directory: %s , root_mount)          except Exception as e:            LOG. warning(               Failed to clean up root mount dir %s: %s , root_mount, e            )      LOG. info(         DebOCIEFILVMHardwareManager:    deb_oci_efi_lvm completed successfully       )    except Exception as e:      LOG. error( DebOCIEFILVMHardwareManager:    deb_oci_efi_lvm failed: %s , e)      raise    finally:      # Wait for interactive users to logout      if has_interactive_users():        LOG. info(           DebOCIEFILVMHardwareManager:             interactive users detected, waiting for logout         )        while has_interactive_users():          LOG. info(             DebOCIEFILVMHardwareManager:               users still logged in, checking again               in 60 seconds           )          time. sleep(60)        LOG. info(           DebOCIEFILVMHardwareManager:    all interactive users logged out         )```## Supported OCI ImagesThe hardware manager works with any Debian-based OCI image that has afunctional `apt` package manager. OCI multi-arch images are supported. Tested images include:- `ubuntu:24. 04`- `debian:13`The key benefit of this approach is the ability to create custom OCIimages with your specific OS configuration, packages, and settings. You can build and maintain your own Docker images and use them directlyas the root filesystem for bare metal deployments. The deployment processinstalls additional packages (kernel, GRUB, cloud-init) on top of thebase image. ## Debugging DeploymentsIf a deployment fails, you can connect to the server via BMC consoleduring the IPA phase. The hardware manager includes a feature thatwaits for interactive users to log out before completing, allowingyou to inspect the system state. ## Limitations and ConsiderationsThe following are limitations of this specific `deb_oci_efi_lvm`implementation, not of Metal3's custom deploy mechanism itself. Thecustom deploy framework is flexible and allows implementing alternativehardware managers with different capabilities. 1. **EFI only** - This implementation requires UEFI boot mode1. **Debian-based only** - The package installation assumes `apt` is  available1. **Network required** - The IPA needs network access to pull OCI  images from registries and install packages in target system1. **Root device hints** - Only `serial` and `wwn` hints are supported  for disk selection## ConclusionThe `deb_oci_efi_lvm` hardware manager demonstrates how custom deploysteps can extend Ironic's capabilities beyond traditional image-baseddeployments. The source code and GitHub Actions for building custom IPAimages are available at[s3rj1k/ironic-python-agent](https://github. com/s3rj1k/ironic-python-agent/tree/custom_deploy). ## Future ImprovementsA potential enhancement could add native support for converting OpenStack`network_data. json` format to cloud-init v1 network configuration duringdeployment. ## References- [Integrating CoreOS Installer with Ironic](https://owlet. today/posts/integrating-coreos-installer-with-ironic/) - Dmitry Tantsur's original blog post on custom deploy steps- [Ironic Deploy Steps Documentation](https://docs. openstack. org/ironic/latest/contributor/deploy-steps. html)- [Metal3 Custom Deploy Steps Design](https://github. com/metal3-io/metal3-docs/blob/main/design/baremetal-operator/deploy-steps. md)- [OpenShift CoreOS Install Hardware Manager](https://github. com/openshift/ironic-agent-image/blob/main/hardware_manager/ironic_coreos_install. py)"
    }, {
    "id": 1,
    "url": "/blog/2025/08/27/metal3-becomes-cncf-incubating-project.html",
    "title": "Metal3.io Becomes a CNCF Incubating Project",
    "author" : "Honza Pokorný",
    "tags" : "metal3, cncf, community, announcement",
    "body": "We are pleased to share some incredible news with our community! The CNCFTechnical Oversight Committee has officially voted to accept Metal3 as anincubating project. This milestone represents years of hard work, collaboration,and innovation, and we couldn’t be more excited about what lies ahead! Our Journey from Sandbox to Incubation: What started as a collaboration between Red Hat and Ericsson in 2019 hasblossomed into something truly special. When we joined the CNCF sandbox inSeptember 2020, we knew we had something powerful: a way to make bare metalinfrastructure as Kubernetes-native as any cloud platform. Today, that visionhas grown far beyond our initial dreams. The Numbers Tell Our Story: We’re incredibly proud of what our community has accomplished together:  57 active contributing organizations from around the globe 186 amazing contributors who’ve shaped our project 8,368 merged pull requests representing countless hours of collaboration 1,523 GitHub stars from supporters worldwide 187 releases of continuous improvementBut beyond the numbers, what makes us truly happy is seeing organizations likeFujitsu, Ikea, SUSE, Ericsson, and Red Hat successfully deploying Metal3 inproduction environments. What Makes Us Proud: Metal3 has evolved into so much more than a bare metal provisioning tool. We’vebuilt a comprehensive platform that:  Seamlessly integrates with Cluster API for Kubernetes lifecycle management Provides robust IP address management through our IPAM component Offers enterprise-grade security with automated vulnerability scanning Supports firmware management and day-2 operations Runs entirely on Kubernetes using native APIsOur new Ironic Standalone Operator has revolutionized deployment simplicity,making it easier than ever for teams to get started with Metal3. Looking Forward with Excitement: The roadmap ahead fills us with anticipation! In 2025, we’re planning:  Enhanced multi-tenancy support ARM architecture support beyond x86_64 Improved DHCP-less provisioning capabilities New API revisions across our components Continued simplification of the user experienceThe Adventure Continues: Joining CNCF incubation isn’t the end of our journey – it’s an exciting newchapter! With the foundation’s support and our amazing community behind us,we’re more energized than ever to push the boundaries of what’s possible withbare metal Kubernetes infrastructure. Thank you for being part of this incredible adventure. Here’s to making baremetal as cloud-native as the clouds themselves! "
    }, {
    "id": 2,
    "url": "/blog/2024/12/13/Introducing-BMO-E2E.html",
    "title": "Introducing Baremetal Operator end-to-end test suite",
    "author" : "Lennart Jern",
    "tags" : "metal3, cluster API, provider, edge",
    "body": "In the beginning, there wasmetal3-dev-env. It could set up avirtualized “baremetal” lab and test all the components together. As Metal3matured, it grew in complexity and capabilities, with release branches, APIversions, etc. Metal3-dev-env did everything from cloning the repositories andbuilding the container images, to deploying the controllers and running tests,on top of setting up the virtual machines and the networks, of course. Needlessto say, it became hard to understand and easy to misuse. We tried reducing the scope a bit by introducing end to end tests directly inthe Cluster API providerMetal3(CAPM3). However, metal3-dev-env was still very much entangled with CAPM3. Itwas at this point that I got tired of trying to gradually fix it and took theinitiative to start from scratch with end to end tests in Baremetal Operator(BMO) instead. Up until that point, we had been testing BMO through CAPM3 and the cluster APIflow. It worked, but it was very inefficient. From the perspective on theBaremetal Operator, a test could look something like this:  Register 5 BareMetalHosts Inspect the 5 BareMetalHosts Provision the 5 BareMetalHosts all with the same image Deprovision 1 BareMetalHost Provision it again with another image Deprovision another BareMetalHost Provision it again with the other image Continue in the same way with the rest of the BareMetalHosts… Deprovision all BareMetalHostsAs you can see, it is very repetitive, constantly doing the same thing again andagain. As a consequence of this and the complexity of metal3-dev-env, it wasquite an effort to thoroughly test something related to BMO code. I wasconstantly questioning myself and the test environment. “Is it testing the codeI wrote?” “Is it doing the relevant scenario?” “Is the configuration correct?” Baremetal Operator end to end tests are born: Sometimes it is easier to start from scratch, so this is what wedid. The BaremetalOperator end to end tests started out as a small script that only set upminikube, some VMs and a baseboard management controller (BMC) emulator. Thegoal was simple: do the minimum required to simulate a baremetal lab. From this,it was quite easy to build a test module that was responsible for deploying thenecessary controllers and running some tests. Notice the separation of concerns here! The test module expects a baremetal labenvironment to be already existing and the script that sets up the environmentis not involved in anyway with the tests or deployment of the controllers. Thisdesign is deliberate, with a clear goal that the test module should be usefulacross multiple environments. It should be possible to run the test suiteagainst real baremetal labs with multiple different configurations. I am hopingthat we will get a chance next year to try it for real in a baremetal lab. How does it work?: The flexibility of the end to end module is possible through a configurationfile. It can be used to configure everything from the image URL and checksum tothe timeout limits. Since Ironic can be deployed in many different ways, it wasalso necessary to make this flexible. The user can optionally set up Ironicbefore the test, or provide a kustomization that will be applied automatically. A separate configuration file declares the BMCs that should be used in thetests. The configuration that we use inCIshows how these files look like. As a proof of concept for the flexibility ofthe tests, it can be noted that we already have two different configurations. One for running the tests with Ironic and one for running them with BMO infixture mode. The first is the “normal” mode, the latter means that BMO does notcommunicate with Ironic at all, it just pretends. While that obviously isn’tuseful for any thorough tests, it still provides a quick and light weight testsuite, and ensures that we do not get too attached to one particularconfiguration. The test suite itself is made with Ginkgo and Gomega. Instead of building a longchain of checks and scenarios we have attempted to do small, isolated tests. This makes it possible to run multiple in parallel and shorten the test suiteduration, as well as easily identify where exactly errors occur. In order toaccomplish this, we make heavy use of the statusannotation so that we can skipinspection when possible. Where are we today?: It is already several months since we switched over to the BMO e2e test suite asthe primary, and only required tests for pull requests in the BMO repository. Werun the end to end test suite as GitHubworkflowsand it covers more than the metal3-dev-env and CAPM3 based tests from BMOperspective. That does not mean that we are done though. At the time of writing,there are several GitHubissues for improving andextending the tests. The progress has significantly slowed though, as canperhaps be expected, since the most essentials parts were implemented. The future: In the future we hope to make the BMO end to end module and tooling more usefulfor local development and testing. It should be easy to spin up a minimalenvironment and test specific scenarios, also using Tilt. Additionally, we wantto “rebase” the CAPM3 end to end tests on this work. It should be possible toreuse the code and tooling for simulating a baremetal lab so that we can get ridof the entanglement with metal3-dev-env. "
    }, {
    "id": 3,
    "url": "/blog/2024/10/24/Scaling-Kubernetes-with-Metal3-on-Fake-Node.html",
    "title": "Scaling Kubernetes with Metal3: Simulating 1000 Clusters with Fake Ironic Agents",
    "author" : "Huy Mai",
    "tags" : "metal3, cluster API, ironic, baremetal, scaling",
    "body": "If you’ve ever tried scaling out Kubernetes clusters in a bare-metalenvironment, you’ll know that large-scale testing comes with serious challenges. Most of us don’t have access to enough physical servers—or even virtualmachines—to simulate the kinds of large-scale environments we need for stresstesting, especially when deploying hundreds or thousands of clusters. That’s where this experiment comes in. Using Metal3, we simulated a massive environment—provisioning 1000 single-nodeKubernetes clusters—without any actual hardware. The trick? A combination ofFake Ironic Python Agents (IPA) and Fake Kubernetes API servers. These toolsallowed us to run an entirely realistic Metal3 provisioning workflow whilesimulating thousands of nodes and clusters, all without needing a single realmachine. The motivation behind this was simple: to create a scalable testing environmentthat lets us validate Metal3’s performance, workflow, and reliability withoutneeding an expensive hardware lab or virtual machine fleet. By simulating nodesand clusters, we could push the limits of Metal3’s provisioning processcost-effectively and time-efficiently. In this post, I’ll explain exactly how it all works, from setting up multipleIronic services to faking hardware nodes and clusters and sharing the lessonslearned. Whether you’re a Metal3 user or just curious about how to testlarge-scale Kubernetes environments, it’ll surely be a good read. Let’s getstarted! Prerequisites &amp; Setup: Before diving into the fun stuff, let’s ensure we’re on the same page. You don’tneed to be a Metal3 expert to follow along, but having a bit of background willhelp! What You’ll Need to Know: Let’s start by ensuring you’re familiar with some essential tools and conceptsthat power Metal3 workflow. If you’re confident in your Metal3 skills, pleasefeel free to skip this part. A typical Metal3 Workflow: The following diagram explains a typical Metal3 workflow. We will, then, go intodetails of every component.  Cluster API (CAPI): CAPI is a project that simplifies the deployment and management of Kubernetesclusters. It provides a consistent way to create, update, and scale clustersthrough Kubernetes-native APIs. The magic of CAPI is that it abstracts away manyof the underlying details so that you can manage clusters on different platforms(cloud, bare metal, etc. ) in a unified way. Cluster API Provider Metal3 (CAPM3): CAPM3 extends CAPI to work specifically with Metal3 environments. It connectsthe dots between CAPI, BMO, and Ironic, allowing Kubernetes clusters to bedeployed on bare-metal infrastructure. It handles tasks like provisioning newnodes, registering them with Kubernetes, and scaling clusters. Bare Metal Operator (BMO): BMO is a controller that runs inside a Kubernetes cluster and works alongsideIronic to manage bare-metal infrastructure. It automates the lifecycle ofbare-metal hosts, managing things like registering new hosts, powering them onor off, and monitoring their status. Bare Metal Host (BMH)A BMH is the Kubernetes representation of a bare-metal node. It containsinformation about how to reach the node it represents, and BMO monitors itsdesired state closely. When BMO notices that a BMH object state is requested tochange (either by a human user or CAPM3), it will decide what needs to be doneand tell Ironic. Ironic &amp; Ironic Python Agent (IPA):  Ironic is a bare-metal provisioning tool that handles tasks like bootingservers, deploying bootable media (e. g. , operating systems) to disk, andconfiguring hardware. Think of Ironic as the piece of software that managesactual physical servers. In a Metal3 workflow, Ironic receives orders from BMOand translates them into actionable steps. Ironic has multiple ways to interactwith the machines, and one of them is the so-called “ agent-based direct deploy”method, which is commonly used by BMO. The agent mentioned is called IronicPython Agent (IPA), which is a piece of software that runs on each bare-metalnode and carries out Ironic’s instructions. It interacts with the hardwaredirectly, like wiping disks, configuring networks, and handling boot processes. In a typical Metal3 workflow, BMO reads the desired state of the node from theBMH object, translates the Kubernetes reconciling logic to concrete actions, andforwards them to Ironic, which, as part of the provisioning process, tells IPAthe exact steps it needs to perform to get the nodes to desired states. Duringthe first boot after node image installation, Kubernetes components will beinstalled on the nodes by cloud-init, and once the process succeeds, Ironicand IPA finish the provisioning process, and CAPI and CAPM3 will verify thehealth of the newly provisioned Kubernetes cluster(s). The Experiment: Simulating 1000 Kubernetes Clusters: This experiment aimed to push Metal3 to simulate 1000 single-node Kubernetesclusters on fake hardware. Instead of provisioning real machines, we used FakeIronic Python Agents (Fake IP) and Fake Kubernetes API Servers (FKAS) tosimulate nodes and control planes, respectively. This setup allowed us to test amassive environment without the need for physical infrastructure. Since our goal is to verify the Metal3 limit, our setup will let all the Metal3components (except for IPA, which runs inside and will be scaled with the nodes)to keep working as they do in a typical workflow. In fact, none of thecomponents should be aware that they are running with fake hardware. Take the figure we had earlier as a base, here is the revised workflow with fakenodes.  Step 1: Setting Up the environment: As you may have known, a typical Metal3 workflow requires several components:bootstrap Kubernetes cluster, possible external networks, bare-metal nodes, etc. As we are working on simulating the environment, we will start with a newlyspawned Ubuntu VM, create a cluster with minikube, add networks with libvirt,and so on (If you’re familiar with Metal3’s dev-env, this step is similar towhat script01,02and a part of03do). We will not discuss this part, but you can find the related setup fromthisscriptif interested. Note: If you intend to follow along, note that going to 1000 nodes requiresa large environment and will take a long time. In our setup, we had a VM with 24cores and 32GB of RAM, of which we assigned 14 cores and 20GB of RAM to theminikube VM, and the process took roughly 48 hours. If your environment is lesspowerful, consider reducing the nodes you want to provision. Something like 100nodes will require minimal resources and time while still being impressive. Step 2: Install BMO and Ironic: In Metal3’s typical workflow, we usually rely on Kustomize to install Ironic andBMO. Kustomize helps us define configurations for Kubernetes resources, makingit easier to customize and deploy services. However, our current Kustomizeoverlay for Metal3 configures only a single Ironic instance. This setup workswell for smaller environments, but it becomes a bottleneck when scaling up andhandling thousands of nodes. That’s where Ironic’s special mode comes into play. Ironic has the abilityto run multiple Ironic conductors while sharing the same database. The bestpart? Workload balancing between conductors happens automatically, which meansthat no matter which Ironic conductor receives a request, the load is evenlydistributed across all conductors, ensuring efficient provisioning. Achievingthis requires separating ironic conductor from the database, which allows usto scale up the conductor part. Each conductor will have its ownPROVISIONING_IP, hence the need to have a specialized configMap. We used Helm for this purpose. In our Helm chart, theIronic conductor container and HTTP server (httpd) container areseparated into a new pod, and the rest of the ironic package (mostlyMariaDB-ironic database) stays in another pod. A list of PROVISIONING_IPs isprovided by the chart’s values. yaml, and for each IP, an ironic conductorpod is created, along with a config map whose values are rendered with the IP’svalue. This way, we can dynamically scale up/down ironic (or, more specifically,ironic conductors) by simply adding/removing ips. Another piece of information that we need to keep in mind is the ipa-downloadercontainer. In our current metal3-dev-env, the IPA-downloader container runs asan init Container for ironic, and its job is to download the IPA image to aPersistent Volume. This image contains the Ironic Python Agent, and it isassumed to exist by Ironic. For the multiple-conductor scenario, running thesame init-container for all the conductors, at the same time, could be slowand/or fail due to network issue. To make it work, we made a small “hack” in thechart: the ipa image will exist in a specific location inside the minikube host,and all the conductor pods will mount to that same location. In production, amore throughout solution might be to keep the IPA-downloader as aninit-container, but points the image to the local image server, which we set upin the previous step. BMO, on the other hand, still works well with kustomize, as we do not need toscale it. As with typical metal3 workflow, BMO and Ironic must share someauthentication to work with TLS. You can check out the full Ironic helm charthere. Step 3: Creating Fake Nodes with Fake Ironic Python Agents: As we mentioned at the beginning, instead of using real hardware, we will use anew tool called Fake Ironic Python Agent, or Fake IPA to simulate thenodes. Setting up Fake IPA is relatively straightforward, as Fake IPA runs ascontainers on the host machine, but first, we need to create the list of “nodes”that we will use (Fake IPA requires to have that list ready when it starts). A“node” typically looks like this {    uuid : $uuid,    name : $node_name,    power_state :  Off ,    external_notifier :  True ,    nics : [    { mac : $macaddr,  ip :  192. 168. 0. 100 }   ],}All of the variables (uuid, node_name, macaddress) can be dynamicallygenerated in any way you want (check thisscriptout if you need an idea). Still, we must store this information to generate theBMH objects that match those “nodes. ” The ip is, on the other hand, notessential. It could be anything. We must also start up the sushy-tools container in this step. It is a toolthat simulates the Baseboard ManagementControllerfor non-bare-metal hardware, and we have been using it extensively inside Metal3dev-env and CI to control and provision VMs as if they are bare-metal nodes. Ina bare-metal setup, Ironic will ask the BMC to install IPA on the node, and inour setup, sushy-tools will get the same request, but it will simply fakethe installation and, in the end, forward Ironic traffic to the Fake IPAcontainer. Another piece of information we will need is the cert that Ironic will usein its communication with IPA. IPA is supposed to get it from Ironic, but asFake IPA cannot do that (at least not yet), we must get the cert and provideit in Fake IPA config. mkdir certkubectl get secret -n baremetal-operator-system ironic-cert -o json \ -o=jsonpath= {. data. ca\. crt}  | base64 -d &gt;cert/ironic-ca. crtAlso note that one set of sushy-tools and Fake IPA containers won’t beenough to provision 1000 nodes. Just like Ironic, they need to be scaled upextensively (about 20-30 pairs will be sufficient for 1000 nodes), butfortunately, the scaling is straightforward: We just need to give them differentports. Both of these components also require a Python-based config file. Forconvenience, in this setup, we create a big file and provide it to both of them,using the following shell script: for i in $(seq 1  $N_SUSHY ); do container_conf_dir= $SUSHY_CONF_DIR/sushy-$i  # Use round-robin to choose fake-ipa and sushy-tools containers for the node fake_ipa_port=$((9901 + (($i % ${N_FAKE_IPA:-1})))) sushy_tools_port=$((8000 + i)) ports+=(${sushy_tools_port}) # This is only so that we have the list of the needed ports for other # purposes, like configuring the firewalls.  ports+=(${fake_ipa_port}) mkdir -p  ${container_conf_dir}  # Generate the htpasswd file, which is required by sushy-tools cat &lt;&lt;'EOF' &gt; ${container_conf_dir} /htpasswdadmin:$2b$12$/dVOBNatORwKpF. ss99KB. vESjfyONOxyH. UgRwNyZi1Xs/W2pGVSEOF # Set configuration options cat &lt;&lt;EOF &gt; ${container_conf_dir} /conf. pyimport collectionsSUSHY_EMULATOR_LIBVIRT_URI =  ${LIBVIRT_URI} SUSHY_EMULATOR_IGNORE_BOOT_DEVICE = FalseSUSHY_EMULATOR_VMEDIA_VERIFY_SSL = FalseSUSHY_EMULATOR_AUTH_FILE =  /root/sushy/htpasswd SUSHY_EMULATOR_FAKE_DRIVER = TrueSUSHY_EMULATOR_LISTEN_PORT =  ${sushy_tools_port} EXTERNAL_NOTIFICATION_URL =  http://${ADVERTISE_HOST}:${fake_ipa_port} FAKE_IPA_API_URL =  ${API_URL} FAKE_IPA_URL =  http://${ADVERTISE_HOST}:${fake_ipa_port} FAKE_IPA_INSPECTION_CALLBACK_URL =  ${CALLBACK_URL} FAKE_IPA_ADVERTISE_ADDRESS_IP =  ${ADVERTISE_HOST} FAKE_IPA_ADVERTISE_ADDRESS_PORT =  ${fake_ipa_port} FAKE_IPA_CAFILE =  /root/cert/ironic-ca. crt SUSHY_FAKE_IPA_LISTEN_IP =  ${ADVERTISE_HOST} SUSHY_FAKE_IPA_LISTEN_PORT =  ${fake_ipa_port} SUSHY_EMULATOR_FAKE_IPA = TrueSUSHY_EMULATOR_FAKE_SYSTEMS = $(cat nodes. json)EOF # Start sushy-tools docker run -d --net host --name  sushy-tools-${i}  \  -v  ${container_conf_dir} :/root/sushy \   ${SUSHY_TOOLS_IMAGE}  # Start fake-ipa docker run \  -d --net host --name fake-ipa-${i} \  -v  ${container_conf_dir} :/app \  -v  $(realpath cert) :/root/cert \   ${FAKEIPA_IMAGE} doneIn this setup, we made it so that all the sushy-tools containers willlisten on the port range running from 8001, 8002,…, while the Fake IPAcontainers have ports 9001, 9002,… Step 4: Add the BMH objects: Now that we have sushy-tools and Fake IPA containers running, we canalready generate the manifest for BMH objects, and apply them to the cluster. ABMH object will look like this ---apiVersion: v1kind: Secretmetadata: name: name-bmc-secret labels:  environment. metal3. io: baremetaltype: Opaquedata: username: YWRtaW4= password: cGFzc3dvcmQ=---apiVersion: metal3. io/v1alpha1kind: BareMetalHostmetadata: name: namespec: online: true bmc:  address: redfish+http://192. 168. 222. 1:{port}/redfish/v1/Systems/{uuid}  credentialsName: name-bmc-secret bootMACAddress: random_mac bootMode: legacyIn this manifest:  name is the node name we generated in the previous step.  uuid is the random uuid we generated for the same node.  random_mac is a random mac address for the boot. It’s NOT the same as theNIC mac address we generated for the node.  port is the listening port on one of the sushy-tools containers wecreated in the previous step. Since every sushy-tools and Fake IPAcontainer has information about ALL the nodes, we can decide what container tolocate the “node”. In general, it’s a good idea to spread them out, so allcontainers are loaded equally. We can now run kubectl apply -f on one (or all of) the BMH manifests. What youexpect to see is that a BMH object is created, and its state will change fromregistering to available after a while. It means ironic acknowledgedthat the node is valid, in good state and ready to be provisioned. Step 5: Deploy the fake nodes to kubernetes clusters: Before provisioning our clusters, let’s init the process, so that we have CAPIand CAPM3 installed clusterctl init --infrastructure=metal3After a while, we should see that CAPI, CAPM3, and IPAM pods become available. In a standard Metal3 workflow, after having the BMH objects in an availablestate, we can provision new Kubernetes clusters with clusterctl. However, withfake nodes, things get a tiny bit more complex. At the end of the provisioningprocess, Cluster API expects that there is a new kubernetes API servercreated for the new cluster, from which it will check if all nodes are up, allthe control planes have apiserver, etcd, etc. pods up and running, and soon. It is where the Fake Kubernetes API Server(FKAS)comes in. As the FKAS README linked above already described how it works, we won’t gointo details. We simply need to send FKAS a register POST request (withthe new cluster’s namespace and cluster name), and it will give us an IP and aport, which we can plug into our cluster template and then run clusterctlgenerate cluster. Under the hood, FKAS generates unique API servers for different clusters. Each of the fake API servers does the following jobs:  Mimicking API Calls: The Fake Kubernetes API server was set up to respond tothe essential Kubernetes API calls made during provisioning.  Node Registration: When CAPM3 registered nodes, the Fake API server returnedsuccess responses, making Metal3 believe the nodes had joined a real Kubernetescluster.  Cluster Health and Status: The Fake API responded with “healthy” statuses,allowing CAPI/CAPM3 to continue its workflow without interruption.  Node Creation and Deletion: When CAPI queried for node status or attempted toadd/remove nodes, the Fake API server responded realistically, ensuring theprovisioning process continued smoothly.  Pretending to Host Kubelet: The Fake API server also simulated kubeletresponses, which allowed CAPI/CAPM3 to interact with the fake clusters as thoughthey were managing actual nodes. Note that in this experiment, we provisioned every one of the 1000 fake nodes toa single-node cluster, but it’s possible to increase the number of controlplanes and worker nodes by changing the --control-plane-machine-count andworker-machine-count parameters in the clusterctl generate cluster command. However, you will need to ensure that all clusters’ total nodes do not exceedthe number of BMHs. As a glance, the whole simulation looks like this: It will likely take some time, but once the BMHs are all provisioned, we shouldbe able to verify that all, or at least, most of the clusters are in good shape: # This will list the clusters. kubectl get clusters -A# This will determine the clusters' readiness. kubectl get kcp -A For each cluster, it’s also a good idea to perform a clusterctlcheck. Accessing the fake cluster: A rather interesting (but not essential for our goal) check we can perform onthe fake clusters is to try accessing them. Let’s start with fetching acluster’s kubeconfig: clusterctl -n &lt;cluster-namespace&gt; get kubeconfig &lt;cluster-name&gt; &gt; kubeconfig-&lt;cluster-name&gt;. yamlAs usual, clusterctl will generate a kubeconfig file, but we cannot use itjust yet. Recall that we generated the API endpoint using FKAS; the address wehave now will be a combination of a port with FKAS’s IP address, which isn’taccessible from outside the cluster. What we should do now is:  Edit the kubeconfig-&lt;cluster-name&gt;. yaml so that the endpoint is in the formlocalhost:&lt;port&gt;.  Port-forward the FKAS Pod to the same port the kubeconfig has shown. And voila, now we can access the fake cluster with kubectl --kubeconfigkubeconfig-&lt;cluster-name&gt;. yaml. You can inspect its state and check theresources (nodes, pods, etc. ), but we won’t be able to run any workload on it asit’s fake. Results: In this post, we have demonstrated how it is possible to “generate”bare-metal-based Kubernetes clusters from thin air (or rather, a bunch of nodesthat do not exist). Of course, these “clusters” are not very useful. Still,successfully provisioning them without letting any of our main components(CAPI, CAPM3, BMO, and Ironic) know they are working with fakehardware proves that Metal3 is capable of handling a heavy workload andprovision multiple nodes/clusters. If interested, you could also check (and try out) the experiment by yourselfhere. "
    }, {
    "id": 4,
    "url": "/blog/2024/05/30/Scaling_part_3.html",
    "title": "Scaling to 1000 clusters - Part 3",
    "author" : "Lennart Jern",
    "tags" : "metal3, cluster API, provider, edge",
    "body": "In part 1, we introduced theBare Metal Operator test mode and saw how it can be used to play withBareMetalHosts without Ironic and without any actual hosts. We continued inpart 2 with how to fakeworkload clusters enough for convincing Cluster API’s controllers that they arehealthy. These two pieces together allowed us to run scaling tests and reach ourtarget of 1000 single node clusters. In this final part of the blog post series,we will take a look at the results, the issues that we encountered and theimprovements that have been made. Issues encountered and lessons learned: As part of this work we have learned a lot. We found genuine bugs andperformance issues, but we also learned about relevant configuration options forCluster API and controllers in general. One of the first things we hit was this bug in Bare MetalOperator thatcaused endless requeues for some deleted objects. It was not a big deal, barelynoticeable, at small scale. However, at larger scales things like this become aproblem. The logs become unreadable as they are filled with “spam” fromrequeuing deleted objects and the controller is wasting resources trying toreconcile them. As mentioned, we also learned a lot from this experiment. For example, that allthe controllers have flags for setting their concurrency, i. e. how many objectsthey reconcile in parallel. The default is 10, which works well in most cases,but for larger scales it may be necessary to tune this in order to speed up thereconciliation process. The next thing we hit was rate limits! Bothclient-goandcontroller-runtimehave default rate limits of 10 and 20 QPS (Queries Per Second) respectively thatthe controllers inherit unless overridden. In general, this is a good thing, asit prevents controllers from overloading the API server. They obviously becomean issue once you scale far enough though. For us that happened when we got to600 clusters. Why 600? The number was actually a good clue, and the reason we managed figureout what was wrong! Let’s break it down. By default, the Cluster API controllerwill reconcile objects every 10 minutes (=600 seconds) in addition to reactingto events. Each reconciliation will normally involve one or more API calls, soat 600 clusters, we would have at least one API call per second just from theperiodic sync. In other words, the controllers would at this point use up alarge part of their budget on periodic reconciliation and quickly reach theirlimit when adding reactions to events, such as the creation of a new cluster. At the time, these rate limits were not configurable in the Cluster APIcontrollers, so we had to patch the controllers to increase the limits. We havesince then added flags to the controllers to make this configurable. If youfound this interesting, you can read more about it in thisissue. With concurrency and rate limits taken care of, we managed to reach our targetof 1000 clusters in reasonable time. However, there was still a problem withresource usage. The Kubeadm control plane controller was unreasonably CPUhungry! Luckily, Cluster API has excellent debugging and monitoring toolsavailable so it was easyto collect data and profile the controllers. A quick look at the dashboardconfirmed that the Kubeadm control plane controller was indeed the culprit, witha CPU usage far higher than the other controllers.  We then collected some profiling data and found the cause of the CPU usage. Itwas generating new private keys for accessing the workload cluster API serverevery time it needed to access it. This is a CPU intensive operation, and ithappened four times per reconciliation! The flame graph seen below clearly showsthe four key generation operations, and makes it obvious that this is what takesup most of the time spent on the CPU for the controller.  Improvements: All issues mentioned in the previous section have been addressed. The Bare MetalOperator is no longer re-queuing deleted objects. All controllers have flags forsetting their concurrency and rate limits, and the Kubeadm control planecontroller is now caching and reusing the private keys instead of generating newones every time. The impact of all of this is that  the Bare Metal Operator has more readable logs and lower CPU usage, users can configure rate limits for all Cluster API and Metal3 controllers ifnecessary, and the Kubeadm control plane controller has a much lower CPU usage and fasterreconciliation times. Results: When we set out, it was simply not possible to reach a scale of 1000 clusters ina reasonable time. With the collaboration, help from maintainers and othercommunity members, we managed to reach our target. It is now possible to managethousands of workload clusters through a single Cluster API management cluster. The discussions and efforts also resulted in a deep dive presentation atKubeCon NA2023from the Cluster API maintainers. Cluster API itself now also has an in-memoryproviderwhich makes it almost trivial to test large scale scenarios. However, it must benoted that it can only be used to test the core, bootstrap and control planeproviders. If you want to try it out, you can use the following script. Pleasenote that this will still be CPU intensive, despite the improvements mentionedabove. Creating 1000 clusters is no small task! kind create clusterexport CLUSTER_TOPOLOGY=trueclusterctl init --core=cluster-api:v1. 7. 2 --bootstrap=kubeadm:v1. 7. 2 --control-plane=kubeadm:v1. 7. 2 --infrastructure=in-memory:v1. 7. 2# Patch the controllers to increase the rate limits and concurrencykubectl -n capi-system patch deployment capi-controller-manager \ --type=json -p='[  { op :  add ,  path :  /spec/template/spec/containers/0/args/- ,  value :  --kube-api-qps=100 },  { op :  add ,  path :  /spec/template/spec/containers/0/args/- ,  value :  --kube-api-burst=200 },  { op :  add ,  path :  /spec/template/spec/containers/0/args/- ,  value :  --cluster-concurrency=100 },  { op :  add ,  path :  /spec/template/spec/containers/0/args/- ,  value :  --machine-concurrency=100 } ]'kubectl -n capi-kubeadm-control-plane-system patch deployment capi-kubeadm-control-plane-controller-manager \ --type=json -p='[  { op :  add ,  path :  /spec/template/spec/containers/0/args/- ,  value :  --kube-api-qps=100 },  { op :  add ,  path :  /spec/template/spec/containers/0/args/- ,  value :  --kube-api-burst=200 },  { op :  add ,  path :  /spec/template/spec/containers/0/args/- ,  value :  --kubeadmcontrolplane-concurrency=100 } ]'kubectl -n capi-kubeadm-bootstrap-system patch deployment capi-kubeadm-bootstrap-controller-manager \ --type=json -p='[  { op :  add ,  path :  /spec/template/spec/containers/0/args/- ,  value :  --kube-api-qps=100 },  { op :  add ,  path :  /spec/template/spec/containers/0/args/- ,  value :  --kube-api-burst=200 },  { op :  add ,  path :  /spec/template/spec/containers/0/args/- ,  value :  --kubeadmconfig-concurrency=100 },  { op :  add ,  path :  /spec/template/spec/containers/0/args/- ,  value :  --cluster-concurrency=100 } ]'# Create a ClusterClass and save a Cluster manifestkubectl apply -f https://github. com/kubernetes-sigs/cluster-api/releases/download/v1. 7. 2/clusterclass-in-memory-quick-start. yamlclusterctl generate cluster in-memory-test --flavor=in-memory-development --kubernetes-version=v1. 30. 0 &gt; in-memory-cluster. yaml# Create 1000 clustersSTART=0NUM=1000for ((i=START; i&lt;NUM; i++))do name= test-$(printf  %03d\n   $i )  sed  s/in-memory-test/${name}/g  in-memory-cluster. yaml | kubectl apply -f -doneThis should result in 1000 ready in-memory clusters (and a pretty hot laptop ifyou run it locally). On a laptop with an i9-12900H CPU, it took about 15 minutesuntil all clusters were ready. Conclusion and next steps: We are very happy with the results we achieved. The community has been veryhelpful and responsive, and we are very grateful for all the help we received. Going forward, we will hopefully be able to run scale tests periodically toensure that we are not regressing. Even small scale tests can be enough todetect performance regressions as long as we keep track of the performancemetrics. This is something we hope to incorporate into the CI system in thefuture. "
    }, {
    "id": 5,
    "url": "/blog/2024/04/10/Metal3_at_KubeCon_EU_2024.html",
    "title": "Metal3 at KubeCon EU 2024",
    "author" : "Lennart Jern",
    "tags" : "metal3, talk, conference, kubecon",
    "body": "The Metal3 project was present at KubeCon EU 2024 with multiple maintainers,contributors and users! For many of us, this was the first time we met in thephysical world, despite working together for years already. This was veryvaluable and appreciated by many of us, I am sure. We had time to casuallydiscuss ideas and proposals, hack together on theironic-standalone-operatorand simply get to know each other.  Photo by Michael Captain. As a project, we had the opportunity to give an update through a lightningtalk on Tuesday! On Wednesday we continued with a contribfest sessionwhere we gave an introduction to the project for potential new contributors. Wehad prepared a number of good-first-issue’s that people could choose from ifthey wanted. Perhaps more important though, was that we had time to answerquestions, discuss use-cases, issues and features with the attendees. The newquick-start page was also launched just intime for the contribfest. It should hopefully make it easier to get started withthe project and we encourage everyone to run through it and report or fix anyissues found.  Photo from the official CNCF Flickr. More photoshere. Finally, just like previous, we had a table in the Project Pavilion. There was alot of interest in Metal3, more than last year I would say. Even with fivemaintainers working in parallel, we still had a hard time keeping up with theamount of people stopping by to ask questions! My takeaway from this event isthat we still have work to do on explaining what Metal3 is and how it works. Itis quite uncommon that people know about baseboard management controllers (BMCs)and this of course makes it harder to grasp what Metal3 is all about. However,the interest is there, so we just need to get the information out there so thatpeople can learn! Another takeaway is that Cluster API in general seems toreally take off. Many people that came by our kiosk knew about Cluster API andwere interested in Metal3 because of the integration with have with it. For those of you who couldn’t attend, I hope this post gives an idea about whathappened at KubeCon related to Metal3. Did you miss the contribfest? Maybe youwould like to contribute but don’t know where to start? Check out thegood-first-issue’s!There are still plenty to choose from, and we will keep adding more. "
    }, {
    "id": 6,
    "url": "/blog/2024/01/18/How_to_run_Metal3_website_locally_with_Jekyll.html",
    "title": "How to run Metal3 website locally with Jekyll",
    "author" : "Salima Rabiu",
    "tags" : "metal3, baremetal, metal3-dev-env, documentation, development",
    "body": "Introduction: If you’re a developer or contributor to the Metal3 project, you may needto run the Metal3 website locally to test changes and ensure everythinglooks as expected before deploying them. In this guide, we’ll walk youthrough the process of setting up and running Metal3’s website locallyon your machine using Jekyll. Prerequisites: Before we begin, make sure you have the following prerequisitesinstalled on your system:    Ruby: Jekyll, the static site generator used by Metal3, is built withRuby. Install Ruby and its development tools by running the followingcommand in your terminal:   sudo apt install ruby-full   Setting up Metal3’s Website: Once Ruby is installed, we can proceed to set up Metal3’s website andits dependencies. Follow these steps:    Clone the Metal3 website repository from GitHub. Open your terminaland navigate to the directory where you want to clone the repository,then run the following command:   git clone https://github. com/metal3-io/metal3-io. github. io. git      Change to the cloned directory:   cd metal3-io. github. io      Install the required gems and dependencies using Bundler. Run thefollowing command:   bundle install   Running the Metal3 Website Locally: With Metal3’s website and its dependencies installed, you can now start the localdevelopment server to view and test the website. In the terminal, navigate to theproject’s root directory (metal3-io. github. io) and run the following command: bundle exec jekyll serveThis command tells Jekyll to build the website and start a local server. Once the server is running, you’ll see output indicating the localaddress where the Metal3 website is being served, typicallyhttp://localhost:4000. Open your web browser and enter the provided address. Congratulations!You should now see the Metal3 website running locally, allowing you topreview your changes and ensure everything is working as expected. Conclusion: Running Metal3’s website locally using Jekyll is a great way to testchanges and ensure the site functions properly before deploying them. Byfollowing the steps outlined in this guide, you’ve successfully set upand run Metal3’s website locally. Feel free to explore the Metal3documentation and contribute to the project further. "
    }, {
    "id": 7,
    "url": "/blog/2023/05/17/Scaling_part_2.html",
    "title": "Scaling to 1000 clusters - Part 2",
    "author" : "Lennart Jern",
    "tags" : "metal3, cluster API, provider, edge",
    "body": "In part 1, we introduced the Bare Metal Operator test mode and saw how it can be used to play with BareMetalHosts without Ironic and without any actual hosts. Now we will take a look at the other end of the stack and how we can fake the workload cluster API’s. Test setup: The end goal is to have one management cluster where the Cluster API and Metal3 controllers run. In this cluster we would generate BareMetalHosts and create Clusters, Metal3Clusters, etc to benchmark the controllers. To give them a realistic test, we also need to fake the workload cluster API’s. These will run separately in “backing” clusters to avoid interfering with the test (e. g. by using up all the resources in the management cluster). Here is a diagram that describes the setup: How are we going to fake the workload cluster API’s then?The most obvious solution is to just run the real deal, i. e. the kube-apiserver. This is what would be run in a real workload cluster, together with the other components that make up the Kubernetes control plane. If you want to follow along and try to set this up yourself, you will need at least the following tools installed:  kind kubectl kubeadm clusterctl openssl curl wgetThis has been tested with Kubernetes v1. 25, kind v0. 19 and clusterctl v1. 4. 2. All script snippets are assumed to be for the bash shell. Running the Kubernetes API server: There are many misconceptions, maybe even superstitions, about the Kubernetes control plane. The fact is that it is in no way special. It consists of a few programs that can be run in any way you want: in a container, as a systemd unit or directly executed at the command line. They can run on a Node or outside of the cluster. You can even run multiple instances on the same host as long as you avoid port collisions. For our purposes we basically want to run as little as possible of the control plane components. We just need the API to be available and possible for us to populate with data that the controllers expect to be there. In other words, we need the API server and etcd. The scheduler is not necessary since we won’t run any actual workload (we are just pretending the Nodes are there anyway) and the controller manager would just get in the way when we want to fake resources. It would, for example, try to update the status of the (fake) Nodes that we want to create. The API server will need an etcd instance to connect to. It will also need some TLS configuration, both for connecting to etcd and for handling service accounts. One simple way to generate the needed certificates is to use kubeadm. But before we get there we need to think about how the configuration should look like. For simplicity, we will simply run the API server and etcd in a kind cluster for now. It would then be easy to run them in some other Kubernetes cluster later if needed. Let’s create it right away: kind create cluster# Note: This has been tested with node image# kindest/node:v1. 26. 3@sha256:61b92f38dff6ccc29969e7aa154d34e38b89443af1a2c14e6cfbd2df6419c66fTo try to cut down on the resources required, we will also use a single multi-tenant etcd instance instead of one per API server. We can rely on the internal service discovery so the API server can find etcd via an address like etcd-server. etd-system. svc. cluster. local, instead of using IP addresses. Finally, we will need an endpoint where the API is exposed to the cluster where the controllers are running, but for now we can focus on just getting it up and running with 127. 0. 0. 1:6443 as the endpoint. Based on the above, we can create a kubeadm-config. yaml file like this: apiVersion: kubeadm. k8s. io/v1beta3kind: ClusterConfigurationapiServer: certSANs: - 127. 0. 0. 1clusterName: testcontrolPlaneEndpoint: 127. 0. 0. 1:6443etcd: local:  serverCertSANs:  - etcd-server. etcd-system. svc. cluster. local  peerCertSANs:  - etcd-0. etcd. etcd-system. svc. cluster. localkubernetesVersion: v1. 25. 3certificatesDir: /tmp/test/pkiWe can now use this to generate some certificates and upload them to the cluster: # Generate CA certificateskubeadm init phase certs etcd-ca --config kubeadm-config. yamlkubeadm init phase certs ca --config kubeadm-config. yaml# Generate etcd peer and server certificateskubeadm init phase certs etcd-peer --config kubeadm-config. yamlkubeadm init phase certs etcd-server --config kubeadm-config. yaml# Upload certificateskubectl create namespace etcd-systemkubectl -n etcd-system create secret tls test-etcd --cert /tmp/test/pki/etcd/ca. crt --key /tmp/test/pki/etcd/ca. keykubectl -n etcd-system create secret tls etcd-peer --cert /tmp/test/pki/etcd/peer. crt --key /tmp/test/pki/etcd/peer. keykubectl -n etcd-system create secret tls etcd-server --cert /tmp/test/pki/etcd/server. crt --key /tmp/test/pki/etcd/server. keyDeploying a multi-tenant etcd instance: Now it is time to deploy etcd! curl -L https://github. com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/manifests/v2/etcd. yaml \ | sed  s/CLUSTER/test/g  | kubectl -n etcd-system apply -f -kubectl -n etcd-system wait sts/etcd --for=jsonpath= {. status. availableReplicas} =1As mentioned before, we want to create a multi-tenant etcd that many API servers can share. For this reason, we will need to create a root user and enable authentication for etcd: # Create root rolekubectl -n etcd-system exec etcd-0 -- etcdctl \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ role add root# Create root userkubectl -n etcd-system exec etcd-0 -- etcdctl \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ user add root --new-user-password= rootpw kubectl -n etcd-system exec etcd-0 -- etcdctl \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ user grant-role root root# Enable authenticationkubectl -n etcd-system exec etcd-0 -- etcdctl \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ auth enableAt this point we have a working etcd instance with authentication and TLS enabled. Each client will need to have an etcd user to interact with this instance so we need to create an etcd user for the API server. We already created a root user before so this should look familiar. ## Create etcd tenant# Create userkubectl -n etcd-system exec etcd-0 -- etcdctl --user root:rootpw \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ user add test --new-user-password=test# Create rolekubectl -n etcd-system exec etcd-0 -- etcdctl --user root:rootpw \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ role add test# Add read/write permissions for prefix to the rolekubectl -n etcd-system exec etcd-0 -- etcdctl --user root:rootpw \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ role grant-permission test --prefix=true readwrite  /test/ # Give the user permissions from the rolekubectl -n etcd-system exec etcd-0 -- etcdctl --user root:rootpw \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ user grant-role test testFrom etcd’s point of view, everything is now ready. The API server could theoretically use etcdctl and authenticate with the username and password that we created for it. However, that is not how the API server works. It expects to be able to authenticate using client certificates. Luckily, etcd supports this so we just have to generate the certificates and sign them so that etcd trusts them. The key thing is to set the common name in the certificate to the name of the user we want to authenticate as. Since kubeadm always sets the same common name, we will here use openssl to generate the client certificates so that we get control over it. # Generate etcd client certificateopenssl req -newkey rsa:2048 -nodes -subj  /CN=test  \ -keyout  /tmp/test/pki/apiserver-etcd-client. key  -out  /tmp/test/pki/apiserver-etcd-client. csr openssl x509 -req -in  /tmp/test/pki/apiserver-etcd-client. csr  \ -CA /tmp/test/pki/etcd/ca. crt -CAkey /tmp/test/pki/etcd/ca. key -CAcreateserial \ -out  /tmp/test/pki/apiserver-etcd-client. crt  -days 365Deploying the API server: In order to deploy the API server, we will first need to generate some more certificates. The client certificates for connecting to etcd are already ready, but it also needs certificates to secure the exposed API itself, and a few other things. Then we will also need to create secrets from all of these certificates: kubeadm init phase certs ca --config kubeadm-config. yamlkubeadm init phase certs apiserver --config kubeadm-config. yamlkubeadm init phase certs sa --cert-dir /tmp/test/pkikubectl create ns workload-apikubectl -n workload-api create secret tls test-ca --cert /tmp/test/pki/ca. crt --key /tmp/test/pki/ca. keykubectl -n workload-api create secret tls test-etcd --cert /tmp/test/pki/etcd/ca. crt --key /tmp/test/pki/etcd/ca. keykubectl -n workload-api create secret tls  test-apiserver-etcd-client  \ --cert  /tmp/test/pki/apiserver-etcd-client. crt  \ --key  /tmp/test/pki/apiserver-etcd-client. key kubectl -n workload-api create secret tls apiserver \ --cert  /tmp/test/pki/apiserver. crt  \ --key  /tmp/test/pki/apiserver. key kubectl -n workload-api create secret generic test-sa \ --from-file=tls. crt= /tmp/test/pki/sa. pub  \ --from-file=tls. key= /tmp/test/pki/sa. key With all that out of the way, we can finally deploy the API server!For this we will use a normal Deployment. # Deploy API servercurl -L https://github. com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/manifests/v2/kube-apiserver-deployment. yaml | sed  s/CLUSTER/test/g  | kubectl -n workload-api apply -f -kubectl -n workload-api wait --for=condition=Available deploy/test-kube-apiserverTime to check if it worked!We can use port-forwarding to access the API, but of course we will need some authentication method for it to be useful. With kubeadm we can generate a kubeconfig based on the certificates we already have. kubeadm kubeconfig user --client-name kubernetes-admin --org system:masters \ --config kubeadm-config. yaml &gt; kubeconfig. yamlNow open another terminal and set up port-forwarding to the API server: kubectl -n workload-api port-forward svc/test-kube-apiserver 6443Back in the original terminal, you should now be able to reach the workload API server: kubectl --kubeconfig kubeconfig. yaml cluster-infoNote that it won’t have any Nodes or Pods running. It is completely empty since it is running on its own. There is no kubelet that registered as a Node or applied static manifests, there is no scheduler or controller manager. Exactly like we want it. Faking Nodes and other resources: Let’s take a step back and think about what we have done so far. We have deployed a Kubernetes API server and a multi-tenant etcd instance. More API servers can be added in the same way, so it is straight forward to scale. All of it runs in a kind cluster, which means that it is easy to set up and we can switch to any other Kubernetes cluster if needed later. Through Kubernetes we also get an easy way to access the API servers by using port-forwarding, without exposing all of them separately. The time has now come to think about what we need to put in the workload cluster API to convince the Cluster API and Metal3 controllers that it is healthy. First of all they will expect to see Nodes that match the Machines and that they have a provider ID set. Secondly, they will expect to see healthy control plane Pods. Finally, they will try to check on the etcd cluster. The final point is a problem, but we can work around it for now by configuring external etcd. It will lead to a different code path for the bootstrap and control plane controllers, but until we have something better it will be a good enough test. Creating the Nodes and control plane Pods is really easy though. We are just adding resources and there are no controllers or validating web hooks that can interfere. Try it out! # Create a Nodekubectl --kubeconfig=kubeconfig. yaml create -f https://github. com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/fake-node. yaml# Check that it workedkubectl --kubeconfig=kubeconfig. yaml get nodes# Maybe label it as part of the control plane?kubectl --kubeconfig=kubeconfig. yaml label node fake-node node-role. kubernetes. io/control-plane=  Now add a Pod: kubectl --kubeconfig=kubeconfig. yaml create -f https://github. com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-apiserver-pod. yaml# Set status on the pods (it is not added when using create/apply). curl -L https://github. com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-apiserver-pod-status. yaml | kubectl --kubeconfig=kubeconfig. yaml -n kube-system patch pod kube-apiserver-node-name \  --subresource=status --patch-file=/dev/stdinYou should be able to see something like this: $ kubectl --kubeconfig kubeconfig. yaml get pods -ANAMESPACE   NAME            READY  STATUS  RESTARTS  AGEkube-system  kube-apiserver-node-name  1/1   Running  0     16h$ kubectl --kubeconfig kubeconfig. yaml get nodesNAME    STATUS  ROLES  AGE  VERSIONfake-node  Ready  &lt;none&gt;  16h  v1. 25. 3Now all we have to do is to ensure that the API returns information that the controllers expect. Hooking up the API server to a Cluster API cluster: We will now set up a fresh cluster where we can run the Cluster API and Metal3 controllers. # Delete the previous clusterkind delete cluster# Create a fresh new clusterkind create cluster# Initialize Cluster API with Metal3clusterctl init --infrastructure metal3## Deploy the Bare Metal Opearator# Create the namespace where it will runkubectl create ns baremetal-operator-system# Deploy it in normal modekubectl apply -k https://github. com/metal3-io/baremetal-operator/config/default# Patch it to run in test modekubectl patch -n baremetal-operator-system deploy baremetal-operator-controller-manager --type=json \ -p='[{ op :  add ,  path :  /spec/template/spec/containers/0/args/- ,  value :  --test-mode }]'You should now have a cluster with the Cluster API, Metal3 provider and Bare Metal Operator running. Next, we will prepare some files that will come in handy later, namely a cluster template, BareMetalHost manifest and Kubeadm configuration file. # Download cluster-templateCLUSTER_TEMPLATE=/tmp/cluster-template. yaml# https://github. com/metal3-io/cluster-api-provider-metal3/blob/main/examples/clusterctl-templates/clusterctl-cluster. yamlCLUSTER_TEMPLATE_URL= https://raw. githubusercontent. com/metal3-io/cluster-api-provider-metal3/main/examples/clusterctl-templates/clusterctl-cluster. yaml wget -O  ${CLUSTER_TEMPLATE}   ${CLUSTER_TEMPLATE_URL} # Save a manifest of a BareMetalHostcat &lt;&lt; EOF &gt; /tmp/test-hosts. yaml---apiVersion: v1kind: Secretmetadata: name: worker-1-bmc-secrettype: Opaquedata: username: YWRtaW4= password: cGFzc3dvcmQ=---apiVersion: metal3. io/v1alpha1kind: BareMetalHostmetadata: name: worker-1spec: online: true bmc:  address: libvirt://192. 168. 122. 1:6233/  credentialsName: worker-1-bmc-secret bootMACAddress:  00:60:2F:10:E9:A7 EOF# Save a kubeadm config templatecat &lt;&lt; EOF &gt; /tmp/kubeadm-config-template. yamlapiVersion: kubeadm. k8s. io/v1beta3kind: ClusterConfigurationapiServer: certSANs:  - localhost  - 127. 0. 0. 1  - 0. 0. 0. 0  - HOSTclusterName: testcontrolPlaneEndpoint: HOST:6443etcd: local:  serverCertSANs:   - etcd-server. etcd-system. svc. cluster. local  peerCertSANs:   - etcd-0. etcd. etcd-system. svc. cluster. localkubernetesVersion: v1. 25. 3certificatesDir: /tmp/CLUSTER/pkiEOFWith this we have enough to start creating the workload cluster. First, we need to set up some certificates. This should look very familiar from earlier when we created certificates for the Kubernetes API server and etcd. mkdir -p /tmp/pki/etcdCLUSTER= test NAMESPACE=etcd-systemCLUSTER_APIENDPOINT_HOST= test-kube-apiserver. ${NAMESPACE}. svc. cluster. local sed -e  s/NAMESPACE/${NAMESPACE}/g  -e  s/\/CLUSTER//g  -e  s/HOST/${CLUSTER_APIENDPOINT_HOST}/g  \ /tmp/kubeadm-config-template. yaml &gt;  /tmp/kubeadm-config-${CLUSTER}. yaml # Generate CA certificateskubeadm init phase certs etcd-ca --config  /tmp/kubeadm-config-${CLUSTER}. yaml kubeadm init phase certs ca --config  /tmp/kubeadm-config-${CLUSTER}. yaml # Generate etcd peer and server certificateskubeadm init phase certs etcd-peer --config  /tmp/kubeadm-config-${CLUSTER}. yaml kubeadm init phase certs etcd-server --config  /tmp/kubeadm-config-${CLUSTER}. yaml Next, we create the namespace, the BareMetalHost and secrets from the certificates: CLUSTER=test-1NAMESPACE=test-1kubectl create namespace  ${NAMESPACE} kubectl -n  ${NAMESPACE}  apply -f /tmp/test-hosts. yamlkubectl -n  ${NAMESPACE}  create secret tls  ${CLUSTER}-etcd  --cert /tmp/pki/etcd/ca. crt --key /tmp/pki/etcd/ca. keykubectl -n  ${NAMESPACE}  create secret tls  ${CLUSTER}-ca  --cert /tmp/pki/ca. crt --key /tmp/pki/ca. keyWe are now ready to create the cluster!We just need a few variables for the template. The important part here is the CLUSTER_APIENDPOINT_HOST and CLUSTER_APIENDPOINT_PORT, since this will be used by the controllers to connect to the workload cluster API. You should set the IP to the private IP of the test machine or similar. This way we can use port-forwarding to expose the API on this IP, which the controllers can then reach. The port just have to be one not in use, and preferably something that is easy to remember and associate with the correct cluster. For example, cluster 1 gets port 10001, cluster 2 gets 10002, etc. export IMAGE_CHECKSUM= 97830b21ed272a3d854615beb54cf004 export IMAGE_CHECKSUM_TYPE= md5 export IMAGE_FORMAT= raw export IMAGE_URL= http://172. 22. 0. 1/images/rhcos-ootpa-latest. qcow2 export KUBERNETES_VERSION= v1. 25. 3 export WORKERS_KUBEADM_EXTRA_CONFIG=  export CLUSTER_APIENDPOINT_HOST= 172. 17. 0. 2 export CLUSTER_APIENDPOINT_PORT= 10001 export CTLPLANE_KUBEADM_EXTRA_CONFIG=   clusterConfiguration:   controlPlaneEndpoint: ${CLUSTER_APIENDPOINT_HOST}:${CLUSTER_APIENDPOINT_PORT}   apiServer:    certSANs:    - localhost    - 127. 0. 0. 1    - 0. 0. 0. 0    - ${CLUSTER_APIENDPOINT_HOST}   etcd:    external:     endpoints:      - https://etcd-server:2379     caFile: /etc/kubernetes/pki/etcd/ca. crt     certFile: /etc/kubernetes/pki/apiserver-etcd-client. crt     keyFile: /etc/kubernetes/pki/apiserver-etcd-client. key Create the cluster! clusterctl generate cluster  ${CLUSTER}  \  --from  ${CLUSTER_TEMPLATE}  \  --target-namespace  ${NAMESPACE}  | kubectl apply -f -This will give you a cluster and all the templates and other resources that are needed. However, we will need to fill in for the non-existent hardware and create the workload cluster API server, like we practiced before. This time it is slightly different, because some of the steps are handled by the Cluster API. We just need to take care of what would happen on the node, plus the etcd part since we are using external etcd configuration. mkdir -p  /tmp/${CLUSTER}/pki/etcd # Generate etcd client certificateopenssl req -newkey rsa:2048 -nodes -subj  /CN=${CLUSTER}  \ -keyout  /tmp/${CLUSTER}/pki/apiserver-etcd-client. key  -out  /tmp/${CLUSTER}/pki/apiserver-etcd-client. csr openssl x509 -req -in  /tmp/${CLUSTER}/pki/apiserver-etcd-client. csr  \ -CA /tmp/pki/etcd/ca. crt -CAkey /tmp/pki/etcd/ca. key -CAcreateserial \ -out  /tmp/${CLUSTER}/pki/apiserver-etcd-client. crt  -days 365# Get the k8s ca certificate and key. # This is used by kubeadm to generate the api server certificateskubectl -n  ${NAMESPACE}  get secrets  ${CLUSTER}-ca  -o jsonpath= {. data. tls\. crt}  | base64 -d &gt;  /tmp/${CLUSTER}/pki/ca. crt kubectl -n  ${NAMESPACE}  get secrets  ${CLUSTER}-ca  -o jsonpath= {. data. tls\. key}  | base64 -d &gt;  /tmp/${CLUSTER}/pki/ca. key # Generate certificatessed -e  s/NAMESPACE/${NAMESPACE}/g  -e  s/CLUSTER/${CLUSTER}/g  -e  s/HOST/${CLUSTER_APIENDPOINT_HOST}/g  \ /tmp/kubeadm-config-template. yaml &gt;  /tmp/kubeadm-config-${CLUSTER}. yaml kubeadm init phase certs apiserver --config  /tmp/kubeadm-config-${CLUSTER}. yaml # Create secretskubectl -n  ${NAMESPACE}  create secret tls  ${CLUSTER}-apiserver-etcd-client  --cert  /tmp/${CLUSTER}/pki/apiserver-etcd-client. crt  --key  /tmp/${CLUSTER}/pki/apiserver-etcd-client. key kubectl -n  ${NAMESPACE}  create secret tls apiserver --cert  /tmp/${CLUSTER}/pki/apiserver. crt  --key  /tmp/${CLUSTER}/pki/apiserver. key Now we will need to set up the fake cluster resources. For this we will create a second kind cluster and set up etcd, just like we did before. # Note: This will create a kubeconfig context named kind-backing-cluster-1,# i. e.  kind-  is prefixed to the name. kind create cluster --name backing-cluster-1# Setup central etcdCLUSTER= test NAMESPACE=etcd-systemkubectl create namespace  ${NAMESPACE} # Upload certificateskubectl -n  ${NAMESPACE}  create secret tls  ${CLUSTER}-etcd  --cert /tmp/pki/etcd/ca. crt --key /tmp/pki/etcd/ca. keykubectl -n  ${NAMESPACE}  create secret tls etcd-peer --cert /tmp/pki/etcd/peer. crt --key /tmp/pki/etcd/peer. keykubectl -n  ${NAMESPACE}  create secret tls etcd-server --cert /tmp/pki/etcd/server. crt --key /tmp/pki/etcd/server. key# Deploy ETCDcurl -L https://github. com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/manifests/v2/etcd. yaml \ | sed  s/CLUSTER/${CLUSTER}/g  | kubectl -n  ${NAMESPACE}  apply -f -kubectl -n etcd-system wait sts/etcd --for=jsonpath= {. status. availableReplicas} =1# Create root rolekubectl -n etcd-system exec etcd-0 -- etcdctl \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ role add root# Create root userkubectl -n etcd-system exec etcd-0 -- etcdctl \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ user add root --new-user-password= rootpw kubectl -n etcd-system exec etcd-0 -- etcdctl \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ user grant-role root root# Enable authenticationkubectl -n etcd-system exec etcd-0 -- etcdctl \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ auth enableSwitch the context back to the first cluster with kubectl config use-context kind-kind so we don’t get confused about which is the main cluster. We will now need to put all the expected certificates for the fake cluster in the kind-backing-cluster-1 so that they can be used by the API server that we will deploy there. CLUSTER=test-1NAMESPACE=test-1# Setup fake resources for cluster test-1kubectl --context=kind-backing-cluster-1 create namespace  ${NAMESPACE} kubectl --context=kind-backing-cluster-1 -n  ${NAMESPACE}  create secret tls  ${CLUSTER}-etcd  --cert /tmp/pki/etcd/ca. crt --key /tmp/pki/etcd/ca. keykubectl --context=kind-backing-cluster-1 -n  ${NAMESPACE}  create secret tls  ${CLUSTER}-ca  --cert /tmp/pki/ca. crt --key /tmp/pki/ca. keykubectl --context=kind-backing-cluster-1 -n  ${NAMESPACE}  create secret tls  ${CLUSTER}-apiserver-etcd-client  --cert  /tmp/${CLUSTER}/pki/apiserver-etcd-client. crt  --key  /tmp/${CLUSTER}/pki/apiserver-etcd-client. key kubectl --context=kind-backing-cluster-1 -n  ${NAMESPACE}  create secret tls apiserver --cert  /tmp/${CLUSTER}/pki/apiserver. crt  --key  /tmp/${CLUSTER}/pki/apiserver. key kubectl -n  ${NAMESPACE}  get secrets  ${CLUSTER}-sa  -o yaml | kubectl --context=kind-backing-cluster-1 create -f -## Create etcd tenant# Create userkubectl --context=kind-backing-cluster-1 -n etcd-system exec etcd-0 -- etcdctl --user root:rootpw \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ user add  ${CLUSTER}  --new-user-password= ${CLUSTER} # Create rolekubectl --context=kind-backing-cluster-1 -n etcd-system exec etcd-0 -- etcdctl --user root:rootpw \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ role add  ${CLUSTER} # Add read/write permissions for prefix to the rolekubectl --context=kind-backing-cluster-1 -n etcd-system exec etcd-0 -- etcdctl --user root:rootpw \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ role grant-permission  ${CLUSTER}  --prefix=true readwrite  /${CLUSTER}/ # Give the user permissions from the rolekubectl --context=kind-backing-cluster-1 -n etcd-system exec etcd-0 -- etcdctl --user root:rootpw \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ user grant-role  ${CLUSTER}   ${CLUSTER} Check that the Metal3Machine is associated with a BareMetalHost. Deploy the API server. # Deploy API servercurl -L https://github. com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/manifests/v2/kube-apiserver-deployment. yaml | sed -e  s/CLUSTER/${CLUSTER}/g  | kubectl --context=kind-backing-cluster-1 -n  ${NAMESPACE}  apply -f -kubectl --context=kind-backing-cluster-1 -n  ${NAMESPACE}  wait --for=condition=Available deploy/test-kube-apiserver# Get kubeconfigclusterctl -n  ${NAMESPACE}  get kubeconfig  ${CLUSTER}  &gt;  /tmp/kubeconfig-${CLUSTER}. yaml # Edit kubeconfig to point to 127. 0. 0. 1:${CLUSTER_APIENDPOINT_PORT}sed -i -e  s/${CLUSTER_APIENDPOINT_HOST}/127. 0. 0. 1/  -e  s/:6443/:${CLUSTER_APIENDPOINT_PORT}/   /tmp/kubeconfig-${CLUSTER}. yaml # Port forward for accessing the APIkubectl --context=kind-backing-cluster-1 -n  ${NAMESPACE}  port-forward \   --address  ${CLUSTER_APIENDPOINT_HOST},127. 0. 0. 1  svc/test-kube-apiserver  ${CLUSTER_APIENDPOINT_PORT} :6443 &amp;# Check that it is workingkubectl --kubeconfig= /tmp/kubeconfig-${CLUSTER}. yaml  cluster-infoNow that we have a working API for the workload cluster, the only remaining thing is to put everything that the controllers expect in it. This includes adding a Node to match the Machine as well as static pods that Cluster API expects to be there. Let’s start with the Node!The Node must have the correct name and a label with the BareMetalHost UID so that the controllers can put the correct provider ID on it. We have only created 1 BareMetalHost so it is easy to pick the correct one. The name of the Node should be the same as the Machine, which is also only a single one. machine= $(kubectl -n  ${NAMESPACE}  get machine -o jsonpath= {. items[0]. metadata. name} ) bmh_uid= $(kubectl -n  ${NAMESPACE}  get bmh -o jsonpath= {. items[0]. metadata. uid} ) curl -L https://github. com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/fake-node. yaml | sed -e  s/fake-node/${machine}/g  -e  s/fake-uuid/${bmh_uid}/g  | \ kubectl --kubeconfig= /tmp/kubeconfig-${CLUSTER}. yaml  create -f -# Label it as control-plane since this is a control-plane node. kubectl --kubeconfig= /tmp/kubeconfig-${CLUSTER}. yaml  label node  ${machine}  node-role. kubernetes. io/control-plane=  # Upload kubeadm config to configmap. This will mark the KCP as initialized. kubectl --kubeconfig= /tmp/kubeconfig-${CLUSTER}. yaml  -n kube-system create cm kubeadm-config \ --from-file=ClusterConfiguration= /tmp/kubeadm-config-${CLUSTER}. yaml This should be enough to make the Machines healthy!You should be able to see something similar to this: $ clusterctl -n test-1 describe cluster test-1NAME                      READY SEVERITY REASON SINCE MESSAGECluster/test-1                 True           46s├─ClusterInfrastructure - Metal3Cluster/test-1 True           114m└─ControlPlane - KubeadmControlPlane/test-1   True           46s └─Machine/test-1-f2nw2            True           47sHowever, if you check the KubeadmControlPlane more carefully, you will notice that it is still complaining about control plane components. This is because we have not created the static pods yet, and it is also unable to check the certificate expiration date for the Machine. Let’s fix it: # Add static pods to make kubeadm control plane manager happycurl -L https://github. com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-apiserver-pod. yaml | sed  s/node-name/${machine}/g  | kubectl --kubeconfig= /tmp/kubeconfig-${CLUSTER}. yaml  create -f -curl -L https://github. com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-controller-manager-pod. yaml | sed  s/node-name/${machine}/g  | kubectl --kubeconfig= /tmp/kubeconfig-${CLUSTER}. yaml  create -f -curl -L https://github. com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-scheduler-pod. yaml | sed  s/node-name/${machine}/g  | kubectl --kubeconfig= /tmp/kubeconfig-${CLUSTER}. yaml  create -f -# Set status on the pods (it is not added when using create/apply). curl -L https://github. com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-apiserver-pod-status. yaml | kubectl --kubeconfig= /tmp/kubeconfig-${CLUSTER}. yaml  -n kube-system patch pod  kube-apiserver-${machine}  \  --subresource=status --patch-file=/dev/stdincurl -L https://github. com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-controller-manager-pod-status. yaml | kubectl --kubeconfig= /tmp/kubeconfig-${CLUSTER}. yaml  -n kube-system patch pod  kube-controller-manager-${machine}  \  --subresource=status --patch-file=/dev/stdincurl -L https://github. com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-scheduler-pod-status. yaml | kubectl --kubeconfig= /tmp/kubeconfig-${CLUSTER}. yaml  -n kube-system patch pod  kube-scheduler-${machine}  \  --subresource=status --patch-file=/dev/stdin# Add certificate expiry annotations to make kubeadm control plane manager happyCERT_EXPIRY_ANNOTATION= machine. cluster. x-k8s. io/certificates-expiry EXPIRY_TEXT= $(kubectl -n  ${NAMESPACE}  get secret apiserver -o jsonpath= {. data. tls\. crt}  | base64 -d | openssl x509 -enddate -noout | cut -d= -f 2) EXPIRY= $(date --date= ${EXPIRY_TEXT}  --iso-8601=seconds) kubectl -n  ${NAMESPACE}  annotate machine  ${machine}   ${CERT_EXPIRY_ANNOTATION}=${EXPIRY} kubectl -n  ${NAMESPACE}  annotate kubeadmconfig --all  ${CERT_EXPIRY_ANNOTATION}=${EXPIRY} Now we finally have a completely healthy cluster as far as the controllers are concerned. Conclusions and summary: We now have all the tools necessary to start experimenting.  With the BareMetal Operator running in test mode, we can skip Ironic and still work with BareMetalHosts that act like normal.  We can set up separate “backing” clusters where we run etcd and multiple API servers to fake the workload cluster API’s.  Fake Nodes and Pods can be easily added to the workload cluster API’s, and configured as we want.  The workload cluster API’s can be exposed to the controllers in the test cluster using port-forwarding. In this post we have not automated all of this, but if you want to see a scripted setup, take a look at this. It is what we used to scale to 1000 clusters. Just remember that it may need some tweaking for your specific environment if you want to try it out! Specifically we used 10 “backing” clusters, i. e. 10 separate cloud VMs with kind clusters where we run etcd and the workload cluster API’s. Each one would hold 100 API servers. The test cluster was on its own separate VM also running a kind cluster with all the controllers and all the Cluster objects, etc. In the next and final blog post of this series we will take a look at the results of all this. What issues did we run into along the way?How did we fix or work around them?We will also take a look at what is going on in the community related to this and discuss potential future work in the area. "
    }, {
    "id": 8,
    "url": "/blog/2023/05/05/Scaling_part_1.html",
    "title": "Scaling to 1000 clusters - Part 1",
    "author" : "Lennart Jern",
    "tags" : "metal3, cluster API, provider, edge",
    "body": "We want to ensure that Metal3 can scale to thousands of nodes and clusters. However, running tests with thousands of real servers is expensive and we don’t have access to any such large environment in the project. So instead we have been focusing on faking the hardware while trying to keep things as realistic as possible for the controllers. In this first part we will take a look at the Bare Metal Operator and the test mode it offers. The next part will be about how to fake the Kubernetes API of the workload clusters. In the final post we will take a look at the issues we ran into and what is being done in the community to address them so that we can keep scaling! Some background on how to fool the controllers: With the full Metal3 stack, from Ironic to Cluster API, we have the following controllers that operate on Kubernetes APIs:  Cluster API Kubeadm control plane controller Cluster API Kubeadm bootstrap controller Cluster API controller Cluster API provider for Metal3 controller IP address manager controller Bare Metal Operator controllerWe will first focus on the controllers that interact with Nodes, Machines, Metal3Machines and BareMetalHosts, i. e. objects related to actual physical machines that we need to fake. In other words, we are skipping the IP address manager for now. What do these controllers care about really?What do we need to do to fool them?At the Cluster API level, the controllers just care about the Kubernetes resources in the management cluster (e. g. Clusters and Machines) and some resources in the workload cluster (e. g. Nodes and the etcd Pods). The controllers will try to connect to the workload clusters in order to check the status of the resources there, so if there is no real workload cluster, this is something we will need to fake if we want to fool the controllers. When it comes to Cluster API provider for Metal3, it connects the abstract high level objects with the BareMetalHosts, so here we will need to make the BareMetalHosts to behave realistically in order to provide a good test. This is where the Bare Metal Operator test mode comes in. If we can fake the workload cluster API and the BareMetalHosts, then all the Cluster API controllers and the Metal3 provider will get a realistic test that we can use when working on scalability. Bare Metal Operator test mode: The Bare Metal Operator has a test mode, in which it doesn’t talk to Ironic. Instead it just pretends that everything is fine and all actions succeed. In this mode the BareMetalHosts will move through the state diagram just like they normally would (but quite a bit faster). To enable it, all you have to do is add the -test-mode flag when running the Bare Metal Operator controller. For convenience there is also a make target (make run-test-mode) that will run the Bare Metal Operator directly on the host in test mode. Here is an example of how to use it. You will need kind and kubectl installed for this to work, but you don’t need the Bare Metal Operator repository cloned.    Create a kind cluster and deploy cert-manager (needed for web hook certificates):   kind create cluster# Install cert-managerkubectl apply -f https://github. com/cert-manager/cert-manager/releases/download/v1. 11. 0/cert-manager. yaml      Deploy the Bare Metal Operator in test mode:   # Create the namespace where it will runkubectl create ns baremetal-operator-system# Deploy it in normal modekubectl apply -k https://github. com/metal3-io/baremetal-operator/config/default# Patch it to run in test modekubectl patch -n baremetal-operator-system deploy baremetal-operator-controller-manager --type=json \ -p='[{ op :  add ,  path :  /spec/template/spec/containers/0/args/- ,  value :  --test-mode }]'      In a separate terminal, create a BareMetalHost from the example manifests:   kubectl apply -f https://github. com/metal3-io/baremetal-operator/raw/main/examples/example-host. yaml   After applying the BareMetalHost, it will quickly go through registering and become available. $ kubectl get bmhNAME          STATE     CONSUMER  ONLINE  ERROR  AGEexample-baremetalhost  registering       true       2s$ kubectl get bmhNAME          STATE    CONSUMER  ONLINE  ERROR  AGEexample-baremetalhost  available       true       6sWe can now provision the BareMetalHost, turn it off, deprovision, etc. Just like normal, except that the machine doesn’t exist. Let’s try provisioning it! kubectl patch bmh example-baremetalhost --type=merge --patch-file=/dev/stdin &lt;&lt;EOFspec: image:  url:  http://example. com/totally-fake-image. vmdk   checksum:  made-up-checksum   format: vmdkEOFYou will see it go through provisioning and end up in provisioned state: $ kubectl get bmhNAME          STATE     CONSUMER  ONLINE  ERROR  AGEexample-baremetalhost  provisioning       true       7m20s$ kubectl get bmhNAME          STATE     CONSUMER  ONLINE  ERROR  AGEexample-baremetalhost  provisioned       true       7m22sWrapping up: With Bare Metal Operator in test mode, we have the foundation for starting our scalability journey. We can easily create BareMetalHost objects and they behave similar to what they would in a real scenario. A simple bash script will at this point allow us to create as many BareMetalHosts as we would like. To wrap things up, we will now do just that: put together a script and try generating a few BareMetalHosts. The script will do the same thing we did before when creating the example BareMetalHost, but it will also give them different names so we don’t get naming collisions. Here it is: #!/usr/bin/env bashset -eucreate_bmhs() { n= ${1}  for (( i = 1; i &lt;= n; ++i )); do  cat &lt;&lt; EOF---apiVersion: v1kind: Secretmetadata: name: worker-$i-bmc-secrettype: Opaquedata: username: YWRtaW4= password: cGFzc3dvcmQ=---apiVersion: metal3. io/v1alpha1kind: BareMetalHostmetadata: name: worker-$ispec: online: true bmc:  address: libvirt://192. 168. 122. $i:6233/  credentialsName: worker-$i-bmc-secret bootMACAddress:  $(printf '00:60:2F:%02X:%02X:%02X\n' $((RANDOM%256)) $((RANDOM%256)) $((RANDOM%256))) EOF done}NUM= ${1:-10} create_bmhs  ${NUM} Save it as produce-available-hosts. sh and try it out: $ . /produce-available-hosts. sh 10 | kubectl apply -f -secret/worker-1-bmc-secret createdbaremetalhost. metal3. io/worker-1 createdsecret/worker-2-bmc-secret createdbaremetalhost. metal3. io/worker-2 createdsecret/worker-3-bmc-secret createdbaremetalhost. metal3. io/worker-3 createdsecret/worker-4-bmc-secret createdbaremetalhost. metal3. io/worker-4 createdsecret/worker-5-bmc-secret createdbaremetalhost. metal3. io/worker-5 createdsecret/worker-6-bmc-secret createdbaremetalhost. metal3. io/worker-6 createdsecret/worker-7-bmc-secret createdbaremetalhost. metal3. io/worker-7 createdsecret/worker-8-bmc-secret createdbaremetalhost. metal3. io/worker-8 createdsecret/worker-9-bmc-secret createdbaremetalhost. metal3. io/worker-9 createdsecret/worker-10-bmc-secret createdbaremetalhost. metal3. io/worker-10 created$ kubectl get bmhNAME    STATE     CONSUMER  ONLINE  ERROR  AGEworker-1  registering       true       2sworker-10  available        true       2sworker-2  available        true       2sworker-3  available        true       2sworker-4  available        true       2sworker-5  available        true       2sworker-6  registering       true       2sworker-7  available        true       2sworker-8  available        true       2sworker-9  available        true       2sWith this we conclude the first part of the scaling series. In the next post, we will take a look at how to fake the other end of the stack: the workload cluster API. "
    }, {
    "id": 9,
    "url": "/blog/2022/07/08/One_cluster_multiple_providers.html",
    "title": "One cluster - multiple providers",
    "author" : "Lennart Jern",
    "tags" : "metal3, cluster API, provider, hybrid, edge",
    "body": "Running on bare metal has both benefits and drawbacks. You can get thebest performance possible out of the hardware, but it can also be quiteexpensive and maybe not necessary for all workloads. Perhaps a hybridcluster could give you the best of both? Raw power for the workload thatneeds it, and cheap virtualized commodity for the rest. This blog postwill show how to set up a cluster like this using the Cluster API backedby the Metal3 and BYOH providers. The problem: Imagine that you have some bare metal servers that you want to use forsome specific workload. Maybe the workload benefits from the specifichardware or there are some requirements that make it necessary to run itthere. The rest of the organization already uses Kubernetes and thecluster API everywhere so of course you want the same for this as well. Perfect, grab Metal³ and start working! But hold on, this would mean that you use some of the servers forrunning the Kubernetes control plane and possibly all the cluster APIcontrollers. If there are enough servers this is probably not an issue,but do you really want to “waste” these servers on such genericworkloads that could be running anywhere? This can become especiallypainful if you need multiple control plane nodes. Each server isprobably powerful enough to run all the control planes and controllers,but it would be a single point of failure… What if there was a way to use a different cluster API infrastructureprovider for some nodes? For example, use the Openstack infrastructureprovider for the control plane and Metal³ for the workers. Let’s do anexperiment! Setting up the experiment environment: This blog post will use the Bring your ownhost(BYOH) provider together with Metal³ as a proof of concept to show whatis currently possible. The BYOH provider was chosen as the second provider for two reasons:  Due to its design (you provision the host yourself), it is very easyto adapt it to the test (e. g. use a VM in the same network that themetal3-dev-env uses).  It is one of the providers that is known to work when combiningmultiple providers for a single cluster. We will be using themetal3-dev-env on Ubuntuas a starting point for this experiment. Note that it makes substantialchanges to the machine where it is running, so you may want to use adedicated lab machine instead of your laptop for this. If you have notdone so already, clone it and run make. This should give you amanagement cluster with the Metal³ provider installed and twoBareMetalHosts ready for provisioning. The next step is to add the BYOH provider and a ByoHost. clusterctl init --infrastructure byohFor the ByoHost we will use Vagrant. You can install it with sudo apt install vagrant. Then copy the Vagrantfile below to a new folder and run vagrant up. # -*- mode: ruby -*-hosts = {   control-plane1  =&gt; {  memory  =&gt; 2048,  ip  =&gt;  192. 168. 10. 10 },  #  control-plane2  =&gt; {  memory  =&gt; 2048,  ip  =&gt;  192. 168. 10. 11 },  #  control-plane3  =&gt; {  memory  =&gt; 2048,  ip  =&gt;  192. 168. 10. 12 },}Vagrant. configure( 2 ) do |config|  # Choose which box you want below  config. vm. box =  generic/ubuntu2004   config. vm. synced_folder  .  ,  /vagrant , disabled: true  config. vm. provider :libvirt do |libvirt|   # QEMU system connection is required for private network configuration   libvirt. qemu_use_session = false  end  # Loop over all machine names  hosts. each_key do |host|    config. vm. define host, primary: host == hosts. keys. first do |node|      node. vm. hostname = host      node. vm. network :private_network, ip: hosts[host][ ip ],       libvirt__forward_mode:  route       node. vm. provider :libvirt do |lv|        lv. memory = hosts[host][ memory ]        lv. cpus = 2      end    end  endendVagrant should now have created a new VM to use as a ByoHost. Now wejust need to run the BYOH agent in the VM to make it register as aByoHost in the management cluster. The BYOH agent needs a kubeconfigfile to do this, so we start by copying it to the VM: cp ~/. kube/config ~/. kube/management-cluster. conf# Ensure that the correct IP is used (not localhost)export KIND_IP=$(docker inspect -f '{{range . NetworkSettings. Networks}}{{. IPAddress}}{{end}}' kind-control-plane)sed -i 's/  server\:. */  server\: https\:\/\/' $KIND_IP '\:6443/g' ~/. kube/management-cluster. confscp -i . vagrant/machines/control-plane1/libvirt/private_key \ /home/ubuntu/. kube/management-cluster. conf vagrant@192. 168. 10. 10:management-cluster. confNext, install the prerequisites and host agent in the VM and run it. vagrant sshsudo apt install -y socat ebtables ethtool conntrackwget https://github. com/vmware-tanzu/cluster-api-provider-bringyourownhost/releases/download/v0. 2. 0/byoh-hostagent-linux-amd64mv byoh-hostagent-linux-amd64 byoh-hostagentchmod +x byoh-hostagentsudo . /byoh-hostagent --namespace metal3 --kubeconfig management-cluster. confYou should now have a management cluster with both the Metal³ and BYOHproviders installed, as well as two BareMetalHosts and one ByoHost. $ kubectl -n metal3 get baremetalhosts,byohostsNAME               STATE    CONSUMER  ONLINE  ERROR  AGEbaremetalhost. metal3. io/node-0  available       true       18mbaremetalhost. metal3. io/node-1  available       true       18mNAME                           AGEbyohost. infrastructure. cluster. x-k8s. io/control-plane1  73sCreating a multi-provider cluster: The trick is to create both a Metal3Cluster and a ByoCluster that areowned by one common Cluster. We will use the ByoCluster for the controlplane in this case. First the Cluster: apiVersion: cluster. x-k8s. io/v1beta1kind: Clustermetadata: labels:  cni: mixed-cluster-crs-0  crs:  true  name: mixed-clusterspec: clusterNetwork:  pods:   cidrBlocks:   - 192. 168. 0. 0/16  serviceDomain: cluster. local  services:   cidrBlocks:   - 10. 128. 0. 0/12 controlPlaneRef:  apiVersion: controlplane. cluster. x-k8s. io/v1beta1  kind: KubeadmControlPlane  name: mixed-cluster-control-plane infrastructureRef:  apiVersion: infrastructure. cluster. x-k8s. io/v1beta1  kind: ByoCluster  name: mixed-clusterAdd the rest of the BYOH manifests to get a control plane. The code is collapsed here for easier reading. Please click on the line below to expand it.  KubeadmControlPlane, ByoCluster and ByoMachineTemplate      #{% raw %}apiVersion: controlplane. cluster. x-k8s. io/v1beta1kind: KubeadmControlPlanemetadata: labels:  nodepool: pool0 name: mixed-cluster-control-planespec: kubeadmConfigSpec:  clusterConfiguration:   apiServer:    certSANs:    - localhost    - 127. 0. 0. 1    - 0. 0. 0. 0    - host. docker. internal   controllerManager:    extraArgs:     enable-hostpath-provisioner:  true   files:  - content: |    apiVersion: v1    kind: Pod    metadata:     creationTimestamp: null     name: kube-vip     namespace: kube-system    spec:     containers:     - args:      - start      env:      - name: vip_arp       value:  true       - name: vip_leaderelection       value:  true       - name: vip_address       value: 192. 168. 10. 20      - name: vip_interface       value: {{ . DefaultNetworkInterfaceName }}      - name: vip_leaseduration       value:  15       - name: vip_renewdeadline       value:  10       - name: vip_retryperiod       value:  2       image: ghcr. io/kube-vip/kube-vip:v0. 3. 5      imagePullPolicy: IfNotPresent      name: kube-vip      resources: {}      securityContext:       capabilities:        add:        - NET_ADMIN        - SYS_TIME      volumeMounts:      - mountPath: /etc/kubernetes/admin. conf       name: kubeconfig     hostNetwork: true     volumes:     - hostPath:       path: /etc/kubernetes/admin. conf       type: FileOrCreate      name: kubeconfig    status: {}    owner: root:root    path: /etc/kubernetes/manifests/kube-vip. yaml  initConfiguration:   nodeRegistration:    criSocket: /var/run/containerd/containerd. sock    ignorePreflightErrors:    - Swap    - DirAvailable--etc-kubernetes-manifests    - FileAvailable--etc-kubernetes-kubelet. conf  joinConfiguration:   nodeRegistration:    criSocket: /var/run/containerd/containerd. sock    ignorePreflightErrors:    - Swap    - DirAvailable--etc-kubernetes-manifests    - FileAvailable--etc-kubernetes-kubelet. conf machineTemplate:  infrastructureRef:   apiVersion: infrastructure. cluster. x-k8s. io/v1beta1   kind: ByoMachineTemplate   name: mixed-cluster-control-plane replicas: 1 version: v1. 23. 5---apiVersion: infrastructure. cluster. x-k8s. io/v1beta1kind: ByoClustermetadata: name: mixed-clusterspec: bundleLookupBaseRegistry: projects. registry. vmware. com/cluster_api_provider_bringyourownhost bundleLookupTag: v1. 23. 5 controlPlaneEndpoint:  host: 192. 168. 10. 20  port: 6443---apiVersion: infrastructure. cluster. x-k8s. io/v1beta1kind: ByoMachineTemplatemetadata: name: mixed-cluster-control-planespec: template:  spec: {}#   So far this is a “normal” Cluster backed by the BYOH provider. But nowit is time to do something different. Instead of adding more ByoHosts asworkers, we will add a Metal3Cluster and MachineDeployment backed byBareMetalHosts! Note that the controlPlaneEndpoint of theMetal3Cluster must point to the same endpoint that the ByoCluster isusing. apiVersion: infrastructure. cluster. x-k8s. io/v1beta1kind: Metal3Clustermetadata: name: mixed-clusterspec: controlPlaneEndpoint:  host: 192. 168. 10. 20  port: 6443 noCloudProvider: true IPPools     apiVersion: ipam. metal3. io/v1alpha1kind: IPPoolmetadata: name: provisioning-poolspec: clusterName: mixed-cluster namePrefix: test1-prov pools: - end: 172. 22. 0. 200  start: 172. 22. 0. 100 prefix: 24---apiVersion: ipam. metal3. io/v1alpha1kind: IPPoolmetadata: name: baremetalv4-poolspec: clusterName: mixed-cluster gateway: 192. 168. 111. 1 namePrefix: test1-bmv4 pools: - end: 192. 168. 111. 200  start: 192. 168. 111. 100 prefix: 24   These manifests are quite large but they are just the same as would beused by the metal3-dev-env with some name changes here and there. Thekey thing to note is that all references to a Cluster are to the one wedefined above. Here is the MachineDeployment: apiVersion: cluster. x-k8s. io/v1beta1kind: MachineDeploymentmetadata: labels:  cluster. x-k8s. io/cluster-name: mixed-cluster  nodepool: nodepool-0 name: test1spec: clusterName: mixed-cluster replicas: 1 selector:  matchLabels:   cluster. x-k8s. io/cluster-name: mixed-cluster   nodepool: nodepool-0 template:  metadata:   labels:    cluster. x-k8s. io/cluster-name: mixed-cluster    nodepool: nodepool-0  spec:   bootstrap:    configRef:     apiVersion: bootstrap. cluster. x-k8s. io/v1beta1     kind: KubeadmConfigTemplate     name: test1-workers   clusterName: mixed-cluster   infrastructureRef:    apiVersion: infrastructure. cluster. x-k8s. io/v1beta1    kind: Metal3MachineTemplate    name: test1-workers   nodeDrainTimeout: 0s   version: v1. 23. 5Finally, we add the Metal3MachineTemplate, Metal3DataTemplate andKubeadmConfigTemplate. Here you may want to add your public ssh key inthe KubeadmConfigTemplate (the last few lines).  Metal3MachineTemplate, Metal3DataTemplate and KubeadmConfigTemplate      #apiVersion: infrastructure. cluster. x-k8s. io/v1beta1kind: Metal3MachineTemplatemetadata: name: test1-workersspec: template:  spec:   dataTemplate:    name: test1-workers-template   image:    checksum: http://172. 22. 0. 1/images/UBUNTU_22. 04_NODE_IMAGE_K8S_v1. 23. 5-raw. img. md5sum    checksumType: md5    format: raw    url: http://172. 22. 0. 1/images/UBUNTU_22. 04_NODE_IMAGE_K8S_v1. 23. 5-raw. img---apiVersion: infrastructure. cluster. x-k8s. io/v1beta1kind: Metal3DataTemplatemetadata: name: test1-workers-template namespace: metal3spec: clusterName: mixed-cluster metaData:  ipAddressesFromIPPool:  - key: provisioningIP   name: provisioning-pool  objectNames:  - key: name   object: machine  - key: local-hostname   object: machine  - key: local_hostname   object: machine  prefixesFromIPPool:  - key: provisioningCIDR   name: provisioning-pool networkData:  links:   ethernets:   - id: enp1s0    macAddress:     fromHostInterface: enp1s0    type: phy   - id: enp2s0    macAddress:     fromHostInterface: enp2s0    type: phy  networks:   ipv4:   - id: baremetalv4    ipAddressFromIPPool: baremetalv4-pool    link: enp2s0    routes:    - gateway:      fromIPPool: baremetalv4-pool     network: 0. 0. 0. 0     prefix: 0  services:   dns:   - 8. 8. 8. 8---apiVersion: bootstrap. cluster. x-k8s. io/v1beta1kind: KubeadmConfigTemplatemetadata: name: test1-workersspec: template:  spec:   files:   - content: |     network:      version: 2      renderer: networkd      bridges:       ironicendpoint:        interfaces: [enp1s0]        addresses:        - {{ ds. meta_data. provisioningIP }}/{{ ds. meta_data. provisioningCIDR }}    owner: root:root    path: /etc/netplan/52-ironicendpoint. yaml    permissions:  0644    - content: |     [registries. search]     registries = ['docker. io']     [registries. insecure]     registries = ['192. 168. 111. 1:5000']    path: /etc/containers/registries. conf   joinConfiguration:    nodeRegistration:     kubeletExtraArgs:      cgroup-driver: systemd      container-runtime: remote      container-runtime-endpoint: unix:///var/run/crio/crio. sock      feature-gates: AllAlpha=false      node-labels: metal3. io/uuid={{ ds. meta_data. uuid }}      provider-id: metal3://{{ ds. meta_data. uuid }}      runtime-request-timeout: 5m     name:  {{ ds. meta_data. name }}    preKubeadmCommands:   - netplan apply   - systemctl enable --now crio kubelet   users:   - name: metal3    # sshAuthorizedKeys:    # - add your public key here for debugging    sudo: ALL=(ALL) NOPASSWD:ALL#   The result of all this is a Cluster with two Machines, one from theMetal³ provider and one from the BYOH provider. $ k -n metal3 get machineNAME                CLUSTER     NODENAME        PROVIDERID                   PHASE   AGE   VERSIONmixed-cluster-control-plane-48qmm  mixed-cluster  control-plane1     byoh://control-plane1/jf5uye          Running  7m41s  v1. 23. 5test1-8767dbccd-24cl5        mixed-cluster  test1-8767dbccd-24cl5  metal3://0642d832-3a7c-4ce9-833e-a629a60a455c  Running  7m18s  v1. 23. 5Let’s also check that the workload cluster is functioning as expected. Get the kubeconfig and add Calico as CNI. clusterctl get kubeconfig -n metal3 mixed-cluster &gt; kubeconfig. yamlexport KUBECONFIG=kubeconfig. yamlkubectl apply -f https://docs. projectcalico. org/v3. 20/manifests/calico. yamlNow check the nodes. $ kubectl get nodesNAME          STATUS  ROLES         AGE  VERSIONcontrol-plane1     Ready  control-plane,master  88m  v1. 23. 5test1-8767dbccd-24cl5  Ready  &lt;none&gt;         82m  v1. 23. 5Going back to the management cluster, we can inspect the state of thecluster API resources. $ clusterctl -n metal3 describe cluster mixed-clusterNAME                                    READY SEVERITY REASON SINCE MESSAGECluster/mixed-cluster                            True           13m├─ClusterInfrastructure - ByoCluster/mixed-cluster├─ControlPlane - KubeadmControlPlane/mixed-cluster-control-plane      True           13m│ └─Machine/mixed-cluster-control-plane-hp2fp                True           13m│  └─MachineInfrastructure - ByoMachine/mixed-cluster-control-plane-vxft5└─Workers └─MachineDeployment/test1                         True           3m57s  └─Machine/test1-7f77dfb7c8-j7x4q                    True           9m32sConclusion: As we have seen in this post, it is possible to combine at least someinfrastructure providers when creating a single cluster. This can beuseful for example if a provider has a high cost or limited resources. Furthermore, the use case is not addressed by MachineDeployments sincethey would all be from the same provider (even though they can havedifferent properties). There is some room for development and improvement though. The mostobvious thing is perhaps that Clusters only have oneinfrastructureRef. This means that the cluster API controllers are notaware of the “secondary” infrastructure provider(s). Another thing that may be less obvious is the reliance on Nodes andMachines in the Kubeadm control plane provider. It is not an issue inthe example we have seen here since both Metal³ and BYOH creates Nodes. However, there are some projects where Nodes are unnecessary. See forexample Kamaji, which aims tointegrate with the cluster API. The idea here is to run the controlplane components in the management cluster as Pods. Naturally, therewould not be any control plane Nodes or Machines in this case. (A secondprovider would be used to add workers. ) But the Kubeadm control planeprovider expects there to be both Machines and Nodes for the controlplane, so a new provider is likely needed to make this work as desired. This issue can already be seen in thevclusterprovider, where the Cluster stays in Provisioning state because it is“Waiting for the first control plane machine to have itsstatus. nodeRef set”. The idea with vcluster is to reuse the Nodes ofthe management cluster but provide a separate control plane. This givesusers better isolation than just namespaces without the need for another“real” cluster. It is for example possible to have different customresource definitions in each vcluster. But since vcluster runs all thepods (including the control plane) in the management cluster, there willnever be a control plane Machine or nodeRef. There is already one implementation of a control plane provider withoutNodes, i. e. the EKS provider. Perhaps this is the way forward. Oneimplementation for each specific case. It would be nice if it waspossible to do it in a more generic way though, similar to how theKubeadm control plane provider is used by almost all infrastructureproviders. To summarize, there is already some support for mixed clusters withmultiple providers. However, there are some issues that make itunnecessarily awkward. Two things that could be improved in the clusterAPI would be the following:  Make the cluster. infrastructureRef into a list to allow multipleinfrastructure providers to be registered.  Drop the assumption that there will always be control plane Machinesand Nodes (e. g. by implementing a new control plane provider). "
    }, {
    "id": 10,
    "url": "/blog/2021/05/05/Pivoting.html",
    "title": "Metal3 Introduces Pivoting",
    "author" : "Kashif Nizam Khan",
    "tags" : "metal3, baremetal, Pivoting, Move",
    "body": "Metal3 project has introduced pivoting in its CI workflow. The motivation forpivoting is to move all the objects from the ephemeral/managementcluster to a target cluster. This blog post will briefly introduce the conceptof pivoting and the impact it has on the overall CI workflow. For the rest ofthis blog, we refer ephemeral/management cluster as an ephemeral cluster. What is Pivoting?: In the context of Metal3 Provider, Pivoting is the process of movingCluster-API and Metal3 objects from the ephemeral k8s cluster to a targetcluster. In Metal3, this process is performed using theclusterctl toolprovided by Cluster-API. clusterctl recognizes pivoting as a move. During thepivot process, clusterctl pauses any reconciliation of Cluster-API objects andthis gets propagated to Cluster-api-provider-metal3 (CAPM3) objects as well. Once all the objects are paused, the objects are created on the other side onthe target cluster and deleted from the ephemeral cluster. Prerequisites: Prior to the actual pivot process, the target cluster should already have theprovider components, ironic containers and CNI installed and running. To performpivot outside metal3-dev-env, specifically, the following points need to beaddressed:  clusterctl is used to initialize both the ephemeral and target cluster.  BMH objects have correct status annotation.  Maintain connectivity towards the provisioning network.  Baremetal Operator(BMO) is deployed as part of CAPM3.  Objects should have a proper owner reference chain. For a detailed explanation of the above-mentioned prerequisites please read thepivoting documentation. Pivoting workflow in CI: The Metal3 CI currently includes pivoting as part of the deploymentprocess both for Ubuntu and CentOS-based jobs. This essentially means allthe PRs that go in, are tested through the pivoting workflow. Here is theCI deployment workflow:  make the metal3-dev-env. It gives us the ephemeral cluster with all the necessary controllers runningwithin it. The corresponding metal3-dev-env command is make provision target cluster. For normal integration tests, this step deploysa control-plane node and a worker in the target cluster. For, feature-testand feature-test-upgrade the provision step deploys three control-planes anda worker. The corresponding metal3-dev-env commands are (normal integrationtest workflow):. /scripts/provision/cluster. sh. /scripts/provision/controlplane. sh. /scripts/provision/worker. sh Initialize the provider components on the target cluster. This installs allthe controllers and associated components related to cluster-api ,cluster-api-provider-metal3, baremetal-operator and ironic. Since it isnecessary to have only one set of ironic deployment/containers in the picture,this step also deletes the ironic deployment/containers fromephemeral cluster.  Move all the objects from ephemeral to the target cluster.  Check the status of the objects to verify whether the objects are beingreconciled correctly by the controllers in the target cluster. This stepverifies and finalizes the pivoting process. The corresponding metal3-dev-envthe command that performs this and the previous two steps is :. /scripts/feature_tests/pivoting/pivot. sh Move the objects back to the ephemeral cluster. This step alsoremoves the ironic deployment from the target cluster and reinstates theironic deployment/containers in the ephemeral cluster. Since we donot delete the provider components in the ephemeral cluster,installing them again is not necessary. The corresponding metal3-dev-env commandthat performs this step is :. /scripts/feature_tests/pivoting/repivot. sh De-provision the BMHs and delete the target cluster. The correspondingmetal3-dev-env commands to de-provision worker, controlplane and the clusteris as follows:. /scripts/deprovision/worker. sh. /scripts/deprovision/controlplane. sh. /scripts/deprovision/cluster. shNote that, if we de-provision cluster, that would de-provision worker andcontrolplane automatically. Pivoting in Metal3: The pivoting process described above is realized in ansible scriptsmove. ymlandmove_back. yml. Under the hood, pivoting uses the move command fromclusterctlprovided by Cluster-API. As stated earlier, all the PRs that go into any Metal3 repository where theintegration tests are run, the code change introduced in the PR is verified withpivoting also in the integration tests now. Moreover, the upgrade workflow inMetal3 performs all the upgrade operations in Metal3 after pivoting to thetarget cluster. "
    }, {
    "id": 11,
    "url": "/blog/2020/07/06/IP_address_manager.html",
    "title": "Introducing the Metal3 IP Address Manager",
    "author" : "Maël Kimmerlin",
    "tags" : "metal3, baremetal, IPAM, ip address manager",
    "body": "As a part of developing the Cluster API Provider Metal3 (CAPM3) v1alpha4release, the Metal3 crew introduced a new project: its own IP Address Manager. This blog post will go through the motivations behind such a project, thefeatures that it brings, its use in Metal3 and future work. What is the IP Address Manager?: The IP Address Manager (IPAM) is a controller that provides IP addresses andmanages the allocations of IP subnets. It is not a DHCP server in that it onlyreconciles Kubernetes objects and does not answer any DHCP queries. Itallocates IP addresses on request but does not handle any use of thoseaddresses. This sounds like the description of any IPAM system, no? Well, the twistis that this manager is based on Kubernetes to specifically handle someconstraints from Metal3. We will go through the different issues that thisproject tackles. When deploying nodes in a bare metal environment, there are a lot of possiblevariations. This project specifically aims to solve cases where staticIP address configurations are needed. It is designed to specifically addressthis in the Cluster API (CAPI) context. CAPI addresses the deployment of Kubernetes clusters and nodes, usingthe Kubernetes API. As such, it uses objects such as Machine Deployments(similar to deployments for pods) that takes care of creating the requestednumber of machines, based on templates. The replicas can be increased by theuser, triggering the creation of new machines based on the provided templates. This mechanism does not allow for flexibility to be able to provide staticaddresses for each machine. The manager adds this flexibility by providingthe address right before provisioning the node. In addition, all the resources from the source cluster must support the CAPIpivoting, i. e. being copied and recreated in the target cluster. This meansthat all objects must contain all needed information in their spec field torecreate the status in the target cluster without losing information. Allobjects must, through a tree of owner references, be attached to the clusterobject, for the pivoting to proceed properly. In a nutshell, the manager provides an IP Address allocation service, basedon Kubernetes API and fulfilling the needs of Metal3, specifically therequirements of CAPI. How does it work?: The manager follows the same logic as the volume allocation in Kubernetes,with a claim and an object created for that claim. There are three types ofobjects defined, the IPPool, the IPClaim and the IPAddress objects. The IPPool objects contain the definition of the IP subnets from which theAddresses are allocated. It supports both IPv4 and IPv6. The subnets can eitherbe defined as such or given as start and end IP addresses with a prefix. It also supports pre-allocating IP addresses. The following is an example IPPool definition : apiVersion: ipam. metal3. io/v1alpha1kind: IPPoolmetadata: name: pool1spec: clusterName: cluster1 pools: - start: 192. 168. 0. 10  end: 192. 168. 0. 30  prefix: 25  gateway: 192. 168. 0. 1 - subnet: 192. 168. 1. 1/26 - subnet: 192. 168. 1. 128/25 prefix: 24 gateway: 192. 168. 1. 1 preAllocations:  claim2: 192. 168. 0. 12An IPv6 IPPool would be defined similarly : apiVersion: ipam. metal3. io/v1alpha1kind: IPPoolmetadata: name: pool1spec: clusterName: cluster1 pools: - start: 2001:0db8:85a3:0000:0000:8a2e::10  end: 2001:0db8:85a3:0000:0000:8a2e:ffff:fff0  prefix: 96  gateway: 12001:0db8:85a3:0000:0000:8a2e::1 - subnet: 2001:0db8:85a3:0000:0000:8a2d::/96 prefix: 96 gateway: 2001:0db8:85a3:0000:0000:8a2d::1Whenever something requires an IP address from the IPPool, it will create anIPClaim. The IPClaim contains a pointer to the IPPool and an owner referenceto the object that created it. The following is an example of an IPClaim: apiVersion: ipam. metal3. io/v1alpha1kind: IPClaimmetadata: name: claim1spec: pool:  Name: pool1status: address:  Name: pool1-192-168-0-13The controller will then reconcile this object and allocate an IP address. Itwill create an IPAddress object representing the allocated address. It willthen update the IPPool status to list the IP Address and the IPClaim statusto point to the IPAddress. The following is an example of an IPAddress: apiVersion: ipam. metal3. io/v1alpha1kind: IPAddressmetadata: name: pool1-192-168-0-13spec: pool:  Name: pool1 claim:  Name: claim1 address: 192. 168. 0. 13 prefix: 24 gateway: 192. 168. 0. 1After this allocation, the IPPool will be looking like this: apiVersion: ipam. metal3. io/v1alpha1kind: IPPoolmetadata: name: pool1spec: clusterName: cluster1 pools: - start: 192. 168. 0. 10  end: 192. 168. 0. 30  prefix: 25  gateway: 192. 168. 0. 1 - subnet: 192. 168. 1. 1/26 - subnet: 192. 168. 1. 128/25 prefix: 24 gateway: 192. 168. 1. 1 preAllocations:  claim2: 192. 168. 0. 12status: indexes:  claim1: 192. 168. 0. 13  claim2: 192. 168. 0. 12Use in Metal3: The IP Address Manager is used in Metal3 together with the metadata and networkdata templates feature. Each Metal3Machine (M3M) and Metal3MachineTemplate(M3MT) is associated with a Metal3DataTemplate that contains metadata and /or a network data template that will be rendered for each Metal3Machine. Therendered data will then be provided to Ironic. Those templates referenceIPPool objects. For each Metal3Machine, an IPClaim is created for eachIPPool, and the templates are rendered with the allocated IPAddress. This is how we achieve dynamic IP Address allocations in setups thatrequire static configuration, allowing us to use Machine Deployment and KubeadmControl Plane objects from CAPI in hardware labs where DHCP is not supported. Since each IPAddress has an owner reference set to its IPClaim object, andIPClaim objects have an owner reference set to the Metal3Data object createdfrom the Metal3DataTemplate, the owner reference chain links a Metal3Machine toall the IPClaim and IPAddress objects were created for it, allowing for CAPIpivoting. What now?: The project is fulfilling its basic requirements, but we are looking intoextending it and covering more use cases. For example, we are looking atadding integration with Infoblox and other external IPAM services. Do nothesitate to open an issue if you have some ideas for new features! The project can be foundhere. "
    }, {
    "id": 12,
    "url": "/blog/2020/07/05/raw-image-streaming.html",
    "title": "Raw image streaming available in Metal3",
    "author" : "Maël Kimmerlin",
    "tags" : "metal3, baremetal, raw image, image streaming",
    "body": "Metal3 supports multiple types of images for deployment, the mostpopular being QCOW2. We have recently added support for a feature of Ironicthat improves deployments on constrained environments, raw image streaming. We’ll first dive into how Ironic deploys the images on the target hosts, andhow raw image streaming improves this process. Afterwards, we will point outthe changes to take this into use in Metal3. Image deployments with Ironic: In Metal3, the image deployment is performed by the Ironic Python Agent (IPA)image running on the target host. In order to deploy an image, Ironic willfirst boot the target node with an IPA image over iPXE. IPA will run in memory. Once IPA runs on the target node, Ironic will instruct it to download thetarget image. In Metal3, we use HTTP(S) for the download of the image. IPA willdownload the image and, depending on the format of the image, prepare it towrite on the disk. This means that the image is downloaded in memory anddecompressed, two steps that can be both time and memory consuming. In order to improve this process, Ironic implemented a feature called raw imagestreaming. What is raw image streaming?: The target image format when writing to disk is raw. That’s why the images informats like QCOW2 must be processed before being written to disk. However, ifthe image that is downloaded is already in raw format, then no processing isneeded. Ironic leverages this, and instead of first downloading the image and thenprocessing it before writing it to disk, it will directly write thedownloaded image to the disk. This feature is known as image streaming. Image streaming can only be performed with images in raw format. Since the downloaded image when streamed is directly written to disk, thememory size requirements change. For any other format than raw, the targethost needs to have sufficient memory to both run IPA (4GB) anddownload the image in memory. However, with raw images, the only constrainton memory is to run IPA (so 4GB). For example, in order to deploy an Ubuntuimage (around 700MB, QCOW2), the requirement is 8GB when in QCOW2 format, whileit is only 4GB (as for any other image) when streamed as raw. This allowsthe deployment of images that are bigger than the available memory onconstrained nodes. However, this shifts the load on the network, since the raw images are usuallymuch bigger than other formats. Using this feature in network constrainedenvironment is not recommended. Raw image streaming in Metal3: In order to use raw image streaming in Metal3, a couple of steps are needed. The first one is to convert the image to raw and make it available in anHTTP server. This can be achieved by running : qemu-img convert -O raw  ${IMAGE_NAME}   ${IMAGE_RAW_NAME} Once converted the image format needs to be provided to Ironic through theBareMetalHost (BMH) image spec field. If not provided, Ironic will assume thatthe format is unspecified and download it in memory first. The following is an example of the BMH image spec field in Metal3 Dev Env. apiVersion: metal3. io/v1alpha1kind: BareMetalHostspec: image:  format: raw  url: http://172. 22. 0. 1/images/bionic-server-cloudimg-amd64-raw. img  checksum: http://172. 22. 0. 1/images/bionic-server-cloudimg-amd64-raw. img. md5sum  checksumType: md5If deploying with Cluster API provider Metal3 (CAPM3), CAPM3 takes care ofsetting the image field of BMH properly, based on the image field values inthe Metal3Machine (M3M), which might be based on a Metal3MachineTemplate (M3MT). So in order to use raw image streaming, the format of the image must beprovided in the image spec field of the Metal3Machine or Metal3MachineTemplate. The following is an example of the M3M image spec field in metal3-dev-env : apiVersion: infrastructure. cluster. x-k8s. io/v1alpha3kind: Metal3Machinespec: image:  format: raw  url: http://172. 22. 0. 1/images/bionic-server-cloudimg-amd64-raw. img  checksum: http://172. 22. 0. 1/images/bionic-server-cloudimg-amd64-raw. img. md5sum  checksumType: md5The following is for a M3MT in metal3-dev-env : apiVersion: infrastructure. cluster. x-k8s. io/v1alpha3kind: Metal3MachineTemplatespec: template:  spec:   image:    format: raw    url: http://172. 22. 0. 1/images/bionic-server-cloudimg-amd64-raw. img    checksum: http://172. 22. 0. 1/images/bionic-server-cloudimg-amd64-raw. img. md5sum    checksumType: md5This will enable raw image streaming. By default, metal3-dev-env uses the raw imagestreaming, in order to minimize the resource requirements of the environment. In a nutshell: With the addition of raw image streaming, Metal3 now supports a wider range ofhardware, specifically, the memory-constrained nodes and speeds up deployments. Metal3 still supports all the other formats it supported until now. This newfeature changes the way raw images are deployed for better efficiency. "
    }, {
    "id": 13,
    "url": "/blog/2020/06/18/Metal3-dev-env-BareMetal-Cluster-Deployment.html",
    "title": "Metal³ development environment walkthrough part 2: Deploying a new bare metal cluster",
    "author" : "Himanshu Roy",
    "tags" : "metal3, kubernetes, cluster API, metal3-dev-env",
    "body": "Introduction: This blog post describes how to deploy a bare metal cluster, a virtualone for simplicity, usingMetal³/metal3-dev-env. Wewill briefly discuss the steps involved in setting up the cluster aswell as some of the customization available. If you want to know moreabout the architecture of Metal³, this blogpost can be helpful. This post builds upon the detailed metal3-dev-env walkthroughblogpostwhich describes in detail the steps involved in the environment set-upand management cluster configuration. Here we will use that environmentto deploy a new Kubernetes cluster using Metal³. Before we get started, there are a couple of requirements we areexpecting to be fulfilled. Requirements:  Metal³ is already deployed and working, if not please follow theinstructions in the previously mentioned detailed metal3-dev-envwalkthrough blogpost.  The appropriate environment variables are setup via shell or in theconfig_${user}. sh file, for example     CAPM3_VERSION   NUM_NODES   CLUSTER_NAME   Overview of Config and Resource types: In this section, we give a brief overview of the important config filesand resources used as part of the bare metal cluster deployment. Thefollowing sub-sections show the config files and resources that arecreated and give a brief description of some of them. This will help youunderstand the technical details of the cluster deployment. You can alsochoose to skip this section, visit the next section about provisioningfirst and then revisit this. Config Files and Resources Types:  info “Information” Among these the config files are rendered under thepathhttps://github. com/metal3-io/metal3-dev-env/tree/master/vm-setup/roles/v1aX_integration_test/filesas part of the provisioning process. A description of some of the files part of provisioning a cluster, in acentos-based environment:       Name   Description   Path         provisioning scripts   Scripts to trigger provisioning of cluster, control plane or worker   ${metal3-dev-env}/scripts/provision/       deprovisioning scripts   Scripts to trigger deprovisioning of cluster, control plane or worker   ${metal3-dev-env}/scripts/deprovision/       templates directory   Templates for cluster, control plane, worker definitions   ${metal3-dev-env}/tests/roles/run_tests/templates       clusterctl env file   Cluster parameters and details   ${Manifests}/clusterctl_env_centos. rc       generate templates   Renders cluster, control plane and worker definitions in the Manifest directory   ${metal3-dev-env}/tests/roles/run_tests/tasks/generate_templates. yml       main vars file   Variable file that assigns all the defaults used during deployment   ${metal3-dev-env}/tests/roles/run_tests/vars/main. yml   Here are some of the resources that are created as part of provisioning :       Name   Description         Cluster   a Cluster API resource for managing a cluster       Metal3Cluster   Corresponding Metal3 resource generated as part of bare metal cluster deployment, and managed by Cluster       KubeadmControlPlane   Cluster API resource for managing the control plane, it also manages the Machine object, and has the KubeadmConfig       MachineDeployment   Cluster API resource for managing workers via MachineSet object, it can be used to add/remove workers by scaling Up/Down       MachineSet   Cluster API resource for managing Machine objects for worker nodes       Machine   Cluster API resource for managing nodes - control plane or workers. In case of Controlplane, its directly managed by KubeadmControlPlane, whereas for Workers it’s managed by a MachineSet       Metal3Machine   Corresponding Metal3 resource for managing bare metal nodes, it’s managed by a Machine resource       Metal3MachineTemplate   Metal3 resource which acts as a template when creating a control plane or a worker node       KubeadmConfigTemplate   A template of KubeadmConfig, for Workers, used to generate KubeadmConfig when a new worker node is provisioned   Note The corresponding KubeadmConfig is copied to the controlplane/worker at the time of provisioning. Bare Metal Cluster Deployment: The deployment scripts primarily use ansible and the existing Kubernetesmanagement cluster (based on minikube ) for deploying the bare-metalcluster. Make sure that some of the environment variables used forMetal³ deployment are set, if you didn’t use config_${user}. sh forsetting the environment variables.       Parameter   Description   Default         CAPM3_VERSION   Version of Metal3 API   v1alpha3       POD_CIDR   Pod Network CIDR   192. 168. 0. 0/18       CLUSTER_NAME   Name of bare metal cluster   test1   === Steps Involved: All the scripts for cluster provisioning or de-provisioning are locatedat -${metal3-dev-env}/scripts/. The scripts call a common playbook which handles all the tasks that areavailable. The steps involved in the process are:  The script calls an ansible playbook with necessary parameters ( fromenv variables and defaults ) The playbook executes the role -,${metal3-dev-env}/tests/roles/run_tests,which runs the maintask_filefor provisioning/deprovisioning the cluster, control plane or a worker There aretemplatesin the role, which are used to render configurations in the Manifestdirectory. These configurations use kubeadm and are supplied to theKubernetes module of ansible to create the cluster.  During provisioning, first the clusterctl env file is generated,then the cluster, control plane and worker definition templates forclusterctl are generated at${HOME}/. cluster-api/overrides/infrastructure-metal3/${CAPM3RELEASE}.  Using the templates generated in the previous step, the definitionsfor resources related to cluster, control plane and worker arerendered using clusterctl.  Centos or Ubuntu image isdownloadedin the next step.  Finally using the above definitions, which are passed to the K8smodule in ansible, the corresponding resource( cluster/controlplane/worker ) is provisioned.  These same definitions are reused at the time of de-provisioning thecorresponding resource, again using the K8s module in ansible     note “Note” The manifest directory is created when provisioning istriggered for the first time and is subsequently used to store theconfig files that are rendered for deploying the bare metal cluster.     Provision Cluster: This script, located at the path -${metal3-dev-env}/scripts/provision/cluster. sh, provisions the clusterby creating a Metal3Cluster and a Cluster resource. To see if you have a successful Cluster resource creation( the clusterstill doesn’t have a control plane or workers ), just do: kubectl get Metal3Cluster ${CLUSTER_NAME} -n metal3 This will return the cluster deployed, and you can check the clusterdetails by describing the returned resource. Here is what a Cluster resource looks like: kubectl describe Cluster ${CLUSTER_NAME} -n metal3apiVersion: cluster. x-k8s. io/v1alpha3kind: Clustermetadata: #[. . . . ]spec: clusterNetwork:  pods:   cidrBlocks:   - 192. 168. 0. 0/18  services:   cidrBlocks:   - 10. 96. 0. 0/12 controlPlaneEndpoint:  host: 192. 168. 111. 249  port: 6443 controlPlaneRef:  apiVersion: controlplane. cluster. x-k8s. io/v1alpha3  kind: KubeadmControlPlane  name: bmetalcluster  namespace: metal3 infrastructureRef:  apiVersion: infrastructure. cluster. x-k8s. io/v1alpha3  kind: Metal3Cluster  name: bmetalcluster  namespace: metal3status: infrastructureReady: true phase: ProvisionedProvision Controlplane: This script, located at the path -${metal3-dev-env}/scripts/provision/controlplane. sh, provisions thecontrol plane member of the cluster using the rendered definition of thecontrol plane explained in the Steps Involved section. TheKubeadmControlPlane creates a Machine which picks up a BareMetalHostsatisfying its requirements as the control plane node, and it is thenprovisioned by the Bare Metal Operator. A Metal3MachineTemplateresource is also created as part of the provisioning process. Note It takes some time for the provisioning of the control plane, you canwatch the process using some steps shared a bit later kubectl get KubeadmControlPlane ${CLUSTER_NAME} -n metal3kubectl describe KubeadmControlPlane ${CLUSTER_NAME} -n metal3apiVersion: controlplane. cluster. x-k8s. io/v1alpha3kind: KubeadmControlPlanemetadata: #[. . . . ] ownerReferences: - apiVersion: cluster. x-k8s. io/v1alpha3  blockOwnerDeletion: true  controller: true  kind: Cluster  name: bmetalcluster  uid: aec0f73b-a068-4992-840d-6330bf943d22 resourceVersion:  44555  selfLink: /apis/controlplane. cluster. x-k8s. io/v1alpha3/namespaces/metal3/kubeadmcontrolplanes/bmetalcluster uid: 99487c75-30f1-4765-b895-0b83b0e5402bspec: infrastructureTemplate:  apiVersion: infrastructure. cluster. x-k8s. io/v1alpha3  kind: Metal3MachineTemplate  name: bmetalcluster-controlplane  namespace: metal3 kubeadmConfigSpec:  files:  - content: #[. . . . ] replicas: 1 version: v1. 18. 0status: replicas: 1 selector: cluster. x-k8s. io/cluster-name=bmetalcluster,cluster. x-k8s. io/control-plane= unavailableReplicas: 1 updatedReplicas: 1kubectl get Metal3MachineTemplate ${CLUSTER_NAME}-controlplane -n metal3To track the progress of provisioning, you can try the following: kubectl get BareMetalHosts -n metal3 -w The BareMetalHosts resource is created when Metal³/metal3-dev-envwas deployed. It is a kubernetes resource that represents a bare metalMachine, with all its details and configuration, and is managed by theBare Metal Operator. You can also use the short representationinstead, i. e. bmh ( short for BareMetalHosts) in the commandabove. You should see all the nodes that were created at the time of metal3deployment, along with their current status as the provisioningprogresses Note All the bare metal hosts listed above were created when Metal³ wasdeployed in the detailed metal3-dev-env walkthrough blogpost. kubectl get Machine -n metal3 -w This shows the status of the Machine associated with the control planeand we can watch the status of provisioning under PHASE Once the provisioning is finished, let’s get the host-ip: sudo virsh net-dhcp-leases baremetalInformation baremetal is one of the 2 networks that were created at the time ofMetal3 deployment, the other being “provisioning” which is used - asyou have guessed - for provisioning the bare metal cluster. Moredetails about networking setup in the metal3-dev-env environment aredescribed in the - detailed metal3-dev-env walkthroughblogpost. You can log in to the control plane node if you want, and can check thedeployment status using two methods. ssh metal3@{control-plane-node-ip}ssh metal3@192. 168. 111. 249Provision Workers: The script is located at${metal3-dev-env-path}/scripts/provision/worker. sh and it provisions anode to be added as a worker to the bare metal cluster. It selects oneof the remaining nodes and provisions it and adds it to the bare metalcluster ( which only has a control plane node at this point ). Theresources created for workers are - MachineDeployment which can bescaled up to add more workers to the cluster and MachineSet which thencreates a Machine managing the node. Information Similar to control plane provisioning, worker provisioning also takessome time, and you can watch the process using steps shared a bitlater. This will also apply when you scale Up/Down workers at a laterpoint in time. This is what a MachineDeployment looks like kubectl describe MachineDeployment ${CLUSTER_NAME} -n metal3apiVersion: cluster. x-k8s. io/v1alpha3kind: MachineDeploymentmetadata: #[. . . . ] ownerReferences: - apiVersion: cluster. x-k8s. io/v1alpha3  kind: Cluster  name: bmetalcluster  uid: aec0f73b-a068-4992-840d-6330bf943d22 resourceVersion:  66257  selfLink: /apis/cluster. x-k8s. io/v1alpha3/namespaces/metal3/machinedeployments/bmetalcluster uid: f598da43-0afe-44e4-b793-cd5244c13f4espec: clusterName: bmetalcluster minReadySeconds: 0 progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 1 selector:  matchLabels:   cluster. x-k8s. io/cluster-name: bmetalcluster   nodepool: nodepool-0 strategy:  rollingUpdate:   maxSurge: 1   maxUnavailable: 0  type: RollingUpdate template:  metadata:   labels:    cluster. x-k8s. io/cluster-name: bmetalcluster    nodepool: nodepool-0  spec:   bootstrap:    configRef:     apiVersion: bootstrap. cluster. x-k8s. io/v1alpha3     kind: KubeadmConfigTemplate     name: bmetalcluster-workers   clusterName: bmetalcluster   infrastructureRef:    apiVersion: infrastructure. cluster. x-k8s. io/v1alpha3    kind: Metal3MachineTemplate    name: bmetalcluster-workers   version: v1. 18. 0status: observedGeneration: 1 phase: ScalingUp replicas: 1 selector: cluster. x-k8s. io/cluster-name=bmetalcluster,nodepool=nodepool-0 unavailableReplicas: 1 updatedReplicas: 1To check the status we can follow steps similar to Controlplane case: kubectl get bmh -n metal3 -w We can see the live status of the node being provisioned. As mentionedbefore bmh is the short representation of BareMetalHosts. kubectl get Machine -n metal3 -w This shows the status of Machines associated with workers, apart fromthe one for Controlplane, and we can watch the status of provisioningunder PHASE sudo virsh net-dhcp-leases baremetal To get the node’s IP ssh metal3@{control-plane-node-ip}kubectl get nodes To check if it’s added to the cluster ssh metal3@{node-ip} If you want to log in to the node kubectl scale --replicas=3 MachineDeployment ${CLUSTER_NAME} -n metal3 We can add or remove workers to the cluster, and we can scale up theMachineDeployment up or down, in this example we are adding 2 moreworker nodes, making the total nodes = 3 Deprovisioning: All of the previous components have corresponding de-provisioningscripts which use config files, in the previously mentioned manifestdirectory, and use them to clean up the worker, control plane andcluster. This step will use the already generated cluster/control plane/workerdefinition file, and supply it to Kubernetes ansible module toremove/de-provision the resource. You can find it, under the Manifestdirectory, in the Snapshot shared at the beginning of this blogpostwhere we show the file structure. For example, if you wish to de-provision the cluster, you would do: sh ${metal3-dev-env-path}/scripts/deprovision/worker. shsh ${metal3-dev-env-path}/scripts/deprovision/controlplane. shsh ${metal3-dev-env-path}/scripts/deprovision/cluster. shNote The reason for running the deprovision/worker. sh anddeprovision/controlplane. sh scripts is that not all objects arecleared when we just run the deprovision/cluster. sh script. Following this, if you want to de-provision the control plane it isrecommended to de-provision the cluster itself since we can’tprovision a new control plane with the same cluster. For workerde-provisioning, we only need to run the worker script. The following video demonstrates all the steps to provision andde-provision a Kubernetes cluster explained above. Summary: In this blogpost we saw how to deploy a bare metal cluster once we havea Metal³(metal3-dev-env repo) deployed and by that point we will alreadyhave the nodes ready to be used for a bare metal cluster deployment. In the first section, we show the various configuration files,templates, resource types and their meanings. Then we see the commonsteps involved in the provisioning process. After that, we see a generaloverview of how all resources are related and at what point are theycreated - provision cluster/control plane/worker. In each of the provisioning sections, we see the steps to monitor theprovisioning and how to confirm if it’s successful or not, with briefexplanations wherever required. Finally, we see the de-provisioningsection which uses the resource definitions generated at the time ofprovisioning to de-provision cluster, control plane or worker. Here are a few resources which you might find useful if you want toexplore further, some of them have already been shared earlier.  Metal3-Documentation     Metal3-Try-it    Metal³/metal3-dev-env Detailed metal3-dev-env walkthrough blogpost Kubernetes Metal3 Talk Metal3-Docs-github"
    }, {
    "id": 14,
    "url": "/blog/2020/03/05/CAPI_provider_renaming.html",
    "title": "Cluster API provider renaming",
    "author" : "Maël Kimmerlin",
    "tags" : "metal3, baremetal, cluster API, provider",
    "body": "Renaming of Cluster API provider: Backwards compatibility for v1alpha3 There is no backwards compatibility between v1alpha3 and v1alpha2 releases ofthe Cluster API provider for Metal3. For the v1alpha3 release of Cluster API, the Metal3 provider was renamed fromcluster-api-provider-baremetal to cluster-api-provider-metal3. The CustomResource Definitions were also modified. This post dives into the changes. Repository renaming: From v1alpha3 onwards, the Cluster API provider will be developed incluster-api-provider-metal3. The v1alpha1 and v1alpha2 content will remain incluster-api-provider-baremetal. This repository will be archived but kept for the integration in metal3-dev-env. Custom Resource Definition modifications: The kind of Custom Resource Definition (CRD) has been modified for thefollowing objects:  BareMetalCluster -&gt; Metal3Cluster baremetalcluster -&gt; metal3cluster BareMetalMachine -&gt; Metal3Machine baremetalmachine -&gt; metal3machine BareMetalMachineTemplate -&gt; Metal3MachineTemplate baremetalmachinetemplate -&gt; metal3machinetemplateThe custom resources deployed need to be modified accordingly. Deployment modifications: The prefix of all deployed components for the Metal3 provider was modifiedfrom capbm- to capm3-. The namespace in which the components are deployed bydefault was modified from capbm-system to capm3-system. "
    }, {
    "id": 15,
    "url": "/blog/2020/02/27/talk-kubernetes-finland-metal3.html",
    "title": "Metal³: Kubernetes Native Bare Metal Cluster Management - Maël Kimmerlin - Kubernetes and CNCF Finland Meetup",
    "author" : "Alberto Losada",
    "tags" : "metal3, baremetal, talk, conference, kubernetes, meetup",
    "body": "Conference talk: Metal³: Kubernetes Native Bare Metal Cluster Management - Maël Kimmerlin: On the 20th of January at the Kubernetes and CNCF Finland Meetup, Maël Kimmerlin gave a brilliant presentation about the status of the Metal³ project. In this presentation, Maël starts giving a short introduction of the Cluster API project which provides a solid foundation to develop the Metal³ Bare Metal Operator (BMO). The talk basically focuses on the v1alpha2 infrastructure provider features from the Cluster API. Information The video recording from the “Kubernetes and CNCF Finland Meetup” is composed of three talks. The video embedded starts with Maël’s talk. warning “Warning”Playback of the video has been disabled by the author. Click on the play button and then on the “Watch this video on Youtube” link once it appears. During the first part of the presentation, a detailed explanation of the different Kubernetes Custom Resource Definitions (CRDs) inside Metal³ is shown as also how they are linked with the Cluster API project. As an example, the image below shows the interaction between objects and controllers from both projects: Once finished the introductory part, Maël focuses on the main components of the Metal³ BMO and the provisioning process. This process starts with introspection, where the bare metal server is registered by the operator. Then, the Ironic Python Agent (IPA) image is executed to collect all hardware information from the server.  The second part of the process is the provisioning. In this step, Maël explains how the Bare Metal Operator (BMO) is in charge along with Ironic to present the Operating System image to the physical server and complete its installation.  Next, Maël deeply explains each Custom Resource (CR) used during the provisioning of target Kubernetes clusters in bare metal servers. He refers to objects such as Cluster, BareMetalCluster, Machine, BareMetalMachine, BareMetalHost and so on. Each one is clarified with a YAML file definition of a real case and a workflow diagram that shows the reconciliation procedure. The last part of the talk is dedicated to executing a demo where Maël creates a target Kubernetes cluster from a running minikube VM (also called bootstrap cluster) where Metal³ is deployed. As it is pointed out in the video, the demo is running in emulated hardware. Actually, something similar to the metal3-dev-env project can be used to reproduce the demo. More information on the Metal³ development environment (metal3-dev-env) can be found in the Metal³ try-it section. In case you want to go deeper, take a look at the blog post A detailed walkthrough of the Metal³ development environment. In the end, the result is a new Kubernetes cluster up and running. The cluster is deployed on two emulated physical servers: one runs as the control-plane node and the other as a worker node. Information The slides of the talk can be downloaded from here Speakers: Maël Kimmerlin Maël Kimmerlin is a Senior Software Engineer at Ericsson. In his own words: I am an open-source enthusiast, focusing in Ericsson on Life Cycle Management of Kubernetes clusters on Bare Metal. I am very interested in the Cluster API project from the Kubernetes Lifecycle SIG, and active in its Bare Metal provider, that is Metal³, developing and encouraging the adoption of this project. References:  Video: Metal³: Kubernetes Native Bare Metal Cluster Management Slides"
    }, {
    "id": 16,
    "url": "/blog/2020/02/18/metal3-dev-env-install-deep-dive.html",
    "title": "A detailed walkthrough of the Metal³ development environment",
    "author" : "Alberto Losada",
    "tags" : "metal3, baremetal, metal3-dev-env, documentation, development",
    "body": "Introduction to metal3-dev-env: The metal3-dev-env is acollection of scripts in a GitHub repository inside theMetal³ project that aims toallow contributors and other interested users to run a fully functionalMetal³ environment for testing and have a first contact with theproject. Actually, metal3-dev-env sets up an emulated environmentwhich creates a set of virtual machines (VMs) to manage as if they werebare metal hosts. Warning This is not an installation that is supposed to be run in production. Instead, it is focused on providing a development environment to testand validate new features. The metal3-dev-env repository includes a set of scripts, libraries andresources used to set up a Metal³ development environment. On theMetal³ websitethere is already a documented process on how to use the metal3-dev-envscripts to set up a fully functional cluster to test the functionalityof the Metal³ components. This procedure at a 10,000-foot view is composed of 3 bash scripts plusa verification one:  01_prepare_host. sh - Mainly installs all needed packages.  02_configure_host. sh - Basically create a set of VMs that will bemanaged as if they were bare metal hosts. It also downloads someimages needed for Ironic.  03_launch_mgmt_cluster. sh - Launches a management cluster usingminikube and runs the baremetal-operator on that cluster.  04_verify. sh - Finally runs a set of tests that verify that thedeployment was completed successfullyIn this blog post, we are going to expand the information and providesome hints and recommendations. Warning Metal³ project is changing rapidly, so probably this information isvaluable in the short term. In any case, it is encouraged todouble-check that the information provided is still valid. Before getting down to it, it is worth defining the nomenclature used in the blog post:  Host. It is the server where the virtual environment is running. In this case, it is a physical PowerEdge M520 with 2 x Intel(R)Xeon(R) CPU E5-2450 v2 @ 2. 50GHz, 96GB RAM and a 140GB drive runningCentOS 7 latest. Do not panic, lab environment should work with lowerresources as well.  Virtual bare metal hosts. These are the virtual machines (KVMbased) that are running on the host which are emulating physical hostsin our lab. They are also called bare metal hosts even if they are notphysical servers.  Management or bootstrap cluster. It is a fully functionalKubernetes cluster in charge of running all the necessary Metal³operators and controllers to manage the infrastructure. In this caseit is the minikube virtual machine.  Target cluster. It is the Kubernetes cluster created from themanagement one. It is provisioned and configured using a nativeKubernetes API for that purpose. Create the Metal³ laboratory: Information A non-root user must exist in the host with password-less sudo access. This user is in charge of running the metal3-dev-env scripts. The first thing that needs to be done is, obviously, cloning themetal3-dev-env repository: [alosadag@eko1: ~]$ git clone https://github. com/metal3-io/metal3-dev-env. gitCloning into 'metal3-dev-env'. . . remote: Enumerating objects: 22, done. remote: Counting objects: 100% (22/22), done. remote: Compressing objects: 100% (22/22), done. remote: Total 1660 (delta 8), reused 8 (delta 0), pack-reused 1638Receiving objects: 100% (1660/1660), 446. 08 KiB | 678. 00 KiB/s, done. Resolving deltas: 100% (870/870), done. Before starting to deploy the Metal³ environment, it makes sense todetail a series of scripts inside the library folder that will besourced in every step of the installation process. They are calledshared libraries. [alosadag@eko1:~]$ ls -1 metal3-dev-env/lib/common. shimages. shlogging. shnetwork. shShared libraries: Although there are several scripts placed inside the lib folder that aresourced in some of the deployment steps, common. sh and logging. share the only ones used in all of the executions during the installationprocess. common. sh: The first time this library is run, a new configuration file is createdwith several variables along with their default values. They will beused during the installation process. On the other hand, if the filealready exists, then it just sources the values configured. Theconfiguration file is created inside the cloned folder withconfig_$USER as the file name. [alosadag@eko1 metal3-dev-env]$ ls config_*config_alosadag. shThe configuration file contains multiple variables that will be usedduring the set-up. Some of them are detailed in the setup section ofthe Metal³ try-it web page. In case you need to add or change global variables it should be done inthis config file. Note I personally recommend modifying or adding variables in this configfile instead of exporting them in the shell. By doing that, it isassured that they are persisted [alosadag@eko1 metal3-dev-env]$ cat ~/metal3-dev-env/config_alosadag. sh#!/bin/bash## This is the subnet used on the  baremetal  libvirt network, created as the# primary network interface for the virtual bare metalhosts. ## Default of 192. 168. 111. 0/24 set in lib/common. sh##export EXTERNAL_SUBNET= 192. 168. 111. 0/24 ## This SSH key will be automatically injected into the provisioned host# by the provision_host. sh script. ## Default of ~/. ssh/id_rsa. pub is set in lib/common. sh##export SSH_PUB_KEY=~/. ssh/id_rsa. pub. . . This common. sh library also makes sure there is an ssh public keyavailable in the user’s ssh folder. This key will be injected bycloud-init in all the virtual bare metal machines that will beconfigured later. Then, the user that executed the metal3-dev-envscripts is able to access the target cluster through ssh. Also, common. sh library also sets more global variables apart fromthose in the config file. Note that these variables can be added to theconfig file along with the proper values for your environment.       Name of the variable   Default value         SSH_KEY   ${HOME}/. ssh/id_rsa       SSH_PUB_KEY   ${SSH_KEY}. pub       NUM_NODES   2       VM_EXTRADISKS   false       DOCKER_REGISTRY_IMAGE   docker. io/registry:latest       VBMC_IMAGE   quay. io/metal3-io/vbmc       SUSHY_TOOLS_IMAGE   quay. io/metal3-io/sushy-tools       IPA_DOWNLOADER_IMAGE   quay. io/metal3-io/ironic-ipa-downloader       IRONIC_IMAGE   quay. io/metal3-io/ironic       IRONIC_INSPECTOR_IMAGE   quay. io/metal3-io/ironic-inspector       BAREMETAL_OPERATOR_IMAGE   quay. io/metal3-io/baremetal-operator       CAPM3_VERSION   v1alpha3       CAPBM_IMAGE   quay. io/metal3-io/cluster-api-provider-baremetal:v1alpha1       CAPBM_IMAGE   quay. io/metal3-io/cluster-api-provider-baremetal       DEFAULT_HOSTS_MEMORY   8192       CLUSTER_NAME   test1       KUBERNETES_VERSION   v1. 17. 0       KUSTOMIZE_VERSION   v3. 2. 3   Information It is important to mention that there are several basic functionsdefined in this file that will be used by the rest of scripts. logging. sh: This script ensures that there is a log folder where all the informationgathered during the execution of the scripts is stored. If there is anyissue during the deployment, this is one of the first places to look at. [alosadag@eko1 metal3-dev-env]$ ls -1 logs/01_prepare_host-2020-02-03-122452. log01_prepare_host-2020-02-03-122956. loghost_cleanup-2020-02-03-122656. logFirst step: Prepare the host: In this first step (01_prepare_host. sh), the requirements needed tostart the preparation of the host where the virtual bare metal hostswill run are fulfilled. Depending on the host’s operating system (OS),it will trigger a specific script for CentOS/Red Hat or Ubuntu.  note: “Note”Currently CentOS Linux 7, Red Hat Enterprise Linux 8 and Ubuntuhave been tested. There is work in progress to adapt the deploymentfor CentOS Linux8. As stated previously, CentOS 7 is the operating system chosen to runin both, the host and virtual servers. Therefore, specific packages ofthe operating system are applied in the following script:    centos_install_requirements. shThis script enables epel and tripleo (current-tripleo)repositories where several packages are installed: dnf, ansible,wget, python3 and python related packages such aspython-virtualbmc from tripleo repository.  Note Notice that SELinux is set to permissive and an OS update istriggered, which will cause several packages to be upgraded sincethere are newer packages in the tripleo repositories (mostly pythonrelated) than in the rest of enabled repositories. At this point, thecontainer runtime is also installed. Note that by setting the variableCONTAINER_RUNTIME defined in common. sh is possible tochoose between docker and podman, which is the default for CentOS. Remember that this behavior can be overwritten in your config file. Once the specific requirements for the elected operating system areaccomplished, the download of several external artifacts is executed. Actually minikube, kubectl and kustomize are downloaded from theinternet. Notice that the version of Kustomize and Kubernetes is definedby KUSTOMIZE_VERSION and KUBERNETES_VERSION variables insidecommon. sh, but minikube is always downloading the latestversion available. The next step deals with cleaning ironic containers and pods thatcould be running in the host from failed deployments. This will ensurethat there will be no issues when creating ironic-pod and infra-poda little bit later in this first step.    network. sh.   At this point, the network library script is sourced. As expected,this library deals with the network configuration which includes: IPaddresses, network definitions and IPv6 support which is disabled bydefault by setting PROVISIONING_IPV6 variable:           Name of the variable    Default value    Option              PROVISIONING_NETWORK    172. 22. 0. 0/24    This is the subnet used to run the OS provisioning process          EXTERNAL_SUBNET    192. 168. 111. 0/24    This is the subnet used on the “baremetal” libvirt network, created as the primary network interface for the virtual bare metal hosts          LIBVIRT_FIRMWARE    bios               PROVISIONING_IPV6    false            Below it is depicted a network diagram of the different virtualnetworks and virtual servers involved in the Metal³ environment:    images. sh.   The images. sh library file is sourced as well in script01_prepare_host. sh. The images. sh script contains multiplevariables that set the URL (IMAGE_LOCATION), name (IMAGE_NAME) anddefault username (IMAGE_USERNAME) of the cloud image that needs tobe downloaded. The values of each variable will differ depending onthe operating system of the virtual bare metal hosts. Note that theseimages will be served from the host to the virtual servers through theprovisioning network.  In our case, since CentOS 7 is the base operating system, valueswill be defined as:           Name of the variable    Default value              IMAGE_NAME    CentOS-7-x86_64-GenericCloud-1907. qcow2          IMAGE_LOCATION    https://cloud. centos. org/centos/7/images/          IMAGE USERNAME    centos      Information In case it is expected to use a custom cloud image, just modify theprevious variables to match the right location. Now that the cloud image is defined, the download process can bestarted. First, a folder defined by IRONIC_IMAGE_DIR should exist sothat the image (CentOS-7-x86_64-GenericCloud-1907. qcow2) and itschecksum can be stored. This folder and its content will be exposedthrough a local ironic container running in the host.       Name of the variable   Default value       IRONIC_IMAGE_DIR   /opt/metal3-dev-env/ironic/html/images   Below it is verified that the cloud image files were downloadedsuccessfully in the defined folder: [alosadag@eko1 metal3-dev-env]$ ll /opt/metal3-dev-env/ironic/html/imagestotal 920324-rw-rw-r--. 1 alosadag alosadag 942407680 Feb 3 12:39 CentOS-7-x86_64-GenericCloud-1907. qcow2-rw-rw-r--. 1 alosadag alosadag    33 Feb 3 12:39 CentOS-7-x86_64-GenericCloud-1907. qcow2. md5sumOnce the shared script images. sh is sourced, the following containerimages are pre-cached locally to the host in order to speed up thingslater. Below is shown the code snippet in charge of that task: + for IMAGE_VAR in IRONIC_IMAGE IPA_DOWNLOADER_IMAGE VBMC_IMAGE SUSHY_TOOLS_IMAGE DOCKER_REGISTRY_IMAGE+ IMAGE=quay. io/metal3-io/ironic+ sudo podman pull quay. io/metal3-io/ironic. . . . . . . The container image location of each one is defined by their respective variables:       Name of the variable   Default value         VBMC_IMAGE   quay. io/metal3-io/vbmc       SUSHY_TOOLS_IMAGE   quay. io/metal3-io/sushy-tools       IPA_DOWNLOADER_IMAGE   quay. io/metal3-io/ironic-ipa-downloader       IRONIC_IMAGE   quay. io/metal3-io/ironic       DOCKER_REGISTRY_IMAGE   docker. io/registry:latest   Information In case it is expected to modify the public container images to testnew features, it is worth mentioning that there is a containerregistry running as a privileged container in the host. Therefore itis recommended to upload your modified images there and just overwritethe previous variables to match the right location. At this point, an Ansible role is run locally in order to complete thelocal configuration. ansible-playbook \ -e  working_dir=$WORKING_DIR  \ -e  virthost=$HOSTNAME  \ -i vm-setup/inventory. ini \ -b -vvv vm-setup/install-package-playbook. ymlThis playbook imports two roles. One is called packages_installation,which is in charge of installing a few more packages. The list ofpackages installed are listed as default Ansible variables in thevm-setup role inside the metal3-dev-envrepository. The other role is based on thefubarhouse. golangAnsible Galaxy role. It is in charge of installing and configuring theexact golang version 1. 12. 12 defined in an Ansible variable in theinstall-package-playbook. ymlplaybook Once the playbook is finished, a pod called ironic-pod is created. Inside that pod, a privileged ironic-ipa-downloader container isstarted and attached to the host network. This container is in charge ofdownloading the Ironic PythonAgent (IPA)files to a shared volume defined by IRONIC_IMAGE_DIR. This folder isexposed by the ironic container through HTTP. Information The Ironic PythonAgent is anagent for controlling and deploying Ironic controlled baremetal nodes. Typically run in a ramdisk, the agent exposes a REST API forprovisioning servers. See below the code snippet that fulfils the task: sudo podman run -d --net host --privileged --name ipa-downloader \ --pod ironic-pod -e IPA_BASEURI= -v /opt/metal3-dev-env/ironic:/shared \ quay. io/metal3-io/ironic-ipa-downloader /usr/local/bin/get-resource. shBelow is shown the status of the pods and containers at this point: [root@eko1 metal3-dev-env]# podman pod list --ctr-namesPOD ID     NAME     STATUS  CREATED   CONTAINER INFO                       INFRA ID5a0d475351aa  ironic-pod  Running  6 days ago  [5a0d475351aa-infra] [ipa-downloader]           18f3a8f61407The process will wait until the ironic-python-agent (IPA) initramfs,kernel and headers files are downloaded successfully. See below thefiles downloaded along with the CentOS 7 cloud image: [alosadag@eko1 metal3-dev-env]$ ll /opt/metal3-dev-env/ironic/html/imagestotal 920324-rw-rw-r--. 1 alosadag alosadag 942407680 Feb 3 12:39 CentOS-7-x86_64-GenericCloud-1907. qcow2-rw-rw-r--. 1 alosadag alosadag    33 Feb 3 12:39 CentOS-7-x86_64-GenericCloud-1907. qcow2. md5sumdrwxr-xr-x. 2 root   root      147 Feb 3 12:41 ironic-python-agent-1862d000-59d9fdc6304b1lrwxrwxrwx. 1 root   root      72 Feb 3 12:41 ironic-python-agent. initramfs -&gt; ironic-python-agent-1862d000-59d9fdc6304b1/ironic-python-agent. initramfslrwxrwxrwx. 1 root   root      69 Feb 3 12:41 ironic-python-agent. kernel -&gt; ironic-python-agent-1862d000-59d9fdc6304b1/ironic-python-agent. kernellrwxrwxrwx. 1 root   root      74 Feb 3 12:41 ironic-python-agent. tar. headers -&gt; ironic-python-agent-1862d000-59d9fdc6304b1/ironic-python-agent. tar. headersAfterwards, the script makes sure that libvirt is running successfullyon the host and that the non-privileged user has permission to interactwith it. Libvirt daemon should be running so that minikube can beinstalled successfully. See the following script snippet starting theminikube VM: + sudo su -l -c 'minikube start --insecure-registry 192. 168. 111. 1:5000'* minikube v1. 6. 2 on Centos 7. 7. 1908* Selecting 'kvm2' driver from user configuration (alternates: [none])In the same way, as with the host, container images are pre-cached butin this case inside minikube local image repository. Notice that in thiscase the Bare Metaloperator (BMO) isalso downloaded since it will run on minikube. The container location isdefined by BAREMETAL_OPERATOR_IMAGE. In case you want to test newfeatures or new fixes to the BMO, just change the value of the variableto match the location of the modified image:       Name of the variable   Default value       BAREMETAL_OPERATOR_IMAGE   quay. io/metal3-io/baremetal-operator   Note Remember that minikube is the management cluster in our environment. So it must run all the operators and controllers needed for Metal³. Below is shown the output of the script once all the container imageshave been pulled to minikube: + sudo su -l -c 'minikube ssh sudo docker image ls' alosadagREPOSITORY                TAG         IMAGE ID      CREATED       SIZEquay. io/metal3-io/ironic         latest       e5d81adf05ee    26 hours ago    693MBquay. io/metal3-io/ironic-ipa-downloader  latest       d55b0dac2144    6 days ago     239MBquay. io/metal3-io/ironic-inspector    latest       8bb5b844ada6    6 days ago     408MBquay. io/metal3-io/baremetal-operator   latest       3c692a32ddd6    9 days ago     1. 77GBk8s. gcr. io/kube-proxy           v1. 17. 0       7d54289267dc    7 weeks ago     116MBk8s. gcr. io/kube-controller-manager    v1. 17. 0       5eb3b7486872    7 weeks ago     161MBk8s. gcr. io/kube-scheduler         v1. 17. 0       78c190f736b1    7 weeks ago     94. 4MBk8s. gcr. io/kube-apiserver         v1. 17. 0       0cae8d5cc64c    7 weeks ago     171MBkubernetesui/dashboard          v2. 0. 0-beta8    eb51a3597525    7 weeks ago     90. 8MBk8s. gcr. io/coredns            1. 6. 5        70f311871ae1    2 months ago    41. 6MBk8s. gcr. io/etcd              3. 4. 3-0       303ce5db0e90    3 months ago    288MBkubernetesui/metrics-scraper       v1. 0. 2       3b08661dc379    3 months ago    40. 1MBk8s. gcr. io/kube-addon-manager       v9. 0. 2       bd12a212f9dc    6 months ago    83. 1MBk8s. gcr. io/pause             3. 1         da86e6ba6ca1    2 years ago     742kBgcr. io/k8s-minikube/storage-provisioner  v1. 8. 1       4689081edb10    2 years ago     80. 8MBOnce the container images are stored, minikube can be stopped. At thatmoment, the virtual networks shown in the previous picture are attachedto the minikube VM as can be verified by the following command: [alosadag@smc-master metal3-dev-env]$ sudo virsh domiflist minikubeInterface Type    Source   Model    MAC--------------------------------------------------------     network  default  virtio   d4:38:25:25:c6:ca-     network  minikube-net virtio  a4:c2:8a:9d:2a:d8-     network  provisioning virtio  52:54:00:c8:50:97-     network  baremetal virtio   52:54:00:17:b4:ecInformation At this point the host is ready to create the virtual infrastructure. The video below exhibits all the configurations explained and executedduring this first step. Step 2: Configure the host: In this step, the script 02_configure_host. sh basically configures thelibvirt/KVM virtual infrastructure and starts services in the host thatwill be consumed by the virtual bare metal machines:  Web server to expose the ironic-python-agent (IPA) initramfs,kernel, headers and operating system cloud images.  Virtual BMC to emulate a real baseboard management controller (BMC).  Container registry where the virtual servers will pull the imagesneeded to run a K8s installation. Information A baseboard management controller (BMC) is a specialized serviceprocessor that monitors the physical state of a computer, networkserver or other hardware device using sensors and communicating withthe system administrator through an independent connection. The BMC ispart of the Intelligent Platform Management Interface (IPMI) and isusually contained in the motherboard or main circuit board of thedevice to be monitored. First, an ssh-key in charge of communicating to libvirt is created if itdoes not exist previously. This key is called id_rsa_virt_power. It isadded to the root authorized_keys and is used by vbmc and sushy toolsto contact libvirt. Information sushy-tools is a set of simple simulation tools aiming at supportingthe development and testing of the Redfish protocol implementations. Next, another Ansible playbook calledsetup-playbook. ymlis run against the host. It is focused on setting up the virtualinfrastructure around metal3-dev-env. Below it is shown the Ansiblevariables that are passed to the playbook, which actually are obtainingthe values from the global variables defined in thecommon. sh or the configuration file. ANSIBLE_FORCE_COLOR=true ansible-playbook \  -e  working_dir=$WORKING_DIR  \  -e  num_nodes=$NUM_NODES  \  -e  extradisks=$VM_EXTRADISKS  \  -e  virthost=$HOSTNAME  \  -e  platform=$NODES_PLATFORM  \  -e  libvirt_firmware=$LIBVIRT_FIRMWARE  \  -e  default_memory=$DEFAULT_HOSTS_MEMORY  \  -e  manage_baremetal=$MANAGE_BR_BRIDGE  \  -e  provisioning_url_host=$PROVISIONING_URL_HOST  \  -i vm-setup/inventory. ini \  -b -vvv vm-setup/setup-playbook. yml      Name of the variable   Default value         WORKING_DIR   /opt/metal3-dev-env       NUM_NODES   2       VM_EXTRADISKS   false       HOSTNAME   hostname       NODES_PLATFORM   libvirt       LIBVIRT_FIRMWARE   bios       DEFAULT_HOSTS_MEMORY   8192       MANAGE_BR_BRIDGE   y       PROVISIONING_URL_HOST   172. 22. 0. 1   Information There are variables that are only defined as Ansible variables, e. g. number of CPUs of the virtual bare metal server, size of disks, etc. In case you would like to change properties not defined globally inthe metal3-dev-env take a look at the default variables specified inrole:commonandlibvirt The setup-playbook. yml is composed by 3 roles, which are detailed below:    Common.   This role sets up the virtual hardware and network configuration ofthe VMs. Actually it is adependencyof the libvirt and virtbmc Ansible roles. This means that thecommon role must always be executed before the roles that depend onthem. Also, they are only executed once. If two roles state the sameone as their dependency, it is only executed the first time.    Libvirt.   It actually is the role that configures the virtual bare metalservers. They are all identically defined with the same hardware andnetwork configuration. Note that they are not started since they willbe booted later by ironic during the provisioning process. Note It is possible to change the number of VMs to provision by replacingthe value of NUMBER_NODES Finally, once the VMs are defined and we have their MAC address, theironic inventory file ironic_nodes_json is created. The action ofcreating a node is part of the enrollment process and the first stepto prepare a node to reach the available status. {  nodes : [  {    name :  node-0 ,    driver :  ipmi ,    resource_class :  baremetal ,    driver_info : {     username :  admin ,     password :  password ,     port :  6230 ,     address :  ipmi://192. 168. 111. 1:6230 ,     deploy_kernel :  http://172. 22. 0. 1/images/ironic-python-agent. kernel ,     deploy_ramdisk :  http://172. 22. 0. 1/images/ironic-python-agent. initramfs    },    ports : [    {      address :  00:00:e0:4b:24:8b ,      pxe_enabled : true    }   ],    properties : {     local_gb :  50 ,     cpu_arch :  x86_64    }  },  {    name :  node-1 ,    driver :  ipmi ,    resource_class :  baremetal ,    driver_info : {     username :  admin ,     password :  password ,     port :  6231 ,     address :  ipmi://192. 168. 111. 1:6231 ,     deploy_kernel :  http://172. 22. 0. 1/images/ironic-python-agent. kernel ,     deploy_ramdisk :  http://172. 22. 0. 1/images/ironic-python-agent. initramfs    },    ports : [    {      address :  00:00:e0:4b:24:8f ,      pxe_enabled : true    }   ],    properties : {     local_gb :  50 ,     cpu_arch :  x86_64    }  },Information This role is also used to tear down the virtual infrastructuredepending on the variablelibvirt_actioninside the Ansible role: setup or teardown.  VirtBMCThis role is only executed if the bare metal virtual machines arecreated in libvirt, because vbmc needs libvirt to emulate a realBMC. info “Information”VirtualBMC (vmbc) tool simulates a Baseboard Management Controller(BMC) by exposing IPMI responder to the network and talking to libvirtat the host vBMC is running at. Basically, manipulate virtual machineswhich pretend to be bare metal servers. The virtbmc Ansible role creates the vbmc and sushy-toolsconfiguration in the host for each virtual bare metal nodes. Note thateach virtual bare metal host will have a different vbmc socketexposed in the host. The communication to each vbmc is needed by theBMO to start, stop, configure the boot order, etc during theprovisioning stage. Finally, this folders containing the configurationwill be mounted by the vbmc and sushy-tools containers. [alosadag@eko1 metal3-dev-env]$ sudo ls -l --color /opt/metal3-dev-env/virtualbmctotal 0drwxr-x---. 2 root root 21 Feb 5 11:07 sushy-toolsdrwxr-x---. 4 root root 70 Feb 5 11:08 vbmcNext, both host provisioning and baremetal interfaces are configured. The provisioning interface, as the name suggests, will be used toprovision the virtual bare metal hosts by means of the Bare MetalOperator. This interface is configured with an static IP (172. 22. 0. 1): [alosadag@smc-master metal3-dev-env]$ ifconfig provisioningprovisioning: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500  inet 172. 22. 0. 1 netmask 255. 255. 255. 0 broadcast 172. 22. 0. 255  inet6 fe80::1091:c1ff:fea1:6a0f prefixlen 64 scopeid 0x20&lt;link&gt;  ether 12:91:c1:a1:6a:0f txqueuelen 1000 (Ethernet)On the other hand, the baremetal virtual interface behaves as anexternal network. This interface is able to reach the internet and it isthe network where the different Kubernetes nodes will exchangeinformation. This interface is configured as auto, so the IP isretrieved by DHCP. [alosadag@smc-master metal3-dev-env]$ ifconfig baremetalbaremetal: flags=4099&lt;UP,BROADCAST,MULTICAST&gt; mtu 1500  inet 192. 168. 111. 1 netmask 255. 255. 255. 0 broadcast 192. 168. 111. 255  ether 52:54:00:db:85:29 txqueuelen 1000 (Ethernet)Next, an Ansible role calledfirewallwill be executed targeting the host to be sure that the proper portsare opened. In case your host is running Red Hat Enterprise Linux orCentOS 8, firewall module will be used. In any other case, iptablesmodule is the choice. Below is the code snippet where firewalld or iptables is assigned: # Use firewalld on CentOS/RHEL, iptables everywhere elseexport USE_FIREWALLD=Falseif [[ ($OS ==  rhel  || $OS =  centos ) &amp;&amp; ${OS_VERSION} == 8 ]]then export USE_FIREWALLD=TruefiNote This behavior can be changed by replacing the value of theUSE_FIREWALLD variable The ports managed by this role are all associated with the services thattake part in the provisioning process: ironic, vbmc, httpd, pxe,container registry. . Note Services like ironic, pxe, keepalived, httpd and the containerregistry are running in the host as containers attached to the hostnetwork on the host’s provisioning interface. On the other hand, thevbmc service is also running as a privileged container and it islistening in the host’s baremetal interface. Once the network is configured, a local container registry is started. It will be needed in the case of using locally built images. In thatcase, the container images can be modified locally and pushed to thelocal registry. At that point, the specific image location variable mustbe changed so it must point out the local registry. This process makesit easy to verify and test changes to the code locally. At this point, the following containers are running inside two pods onthe host: infra-pod and ironic-pod. [root@eko1 metal3-dev-env]# podman pod list --ctr-namesPOD ID     NAME     STATUS  CREATED   CONTAINER INFO                       INFRA ID67cc53713145  infra-pod  Running  6 days ago  [vbmc] [sushy-tools] [httpd-infra] [67cc53713145-infra]  f1da23fcd77f5a0d475351aa  ironic-pod  Running  6 days ago  [5a0d475351aa-infra] [ipa-downloader]           18f3a8f61407Below are detailed the containers inside the infra-pod pod which arerunning as privileged using the host network:    The httpd container. &gt; &gt;A folder called shared where the cloud OS image and IPA files areavailable is mounted and exposed to the virtual bare metal hosts.   sudo podman run -d --net host --privileged --name httpd-infra \ --pod infra-pod -v /opt/metal3-dev-env/ironic:/shared --entrypoint \  /bin/runhttpd quay. io/metal3-io/ironic  This folder also contains the inspector. ipxe file which contains theinformation needed to be able to run the ironic-python-agent kerneland initramfs. Below, httpd-infra container is accessed and it hasbeen verified that host’s /opt/metal3-dev-env/ironic/(IRONIC_DATA_DIR) is mounted inside the shared folder of thecontainer: [alosadag@eko1 metal3-dev-env]$ sudo podman exec -it httpd-infra bash[root@infra-pod shared]# cat html/inspector. ipxe#!ipxe:retry_bootecho In inspector. ipxeimgfree# NOTE(dtantsur): keep inspection kernel params in [mdns]params in ironic-inspector-imagekernel --timeout 60000 http://172. 22. 0. 1:80/images/ironic-python-agent. kernel ipa-inspection-callback-url=http://172. 22. 0. 1:5050/v1/continue ipa-inspection-collectors=default,extra-hardware,logs systemd. journald. forward_to_console=yes BOOTIF=${mac} ipa-debug=1 ipa-inspection-dhcp-all-interfaces=1 ipa-collect-lldp=1 initrd=ironic-python-agent. initramfs || goto retry_bootinitrd --timeout 60000 http://172. 22. 0. 1:80/images/ironic-python-agent. initramfs || goto retry_bootboot   The vbmc container.   This container mounts two host folders: one is/opt/metal3-dev-env/virtualbmc/vbmc where vbmc configuration foreach node is stored, the other folder is /root/. ssh where root keysare located, specifically id_rsa_virt_power which is used to managethe communication with libvirt.  + sudo podman run -d --net host --privileged --name vbmc --pod infra-pod -v /opt/metal3-dev-env/virtualbmc/vbmc:/root/&gt; . vbmc -v /root/. ssh:/root/ssh quay. io/metal3-io/vbmc    The sushy-tools container.   This container mounts the /opt/metal3-dev-env/virtualbmc/sushy-toolsconfig folder and the /root/. ssh local folder as well. Thefunctionality is similar as the vbmc, however this use redfishinstead of ipmi to connect to the BMC.  + sudo podman run -d --net host --privileged --name sushy-tools --pod infra-pod -v /opt/metal3-dev-env/virtualbmc/&gt; sushy-tools:/root/sushy -v /root/. ssh:/root/ssh quay. io/metal3-io/sushy-tools Information At this point the virtual infrastructure must be ready to apply theKubernetes specific configuration. Note that all the VMs specified byNUMBER_NODES and minikube must be shut down and the defined virtualnetwork must be active: [alosadag@smc-master metal3-dev-env]$ sudo virsh list --all Id  Name              State---------------------------------------------------- -   minikube            shut off -   node_0             shut off -   node_1             shut off -   node_2             shut off[alosadag@smc-master metal3-dev-env]$ sudo virsh net-list --all Name         State   Autostart   Persistent---------------------------------------------------------- baremetal      active   yes      yes default       active   yes      yes minikube-net     active   yes      yes provisioning     active   yes      yesIn the video below it is exhibited all the configuration explained andexecuted during this second step. Step 3: Launch the management cluster (minikube): The third script called 03_launch_mgmt_cluster. sh basically configuresminikube to become a Metal³ management cluster. On top of minikube thebaremetal-operator, capi-controller-manager,capbm-controller-manager and cabpk-controller-manager are installedin the metal3 namespace. In a more detailed way, the script clones the Bare Metal Operator(BMO) and ClusterAPI Provider for Managed Bare Metal Hardware operator(CAPBM)git repositories, creates the cloud. yaml file and starts the minikubevirtual machine. Once minikube is up and running, the BMO is built andexecuted in minikube’s Kubernetes cluster. In the case of the Bare Metal Operator, the branch by default to cloneis master, however, this and other variables shown in the followingtable can be replaced in the config file: + BMOREPO=https://github. com/metal3-io/baremetal-operator+ BMOBRANCH=master      Name of the variable   Default value   Options         BMOREPO   https://github. com/metal3-io/baremetal-operator           BMOBRANCH   master           CAPBMREPO   https://github. com/metal3-io/cluster-api-provider-baremetal           CAPM3_VERSION   v1alpha3   v1alpha4 or v1alpha3       FORCE_REPO_UPDATE   false           BMO_RUN_LOCAL   false           CAPBM_RUN_LOCAL   false       Once the BMO variables are configured, it is time for the operator tobe deployed using kustomize and kubectl as it can seen from thelogs:  Information: Kustomize is a Kubernetes tool that lets you customize raw, template-free YAML files for multiple purposes, leaving the original YAML untouched and usable as is. + kustomize build bmo-dirPrHIrcl+ kubectl apply -f-namespace/metal3 createdcustomresourcedefinition. apiextensions. k8s. io/baremetalhosts. metal3. io createdserviceaccount/metal3-baremetal-operator createdclusterrole. rbac. authorization. k8s. io/metal3-baremetal-operator createdclusterrolebinding. rbac. authorization. k8s. io/metal3-baremetal-operator createdconfigmap/ironic-bmo-configmap-75tkt49k5c createdsecret/mariadb-password-d88m524c46 createddeployment. apps/metal3-baremetal-operator createdOnce the BMO objects are applied, it’s time to transform the virtualbare metal hosts information into a yaml file of kind BareMetalHostCustom Resource (CR). This is done by a golang script passing them theIPMI address, BMC username and password, which are stored as aKubernetes secret, MAC address and name: + go run /home/alosadag/go/src/github. com/metal3-io/baremetal-operator/cmd/make-bm-worker/main. go -address ipmi://192. 168. 111. 1:6230 -password password -user admin -boot-mac 00:be:bc:fd:17:f3 node-0+ read -r name address user password mac+ go run /home/alosadag/go/src/github. com/metal3-io/baremetal-operator/cmd/make-bm-worker/main. go -address ipmi://192. 168. 111. 1:6231 -password password -user admin -boot-mac 00:be:bc:fd:17:f7 node-1+ read -r name address user password mac+ go run /home/alosadag/go/src/github. com/metal3-io/baremetal-operator/cmd/make-bm-worker/main. go -address ipmi://192. 168. 111. 1:6232 -password password -user admin -boot-mac 00:be:bc:fd:17:fb node-2+ read -r name address user password macBelow is shown the bare metal host definition of node-1. Note that theIPMI address is the IP of the host’s provisioning interface. Behind thescenes, IPMI is handled by the vbmc container running in the host. ---apiVersion: v1kind: Secretmetadata: name: node-1-bmc-secrettype: Opaquedata: username: YWRtaW4= password: cGFzc3dvcmQ=---apiVersion: metal3. io/v1alpha1kind: BareMetalHostmetadata: name: node-1spec: online: true bootMACAddress: 00:00:e0:4b:24:8f bmc:  address: ipmi://192. 168. 111. 1:6231  credentialsName: node-1-bmc-secretSee that the MAC address configured in the BareMetalHost specdefinition matches node-1 provisioning interface: [root@eko1 metal3-dev-env]# virsh domiflist node_1Interface Type    Source   Model    MAC-------------------------------------------------------vnet4   bridge   provisioning virtio   00:00:e0:4b:24:8fvnet5   bridge   baremetal virtio   00:00:e0:4b:24:91Finally, the script apply in namespace metal3 each of theBareMetalHost yaml files that match each virtual bare metal host: + kubectl apply -f bmhosts_crs. yaml -n metal3secret/node-0-bmc-secret createdbaremetalhost. metal3. io/node-0 createdsecret/node-1-bmc-secret createdbaremetalhost. metal3. io/node-1 createdsecret/node-2-bmc-secret createdbaremetalhost. metal3. io/node-2 createdLastly, it is the turn of the CAPBM. Similar to BMO, kustomize isused to create the different Kubernetes components and kubectl appliedthe files into the management cluster. Warning Note that installing CAPBM includes installing the components of theCluster API and thecomponents of the Cluster API bootstrap providerkubeadm(CABPK) Below the objects are created through the generate. sh script: ++ mktemp -d capbm-XXXXXXXXXX+ kustomize_overlay_path=capbm-eJPOjCPASD+ . /examples/generate. sh -fGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/_out/cluster. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/_out/controlplane. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/_out/metal3crds. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/_out/metal3plane. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/_out/machinedeployment. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/provider-components/provider-components-cluster-api. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/provider-components/provider-components-kubeadm. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/provider-components/provider-components-baremetal. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/_out/provider-components. yamlThen, kustomize configures the files accordingly to the values definedand kubectl applies them: + kustomize build capbm-eJPOjCPASD+ kubectl apply -f-namespace/cabpk-system creatednamespace/capbm-system creatednamespace/capi-system createdcustomresourcedefinition. apiextensions. k8s. io/baremetalclusters. infrastructure. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/baremetalmachines. infrastructure. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/baremetalmachinetemplates. infrastructure. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/clusters. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/kubeadmconfigs. bootstrap. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/kubeadmconfigtemplates. bootstrap. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/machinedeployments. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/machines. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/machinesets. cluster. x-k8s. io createdrole. rbac. authorization. k8s. io/cabpk-leader-election-role createdrole. rbac. authorization. k8s. io/capbm-leader-election-role createdrole. rbac. authorization. k8s. io/capi-leader-election-role createdclusterrole. rbac. authorization. k8s. io/cabpk-manager-role createdclusterrole. rbac. authorization. k8s. io/cabpk-proxy-role createdclusterrole. rbac. authorization. k8s. io/capbm-manager-role createdclusterrole. rbac. authorization. k8s. io/capbm-proxy-role createdclusterrole. rbac. authorization. k8s. io/capi-manager-role createdrolebinding. rbac. authorization. k8s. io/cabpk-leader-election-rolebinding createdrolebinding. rbac. authorization. k8s. io/capbm-leader-election-rolebinding createdrolebinding. rbac. authorization. k8s. io/capi-leader-election-rolebinding createdclusterrolebinding. rbac. authorization. k8s. io/cabpk-manager-rolebinding createdclusterrolebinding. rbac. authorization. k8s. io/cabpk-proxy-rolebinding createdclusterrolebinding. rbac. authorization. k8s. io/capbm-manager-rolebinding createdclusterrolebinding. rbac. authorization. k8s. io/capbm-proxy-rolebinding createdclusterrolebinding. rbac. authorization. k8s. io/capi-manager-rolebinding createdsecret/capbm-webhook-server-secret createdservice/cabpk-controller-manager-metrics-service createdservice/capbm-controller-manager-service createdservice/capbm-controller-metrics-service createddeployment. apps/cabpk-controller-manager createddeployment. apps/capbm-controller-manager createddeployment. apps/capi-controller-manager createdInformation At this point all controllers and operators must be running in thenamespace metal3 of the management cluster (minikube). All virtualbare metal hosts configured must be shown as BareMetalHostsresources in the metal3 namespace as well. They should be in readystatus and stopped (online is false) In the video below it is exhibited all the configuration explained and executed during this third step. Step 4: Verification: The last script 04_verify. sh is in charge of verifying that thedeployment has been successful by checking several things:  Custom resources (CR) and custom resource definition (CRD) wereapplied and exist in the cluster.  Verify that the virtual bare metal hosts matches the informationdetailed in theBareMetalHost object.  All containers are in running status.  Verify virtual network configuration and status.  Verify operators and controllers are running. However, this verification can be easily achieved manually. Forinstance, checking that controllers and operators running in themanagement cluster (minikube) and all the virtual bare metal hosts arein ready status: [alosadag@eko1 ~]$ kubectl get pods -n metal3 -o wideNAME                     READY  STATUS  RESTARTS  AGE   IP        NODE    NOMINATED NODE  READINESS GATEScabpk-controller-manager-5c67dd56c4-wfwbh  2/2   Running  9     6d23h  172. 17. 0. 5    minikube  &lt;none&gt;      &lt;none&gt;capbm-controller-manager-7f9b8f96b7-grl4r  2/2   Running  12     6d23h  172. 17. 0. 4    minikube  &lt;none&gt;      &lt;none&gt;capi-controller-manager-798c76675f-dxh2n   1/1   Running  10     6d23h  172. 17. 0. 6    minikube  &lt;none&gt;      &lt;none&gt;metal3-baremetal-operator-5b4c59755d-h4zkp  6/6   Running  8     6d23h  192. 168. 39. 101  minikube  &lt;none&gt;      &lt;none&gt;Verify that the BareMetalHosts provisioning status is ready and theBMC configuration is correct. Check that all virtual bare metal hostsare shut down (online is false): [alosadag@eko1 ~]$ kubectl get baremetalhosts -n metal3NAME   STATUS  PROVISIONING STATUS  CONSUMER       BMC             HARDWARE PROFILE  ONLINE  ERRORnode-0  OK    ready                   ipmi://192. 168. 111. 1:6230  unknown      falsenode-1  OK    ready                   ipmi://192. 168. 111. 1:6231  unknown      falsenode-2  OK    ready                   ipmi://192. 168. 111. 1:6232  unknown      falseGet the list of CRDs created in the cluster. Check that, at least, thefollowing ones exist: [alosadag@eko1 ~]$ kubectl get crdsNAME                            CREATED ATbaremetalclusters. infrastructure. cluster. x-k8s. io      2020-01-22T13:19:42Zbaremetalhosts. metal3. io                  2020-01-22T13:19:35Zbaremetalmachines. infrastructure. cluster. x-k8s. io      2020-01-22T13:19:42Zbaremetalmachinetemplates. infrastructure. cluster. x-k8s. io  2020-01-22T13:19:42Zclusters. cluster. x-k8s. io                  2020-01-22T13:19:42Zkubeadmconfigs. bootstrap. cluster. x-k8s. io          2020-01-22T13:19:42Zkubeadmconfigtemplates. bootstrap. cluster. x-k8s. io      2020-01-22T13:19:42Zmachinedeployments. cluster. x-k8s. io             2020-01-22T13:19:43Zmachines. cluster. x-k8s. io                  2020-01-22T13:19:43Zmachinesets. cluster. x-k8s. io                2020-01-22T13:19:43ZInformation KUBECONFIG file is stored in the user’s home directory(~/. kube/config) that executed the scripts. Check the status of all the applications running in minikube or bettersaid, in the management cluster. [alosadag@smc-master logs]$ kubectl get pods -ANAMESPACE   NAME                    READY  STATUS  RESTARTS  AGEkube-system  coredns-6955765f44-fkdzp          1/1   Running  1     164mkube-system  coredns-6955765f44-fxzvz          1/1   Running  1     164mkube-system  etcd-minikube                1/1   Running  1     164mkube-system  kube-addon-manager-minikube         1/1   Running  1     164mkube-system  kube-apiserver-minikube           1/1   Running  1     164mkube-system  kube-controller-manager-minikube      1/1   Running  1     164mkube-system  kube-proxy-87g98              1/1   Running  1     164mkube-system  kube-scheduler-minikube           1/1   Running  1     164mkube-system  storage-provisioner             1/1   Running  2     164mmetal3    cabpk-controller-manager-5c67dd56c4-rldk4  2/2   Running  0     156mmetal3    capbm-controller-manager-7f9b8f96b7-mdfcw  2/2   Running  0     156mmetal3    capi-controller-manager-84947c7497-k6twl  1/1   Running  0     156mmetal3    metal3-baremetal-operator-78bffc8d-z5hqs  6/6   Running  0     156mIn the video below it is exhibited all the configuration explained andexecuted during the verification steps. Summary: In this post a deep dive into the metal3-dev-env scripts was shown. Ithas been deeply detailed the process of creating a Metal³ emulatedenvironment from a set of virtual machines (VMs) to manage as if theywere bare metal hosts. After this post, the reader should have acquired a basic understandingof all the pieces involved in the Metal³ project. Also, and moreimportant, how these scripts can be adapted to your specific needs. Remember that this can be achieved in multiple ways: replacing values inthe global variables, replacing Ansible default variables or evenmodifying playbooks or the scripts themselves. Notice that the Metal³ development environment also focuses ondeveloping new features of the BMO or CAPBM and being able to test themlocally. References:  Video playlist: A detailed walkthrough the installation of the metal3-dev-env on Youtube Getting started with Metal3. io Metal³ code repositories"
    }, {
    "id": 17,
    "url": "/blog/2020/01/20/metal3_deploy_kubernetes_on_bare_metal.html",
    "title": "Metal³: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019",
    "author" : "Pedro Ibáñez Requena",
    "tags" : "hybrid, cloud, metal3, baremetal, shiftdev, edge",
    "body": "Conference talk: Metal³: Deploy Kubernetes on Bare Metal - Yolanda Robla, Red Hat: Some of the most influential minds in the developer industry were landing in the gorgeous ancient city of Split, Croatia, to talk at the Shift Dev 2019 - Developer Conference about the most cutting-edge technologies, techniques and biggest trends in the developer space. In this video, Yolanda Robla speaks about the deployment of Kubernetes on Bare Metal with the help of Metal³, a new tool that enables the management of bare metal hosts via custom resources managed through the Kubernetes API. Speakers: Yolanda Robla Yolanda Robla is a Principal Software Engineer at Red Hat. In her own words:  In my current position in Red Hat as an NFV Partner Engineer, I investigate new technologies and create proofs of concept for partners to embrace new technologies. Being the current PTL of Akraino, I am involved in designing and implementing systems based on Kubernetes for the Edge use cases, ensuring high scalability and reproducibility using a GitOps approach. References:  Video: Metal³: Deploy Kubernetes on Bare Metal video"
    }, {
    "id": 18,
    "url": "/blog/2019/12/04/Introducing_metal3_kubernetes_native_bare_metal_host_management.html",
    "title": "Introducing Metal³: Kubernetes Native Bare Metal Host Management - Russell Bryant & Doug Hellmann, Red Hat - KubeCon NA, November 2019",
    "author" : "Pedro Ibáñez Requena",
    "tags" : "hybrid, cloud, metal3, baremetal, kubecon, edge",
    "body": "Conference talk: Introducing Metal³: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat: Metal³ (metal cubed/Kube) is a new open-source bare metal host provisioning tool created to enable Kubernetes-native infrastructure management. Metal³ enables the management of bare metal hosts via custom resources managed through the Kubernetes API as well as the monitoring of bare metal host metrics to Prometheus. This presentation will explain the motivations behind creating the project and what has been accomplished so far. This will be followed by an architectural overview and description of the Custom Resource Definitions (CRDs) for describing bare metal hosts, leading to a demonstration of using Metal³ in a Kubernetes cluster. In this video, Russell Bryant and Doug Hellmann speak about the what’s and how’s of Metal³, a new tool that enables the management of bare metal hosts via custom resources managed through the Kubernetes API. Speakers: Russell Bryant Russell Bryant is a Distinguished Engineer at Red Hat, where he works on infrastructure management to support Kubernetes clusters. Prior to working on the Metal³ project, Russell worked on other open infrastructure projects. Russell worked in Software Defined Networking with Open vSwitch (OVS) and Open Virtual Network (OVN) and worked on various parts of OpenStack. Russell also worked in open source telephony via the Asterisk project. Doug Hellmann Doug Hellmann is a Senior Principal Software Engineer at Red Hat. He has been a professional developer since the mid-1990s and has worked on a variety of projects in fields such as mapping, medical news publishing, banking, data centre automation, and hardware provisioning. He has been contributing to open-source projects for most of his career and for the past 7 years he has been focusing on open-source cloud computing technologies, including OpenStack and Kubernetes. References:  Presentation: Introducing Metal³ KubeCon NA 2019 PDF Video: Introducing Metal³: Kubernetes Native Bare Metal Host Management videoDemos:  First demo (Inspection)  Second demo (Provisioning)  Third demo (Scale up)  Fourth demo (v1alpha2) "
    }, {
    "id": 19,
    "url": "/blog/2019/11/13/Extend_Your_Data_Center_to_the_Hybrid_Edge-Red_Hat_Summit.html",
    "title": "Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019",
    "author" : "Pedro Ibáñez Requena",
    "tags" : "hybrid, cloud, metal3, baremetal, summit, edge",
    "body": "Conference talk: Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019, Paul Cormier, Burr Stutter and Garima Sharma: A critical part of being successful in the hybrid cloud is being successful in your data centre with your own infrastructure. In this video, Paul Cormier, Burr Sutter and Garima Sharma show how you can bring the Open Hybrid cloud to the edge. Cluster management from multiple cloud providers to on-premise. In the demo you’ll see a multi-cluster inventory for the open hybrid cloud at cloud. redhat. com, OpenShift Container Storage providing storage for Virtual Machines and containers (Cloud, Virtualization and bare metal), and everything Kubernetes native. Speakers: Paul Cormier Executive vice president and president, Products and Technologies. Leads Red Hat’s technology and products organizations, including engineering, product management, and product marketing for Red Hat’s technologies. He joined Red Hat in May 2001 as executive vice president, Engineering. Burr Sutter A lifelong developer advocate, community organizer, and technology evangelist, Burr Sutter is a featured speaker at technology events around the globe —from Bangalore to Brussels and Berlin to Beijing (and most parts in between)— he is currently Director of Developer Experience at Red Hat. A Java Champion since 2005 and former president of the Atlanta Java User Group, Burr founded the DevNexus conference —now the second largest Java event in the U. S. — with the aim of making access to the world’s leading developers affordable to the developer community. Garima Sharma Senior Engineering leader at the world’s largest Open Source company. As a seasoned Tech professional, she runs a global team of Solutions Engineers focused on a large portfolio of Cloud Computing products and technology. She has helped shape science and technology for mission-critical software, reliability in operations and re-design of architecture all geared towards advancements in medicine, security, cloud technologies and bottom-line savings for the client businesses. Whether leading the architecture, development and delivery of customer-centric cutting-edge systems or spearheading diversity and inclusion initiatives via keynotes, blogs and conference presentations, Garima champions the idea of STEM. Garima ardently believes in Maya Angelou’s message that diversity makes for a rich tapestry, and we must understand that all the threads of the tapestry are equal in value no matter what their color. Video:  Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019"
    }, {
    "id": 20,
    "url": "/blog/2019/11/07/Kubernetes-native_Infrastructure-Managed_Baremetal_with_Kubernetes_Operators_and_OpenStack_Ironic.html",
    "title": "Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat",
    "author" : "Pedro Ibáñez Requena",
    "tags" : "kubernetes, metal3, operator, baremetal, openstack, ironic",
    "body": "Conference talk: Open Infrastructure Days UK 2019; Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat: In this session, you can hear about a new effort to enable baremetal Kubernetes deployments using native interfaces, and in particular, the Kubernetes Operator framework, combined with OpenStack Ironic. This approach aims to seamlessly integrate your infrastructure with your workloads, including baremetal servers, storage and container/VM workloads. All this can be achieved using kubernetes native applications, combined with existing, proven deployment and storage tooling. In this talk, we cover the options around Kubernetes deployments today, the specific approach taken by the new Kubernetes-native “MetalKube” project, and the status/roadmap of this new community effort. Speakers: Steve Hardy is a Senior Principal Software Engineer at Red Hat, currently involved in kubernetes/OpenShift deployment and architecture. He is also an active member of the OpenStack community and has been a project team leader of both the Heat (orchestration) and TripleO (deployment) projects. References:  Open Infrastructure Days UK 2019, Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat"
    }, {
    "id": 21,
    "url": "/blog/2019/10/31/OpenStack-Ironic-and-Bare-Metal-Infrastructure_All-Abstractions-Start-Somewhere.html",
    "title": "OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere - Chris Hoge, OpenStack Foundation; Julia Kreger, Red Hat",
    "author" : "Pedro Ibáñez Requena",
    "tags" : "kubernetes, metal3, operator, baremetal, openstack",
    "body": "Conference talk: OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere: The history of cloud computing has rapidly layered abstractions on abstractions to deliver applications faster, more reliably, and easier. Serverless functions on top of containers on top of virtualization. However, at the bottom of every stack is physical hardware that has an entire lifecycle that needs to be managed. In this video, Chris and Julia show how OpenStack Ironic is a solution to the problem of managing bare-metal infrastructure. Speakers: Chris Hoge is a Senior Strategic Program Manager for the OpenStack foundation. He’s been an active contributor to the Interop Working Group (formerly DefCore) and helps run the trademark program for the OpenStack Foundation. He also works on collaborations between the OpenStack and Kubernetes communities. Previously he worked as an OpenStack community manager and developer at Puppet Labs and operated a research cloud for the College of Arts and Sciences at The University of Oregon. When not cloud computing, he enjoys long-distance running, dancing, and throwing a ball for his Border Collie. Julia Kreger is Principal Software Engineer at Red Hat. She started her career in networking and eventually shifted to systems engineering. The DevOps movement leads her into software development and the operationalization of software due to the need to automate large-scale systems deployments. She is experienced in conveying an operational perspective while bridging that with requirements and doesn’t mind getting deep down into code to solve a problem. She is an active core contributor and leader in the OpenStack Ironic project, which is a project she feels passionate about due to many misspent hours in data centres deploying hardware. Prior to OpenStack, Julia contributed to the Shared Learning Infrastructure and worked with large-scale litigation database systems. References:  Open Infrastructure Summit, Denver, CO, April 29 - May 1, 2019 OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere"
    }, {
    "id": 22,
    "url": "/blog/2019/09/11/Baremetal-operator.html",
    "title": "Baremetal Operator",
    "author" : "Pablo Iranzo Gómez",
    "tags" : "openshift, kubernetes, metal3, operator",
    "body": "Introduction: The baremetal operator, documented here, it’s the Operator in charge of definitions of physical hosts, containing information about how to reach the Out of Band management controller, URL with the desired image to provision, plus other properties related with hosts being used for provisioning instances. Quoting from the project:  The Bare Metal Operator implements a Kubernetes API for managing bare metal hosts. It maintains an inventory of available hosts as instances of the BareMetalHost Custom Resource Definition. The Bare Metal Operator knows how to:Inspect the host’s hardware details and report them on the corresponding BareMetalHost. This includes information about CPUs, RAM, disks, NICs, and more. Provision hosts with a desired imageClean a host’s disk contents before or after provisioning. A bit more in deep approach: The Baremetal Operator (BMO) keeps a mapping of each host and its management interfaces (vendor-based like iLO, iDrac, iRMC, etc) and is controlled via IPMI. All of this is defined in a CRD, for example: apiVersion: v1kind: Secretmetadata: name: metal3-node01-credentials namespace: metal3type: Opaquedata: username: YWRtaW4= password: YWRtaW4=---apiVersion: metal3. io/v1alpha1kind: BareMetalHostmetadata: name: metal3-node01 namespace: metal3spec: bmc:  address: ipmi://172. 22. 0. 2:6230  credentialsName: metal3-node01-credentials bootMACAddress: 00:c2:fc:3b:e1:01 description:    hardwareProfile:  libvirt  online: falseWith above values (described in API), we’re telling the operator:  MAC: Defines the mac address of the NIC connected to the network that will be used for the provisioning of the host bmc: defines the management controller address and the secret used credentialsName: Defines the name of the secret containing username/password for accessing the IPMI serviceOnce the server is ‘defined’ via the CRD, the underlying service (provided by ironic1 as of this writing) is inspected: [root@metal3-kubernetes ~]# kubectl get baremetalhost -n metal3NAME      STATUS  PROVISIONING STATUS  CONSUMER  BMC           HARDWARE PROFILE  ONLINE  ERRORmetal3-node01  OK    inspecting            ipmi://172. 22. 0. 1:6230           falseOnce the inspection has finished, the status will change to ready and made available for provisioning. When we define a machine, we refer to the images that will be used for the actual provisioning in the CRD (image): apiVersion: v1data: userData: DATAkind: Secretmetadata: name: metal3-node01-user-data namespace: metal3type: Opaque---apiVersion:  cluster. k8s. io/v1alpha1 kind: Machinemetadata: name: metal3-node01 namespace: metal3 generateName: baremetal-machine-spec: providerSpec:  value:   apiVersion:  baremetal. cluster. k8s. io/v1alpha1    kind:  BareMetalMachineProviderSpec    image:    url: http://172. 22. 0. 2/images/CentOS-7-x86_64-GenericCloud-1901. qcow2    checksum: http://172. 22. 0. 2/images/CentOS-7-x86_64-GenericCloud-1901. qcow2. md5sum   userData:    name: metal3-node01-user-data    namespace: metal3[root@metal3-kubernetes ~]# kubectl create -f metal3-node01-machine. ymlsecret/metal3-node01-user-data createdmachine. cluster. k8s. io/metal3-node01 createdLet’s examine the annotation created when provisioning (metal3. io/BareMetalHost): [root@metal3-kubernetes ~]# kubectl get machine -n metal3 metal3-node01 -o yamlapiVersion: cluster. k8s. io/v1alpha1kind: Machinemetadata: annotations:  metal3. io/BareMetalHost: metal3/metal3-node01 creationTimestamp:  2019-07-08T15:30:44Z  finalizers: - machine. cluster. k8s. io generateName: baremetal-machine- generation: 2 name: metal3-node01 namespace: metal3 resourceVersion:  6222  selfLink: /apis/cluster. k8s. io/v1alpha1/namespaces/metal3/machines/metal3-node01 uid: 1bfd384a-5467-43b7-98aa-e80e1ace5ce7spec: metadata:  creationTimestamp: null providerSpec:  value:   apiVersion: baremetal. cluster. k8s. io/v1alpha1   image:    checksum: http://172. 22. 0. 1/images/CentOS-7-x86_64-GenericCloud-1901. qcow2. md5sum    url: http://172. 22. 0. 1/images/CentOS-7-x86_64-GenericCloud-1901. qcow2   kind: BareMetalMachineProviderSpec   userData:    name: metal3-node01-user-data    namespace: metal3 versions:  kubelet:   status: addresses: - address: 192. 168. 122. 79  type: InternalIP - address: 172. 22. 0. 39  type: InternalIP - address: localhost. localdomain  type: Hostname lastUpdated:  2019-07-08T15:30:44Z In the output above, the host assigned was the one we’ve defined earlier as well as the other parameters like IP’s, etc generated. Now, if we check baremetal hosts, we can see how it’s getting provisioned: [root@metal3-kubernetes ~]# kubectl get baremetalhost -n metal3NAME      STATUS  PROVISIONING STATUS  CONSUMER  BMC           HARDWARE PROFILE  ONLINE  ERRORmetal3-node01  OK    provisioned            ipmi://172. 22. 0. 1:6230           trueAnd also, check it via the ironic command: [root@metal3-kubernetes ~]# export OS_TOKEN=fake-token ; export OS_URL=http://localhost:6385 ; openstack baremetal node list+--------------------------------------+---------------+--------------------------------------+-------------+--------------------+-------------+| UUID                 | Name     | Instance UUID            | Power State | Provisioning State | Maintenance |+--------------------------------------+---------------+--------------------------------------+-------------+--------------------+-------------+| 7551cfb4-d758-4ad8-9188-859ee53cf298 | metal3-node01 | 7551cfb4-d758-4ad8-9188-859ee53cf298 | power on  | active       | False    |+--------------------------------------+---------------+--------------------------------------+-------------+--------------------+-------------+Wrap-up: We’ve seen how via a CRD we’ve defined credentials for a baremetal host to make it available to get provisioned and how we’ve also defined a machine that was provisioned on top of that baremetal host.       Ironic was chosen as the initial provider for baremetal provisioning, check Ironic documentation for more details about Ironic usage in Metal³ &#8617;    "
    }, {
    "id": 23,
    "url": "/blog/2019/06/25/Metal3.html",
    "title": "Metal3",
    "author" : "Eduardo Minguez",
    "tags" : "openshift, kubernetes, metal3",
    "body": "Originally posted at https://www. underkube. com/posts/2019-06-25-metal3/ In this blog post, I’m going to try to explain in my own words a high leveloverview of what Metal3 is, the motivation behind it and some concepts relatedto a ‘baremetal operator’. Let’s have some definitions! Custom Resource Definition: The k8s API provides out-of-the-box objects such as pods, services, etc. There are a few methods of extending the k8s API (such as API extensions)but since a few releases back, the k8s API can be extended easily with custom resources definitions (CRDs). Basically, this means you can virtually create any type of object definition in k8s(actually only users with cluster-admin capabilities) with a yaml such as: apiVersion: apiextensions. k8s. io/v1beta1kind: CustomResourceDefinitionmetadata: # name must match the spec fields below, and be in the form: &lt;plural&gt;. &lt;group&gt; name: crontabs. stable. example. comspec: # group name to use for REST API: /apis/&lt;group&gt;/&lt;version&gt; group: stable. example. com # list of versions supported by this CustomResourceDefinition versions: - name: v1  # Each version can be enabled/disabled by Served flag.   served: true  # One and only one version must be marked as the storage version.   storage: true # either Namespaced or Cluster scope: Namespaced names:  # plural name to be used in the URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt;  plural: crontabs  # singular name to be used as an alias on the CLI and for display  singular: crontab  # kind is normally the CamelCased singular type. Your resource manifests use this.   kind: CronTab  # shortNames allow shorter string to match your resource on the CLI  shortNames:  - ct preserveUnknownFields: false validation:  openAPIV3Schema:   type: object   properties:    spec:     type: object     properties:      cronSpec:       type: string      image:       type: string      replicas:       type: integerAnd after kubectl apply -f you can kubectl get crontabs. There is a ton of information with regards to CRDs, like the k8s official documentation. The CRD by himself is not useful per se as nobody will take care of it (that’s why I said definition). Itrequires a controller to watch for those new objects and react to differentevents affecting the object. Controller: A controller is basically a loop that watches the current status of an objectand if it is different from the desired status, it fixes it (reconciliation). This is why k8s is ‘declarative’, you specify the object desired status instead‘how to do it’ (imperative). Again, there are tons of documentation (and examples) around the controller pattern which isbasically the k8s roots, so I’ll let your google-foo take care of it :) Operator: An Operator (in k8s slang) is an application running in your k8scluster that deploys, manages and maintains (so, operates) a k8s application. This k8s application (the one that the operator manages), can be as simple as a ‘hello world’ applicationcontainerized and deployed in your k8s cluster or it can be a much more complexthing, such as a database cluster. The ‘operator’ is like an ‘expert sysadmin’ containerized that takes care ofyour application. Bear in mind that the ‘expert’ tag (meaning the automation behind the operator)depends on the operator implementation… so there can be basic operators thatonly deploy your application or complex operators that handle day 2 operationssuch as upgrades, failovers, backup/restore, etc. See the CoreOS operator definition for more information. Cloud Controller Manager: k8s code is smart enough to be able to leveragethe underlying infrastructure where the cluster is running, such as being ableof creating ‘LoadBalancer’ services, understanding the cluster topology based on the cloud provider AZs where the nodes are running (for scheduling reasons), etc. This task of ‘talking to the cloud provider’ is performed by the Cloud Controller Manager (CCM) and for moreinformation, you can take a look at the official k8s documentation withregards the architecture and the administration (also, if you are brave enough, you can create your own cloud controller manager ) Cluster API: The Cluster API implementation is a WIP ‘framework’ that allows a k8s cluster to manage itself, including the ability to create new clusters, add more nodes, etc. in a ‘k8s way’ (declarative, controllers, CRDs, etc. ), so there are objects such as Cluster that can be expressed as k8s objects: apiVersion:  cluster. k8s. io/v1alpha1 kind: Clustermetadata: name: myclusterspec: clusterNetwork:  services:   cidrBlocks: [ 10. 96. 0. 0/12 ]  pods:   cidrBlocks: [ 192. 168. 0. 0/16 ]  serviceDomain:  cluster. local  providerSpec: . . . but also:  Machine type objects MachineSet type objects MachineDeployment type objects etcThere are someprovider implementations in the wild such as AWS, Azure, GCP, OpenStack,vSphere, etc. ones and the Cluster API project is driven by the SIG Cluster Lifecycle. Please review the official Cluster API repository for more information. Actuator: The actuator is a Cluster API interface that reacts to changes to Machineobjects reconciliating the Machine status. The actuator code is tightly coupled with the provider (that’s why it is aninterface) such as the AWS one. MachineSet vs Machine: To simplify, let’s say that MachineSets are to Machines what ReplicaSets areto Pods. So you can scale the Machines in your cluster just by changingthe number of replicas of a MachineSet. Cluster API vs Cloud Providers: As we have seen, the Cluster API leverages the provider related to the k8sinfrastructure itself (clusters and nodes) and the CCM and the cloud providerintegration for k8s is to leverage the cloud provider to provide support infrastructure. Let’s say Cluster API is for the k8s administrators and theCCM is for the k8s users :) Machine API: The OpenShift 4 Machine API is a combination of some of the upstream Cluster APIwith custom OpenShift resources and it is designed to work in conjunction withthe Cluster Version Operator. OpenShift’s Machine API Operator: The machine-api-operator isan operator that manages the Machine API objects in an OpenShift 4 cluster. The operator is capable of creating machines in AWS and libvirt (more providerscoming soon) via the Machine Controller and it is included out of thebox with OCP 4 (and can be deployed in a k8s vanilla as well) Baremetal: A baremetal server (or bare-metal) is just a computer server. The last year’s terms such as virtualization, containers, serverless, etc. have beenpopular but at the end of the day, all the code running on top of a SaaS, PaaSor IaaS is actually running in a real physical server stored in a datacenterwired to routers, switches and power. That server is a ‘baremetal’ server. If you are used to cloud providers and instances, you probably don’t know thepains of baremetal management… including things such as connecting to thevirtual console (usually it requires an old Java version) to debug issues,configuring pxe for provisioning baremetal hosts (or attach ISOs via the virtual console… or insert a CD/DVD physically into the CD carry if you are‘lucky’ enough…), configuring VLANs for traffic isolation, etc. That kind of operation is not ‘cloud’ ready and there are tools that providebaremetal management, such as maas or ironic. Ironic: OpenStack bare metal provisioning (or ironic) is an open source project (or even better, a number of open source projects) to manage baremetal hosts. Ironic avoids the administrator dealing with pxe configuration, manual deployments, etc. and provides a defined API and a series of plugins to interact with different baremetal models and vendors. Ironic is used in OpenStack to provide baremetal objects but there are someprojects (such as bifrost) to useIronic ‘standalone’ (so, no OpenStack required) Metal3: Metal3 is a project aimed at providing a baremetal operator thatimplements the Cluster API framework required to be able to manage baremetalin a k8s way (easy peasy!). It uses ironic under the hood to avoid reinventing thewheel, but consider it as an implementation detail that may change. The Metal3 baremetal operator watches for BareMetalHost (CRD) objects defined as: apiVersion: metal3. io/v1alpha1kind: BareMetalHostmetadata: name: my-worker-0spec: online: true bootMACAddress: 00:11:22:33:44:55 bmc:  address: ipmi://my-worker-0. ipmi. example. com  credentialsName: my-worker-0-bmc-secretThere are a few more fields in the BareMetalHost object such as the image, hardware profile, etc. The Metal3 project is actually divided into two different components: baremetal-operator: The Metal3 baremetal-operator is the component that manages baremetal hosts. It exposes a new BareMetalHost custom resource in the k8s API that lets you manage hosts in a declarative way. cluster-api-provider-baremetal: The Metal3 cluster-api-provider-baremetal includes the integration with the Cluster API project. This provider currently includes a Machine actuator that acts as a client of the BareMetalHost custom resources. BareMetalHost vs Machine vs Node:  BareMetalHost is a Metal3 object Machine is a Cluster API object Node is where the pods run :)Those three concepts are linked in a 1:1:1 relationship meaning: A BareMetalHost created with Metal3 maps to a Machine object and once theinstallation procedure finishes, a new kubernetes node will be added to thecluster. $ kubectl get nodesNAME                     STATUS  ROLES  AGE  VERSIONmy-node-0. example. com            Ready  master  25h  v1. 14. 0$ kubectl get machines --all-namespacesNAMESPACE        NAME         INSTANCE  STATE  TYPE  REGION  ZONE  AGEopenshift-machine-api  my-node-0                          25h$ kubectl get baremetalhosts --allnamespacesNAMESPACE       NAME   STATUS PROVISIONING STATUS MACHINE BMC HARDWARE PROFILE ONLINE ERRORopenshift-machine-api my-node-0 OK   provisioned my-node-0. example. com ipmi://1. 2. 3. 4 unknown trueThe 1:1 relationship for the BareMetalHost and the Machine is stored in themachineRef field in the BareMetalHost object: $ kubectl get baremetalhost/my-node-0 -n openshift-machine-api -o jsonpath='{. spec. machineRef}'map[name:my-node-0 namespace:openshift-machine-api]In a Machine annotation: $ kubectl get machines my-node-0 -n openshift-machine-api -o jsonpath='{. metadata. annotations}'map[metal3. io/BareMetalHost:openshift-machine-api/my-node-0]The node reference is stored in the . status. nodeRef. name field in theMachine object: $ kubectl get machine my-node-0 -o jsonpath='{. status. nodeRef. name}'my-node-0. example. comRecap: Being able to ‘just scale a node’ in k8s means a lot of underlying concepts and technologies involved behind the scenes :) Resources/links:  https://dzone. com/articles/introducing-the-kubernetes-cluster-api-project-2 https://blogs. vmware. com/tanzu/the-what-and-the-why-of-the-cluster-api/ https://github. com/kubernetes-sigs/cluster-api https://github. com/kubernetes-sigs/cluster-api-provider-aws https://itnext. io/deep-dive-to-cluster-api-a0b4e792d57d https://www. linux. com/topic/cloud/extending-kubernetes-cluster-api/"
    }, {
    "id": 24,
    "url": "/blog/2019/05/13/The_new_stack_Metal3_Uses_OpenStack_Ironic_for_Declarative_Bare_Metal_Kubernetes.html",
    "title": "The new stack Metal³ Uses OpenStack's Ironic for Declarative Bare Metal Kubernetes",
    "author" : "Pedro Ibáñez Requena",
    "tags" : "hybrid, cloud, metal3, baremetal, stack, edge, OpenStack, ironic",
    "body": "The new stack Metal³ Uses OpenStack’s Ironic for Declarative Bare Metal Kubernetes: Mike Melanson talks in this article about the Open Infrastructure Summit in Denver, Colorado. Where bare metal was one of the main leads of the event. During this event, the OpenStack Foundation unveil a new project called Metal³ (pronounced “metal cubed”) that uses Ironic “as a foundation for declarative management of bare metal infrastructure for Kubernetes”. He also comments on how James Penick, Chris Hoge, senior strategic program manager at OpenStack Foundation,and Julia Kreger, OpenStack Ironic Project Team Leader, took to the stage to offer a demonstration of Metal3,the new project that provides “bare metal host provisioning integration for Kubernetes. ” Some words from Kreger in an interview with The New Stack:  “I think the bigger trend that we’re starting to see is a recognition that common tooling and substrate helps everyone succeed faster with more efficiency. ”  “This is combined with a shift in the way operators are choosing to solve their problems at scale, specifically in regards to isolation, cost, or performance. ” For further detail, check out the video of the keynote, which includes a demonstration of Metal3 being used to quickly provision three bare metal servers with Kubernetesor check the full article included below. References:  The new stack: Metal³ Uses OpenStack’s Ironic for Declarative Bare Metal Kubernetes Video of the keynote: OpenStack Ironic and Baremetal Infrastructure. All Abstractions start somewhere"
    }, {
    "id": 25,
    "url": "/blog/2019/04/30/Metal-Kubed-Baremetal-Provisioning-for-Kubernetes.html",
    "title": "Metal³: Baremetal Provisioning for Kubernetes",
    "author" : "Russell Bryant",
    "tags" : "openshift, kubernetes, metal3",
    "body": "Originally posted at https://blog. russellbryant. net/post/2019/04/2019-04-30-metal-metal-kubed-bare-metal-provisioning-for-kubernetes/ Project Introduction: There are a number of great open-source tools for bare metal host provisioning, including Ironic. Metal³ aims to build on these technologies to provide a Kubernetes native API for managing bare metal hosts via a provisioning stack that is also running on Kubernetes. We believe that Kubernetes Native Infrastructure, or managing your infrastructure just like your applications, is a powerful next step in the evolution of infrastructure management. The Metal³ project is also building integration with the Kubernetes cluster-api project, allowing Metal³ to be used as an infrastructure backend for Machine objects from the Cluster API. Metal3 Repository Overview: There is a Metal³ overview and some more detailed design documents in the metal3-docs repository. The baremetal-operator is the component that manages bare metal hosts. It exposes a new BareMetalHost custom resource in the Kubernetes API that lets you manage hosts in a declarative way. Finally, the cluster-api-provider-baremetal repository includes integration with the cluster-api project. This provider currently includes a Machine actuator that acts as a client of the BareMetalHost custom resources. Demo: The project has been going on for a few months now, and there’s enough now to show some working code. For this demonstration, I’ve started with a 3-node Kubernetes cluster installed using OpenShift. $ kubectl get nodesNAME    STATUS  ROLES  AGE  VERSIONmaster-0  Ready  master  24h  v1. 13. 4+d4ce02c1dmaster-1  Ready  master  24h  v1. 13. 4+d4ce02c1dmaster-2  Ready  master  24h  v1. 13. 4+d4ce02c1dMachine objects were created to reflect these 3 masters, as well. $ kubectl get machinesNAME       INSTANCE  STATE  TYPE  REGION  ZONE  AGEostest-master-0                       24hostest-master-1                       24hostest-master-2                       24hFor this cluster-api provider, a Machine has a corresponding BareMetalHost object, which corresponds to the piece of hardware we are managing. There is a design document that covers the relationship between Nodes, Machines, and BareMetalHosts. Since these hosts were provisioned earlier, they are in a special externally provisioned state, indicating that we enrolled them in management while they were already running in a desired state. If changes are needed going forward, the baremetal-operator will be able to automate them. $ kubectl get baremetalhostsNAME         STATUS  PROVISIONING STATUS   MACHINE      BMC             HARDWARE PROFILE  ONLINE  ERRORopenshift-master-0  OK    externally provisioned  ostest-master-0  ipmi://192. 168. 111. 1:6230           trueopenshift-master-1  OK    externally provisioned  ostest-master-1  ipmi://192. 168. 111. 1:6231           trueopenshift-master-2  OK    externally provisioned  ostest-master-2  ipmi://192. 168. 111. 1:6232           trueNow suppose we’d like to expand this cluster by adding another bare metal host to serve as a worker node. First, we need to create a new BareMetalHost object that adds this new host to the inventory of hosts managed by the baremetal-operator. Here’s the YAML for the new BareMetalHost: ---apiVersion: v1kind: Secretmetadata: name: openshift-worker-0-bmc-secrettype: Opaquedata: username: YWRtaW4= password: cGFzc3dvcmQ=---apiVersion: metalkube. org/v1alpha1kind: BareMetalHostmetadata: name: openshift-worker-0spec: online: true bmc:  address: ipmi://192. 168. 111. 1:6233  credentialsName: openshift-worker-0-bmc-secret bootMACAddress: 00:ab:4f:d8:9e:faNow to add the BareMetalHost and its IPMI credentials Secret to the cluster: $ kubectl create -f worker_crs. yamlsecret/openshift-worker-0-bmc-secret createdbaremetalhost. metalkube. org/openshift-worker-0 createdThe list of BareMetalHosts now reflects a new host in the inventory that is ready to be provisioned. It will remain in this ready state until it is claimed by a new Machine object. $ kubectl get baremetalhostsNAME         STATUS  PROVISIONING STATUS   MACHINE      BMC             HARDWARE PROFILE  ONLINE  ERRORopenshift-master-0  OK    externally provisioned  ostest-master-0  ipmi://192. 168. 111. 1:6230           trueopenshift-master-1  OK    externally provisioned  ostest-master-1  ipmi://192. 168. 111. 1:6231           trueopenshift-master-2  OK    externally provisioned  ostest-master-2  ipmi://192. 168. 111. 1:6232           trueopenshift-worker-0  OK    ready                   ipmi://192. 168. 111. 1:6233  unknown      trueWe have a MachineSet already created for workers, but it scaled down to 0. $ kubectl get machinesetsNAME       DESIRED  CURRENT  READY  AVAILABLE  AGEostest-worker-0  0     0               24hWe can scale this MachineSet to 1 to indicate that we’d like a worker provisioned. The baremetal cluster-api provider will then look for an available BareMetalHost, claim it, and trigger provisioning of that host. $ kubectl scale machineset ostest-worker-0 --replicas=1 After the new Machine was created, our cluster-api provider claimed the available host and triggered it to be provisioned. $ kubectl get baremetalhostsNAME         STATUS  PROVISIONING STATUS   MACHINE         BMC             HARDWARE PROFILE  ONLINE  ERRORopenshift-master-0  OK    externally provisioned  ostest-master-0     ipmi://192. 168. 111. 1:6230           trueopenshift-master-1  OK    externally provisioned  ostest-master-1     ipmi://192. 168. 111. 1:6231           trueopenshift-master-2  OK    externally provisioned  ostest-master-2     ipmi://192. 168. 111. 1:6232           trueopenshift-worker-0  OK    provisioning       ostest-worker-0-jmhtc  ipmi://192. 168. 111. 1:6233  unknown      trueThis process takes some time. Under the hood, the baremetal-operator is driving Ironic through a provisioning process. This begins with wiping disks to ensure the host comes up in a clean state. It will eventually write the desired OS image to disk and then reboot into that OS. When complete, a new Kubernetes Node will register with the cluster. $ kubectl get baremetalhostsNAME         STATUS  PROVISIONING STATUS   MACHINE         BMC             HARDWARE PROFILE  ONLINE  ERRORopenshift-master-0  OK    externally provisioned  ostest-master-0     ipmi://192. 168. 111. 1:6230           trueopenshift-master-1  OK    externally provisioned  ostest-master-1     ipmi://192. 168. 111. 1:6231           trueopenshift-master-2  OK    externally provisioned  ostest-master-2     ipmi://192. 168. 111. 1:6232           trueopenshift-worker-0  OK    provisioned       ostest-worker-0-jmhtc  ipmi://192. 168. 111. 1:6233  unknown      true$ kubectl get nodesNAME    STATUS  ROLES  AGE  VERSIONmaster-0  Ready  master  24h  v1. 13. 4+d4ce02c1dmaster-1  Ready  master  24h  v1. 13. 4+d4ce02c1dmaster-2  Ready  master  24h  v1. 13. 4+d4ce02c1dworker-0  Ready  worker  68s  v1. 13. 4+d4ce02c1dThe following screen cast demonstrates this process, as well: Removing a bare metal host from the cluster is very similar. We just have to scale this MachineSet back down to 0. $ kubectl scale machineset ostest-worker-0 --replicas=0 Once the Machine has been deleted, the baremetal-operator will deprovision the bare metal host. $ kubectl get baremetalhostsNAME         STATUS  PROVISIONING STATUS   MACHINE      BMC             HARDWARE PROFILE  ONLINE  ERRORopenshift-master-0  OK    externally provisioned  ostest-master-0  ipmi://192. 168. 111. 1:6230           trueopenshift-master-1  OK    externally provisioned  ostest-master-1  ipmi://192. 168. 111. 1:6231           trueopenshift-master-2  OK    externally provisioned  ostest-master-2  ipmi://192. 168. 111. 1:6232           trueopenshift-worker-0  OK    deprovisioning               ipmi://192. 168. 111. 1:6233  unknown      falseOnce the deprovisioning process is complete, the bare metal host will be back to its ready state, available in the host inventory to be claimed by a future Machine object. $ kubectl get baremetalhostsNAME         STATUS  PROVISIONING STATUS   MACHINE      BMC             HARDWARE PROFILE  ONLINE  ERRORopenshift-master-0  OK    externally provisioned  ostest-master-0  ipmi://192. 168. 111. 1:6230           trueopenshift-master-1  OK    externally provisioned  ostest-master-1  ipmi://192. 168. 111. 1:6231           trueopenshift-master-2  OK    externally provisioned  ostest-master-2  ipmi://192. 168. 111. 1:6232           trueopenshift-worker-0  OK    ready                   ipmi://192. 168. 111. 1:6233  unknown      falseGetting Involved: All development is happening on github. We have a metal3-dev mailing list and use #cluster-api-baremetal on Kubernetes Slack to chat. Occasional project updates are posted to @metal3_io on Twitter. "
    }, {
    "id": 26,
    "url": "/blog/2019/04/12/Raise_some_horns_Red_Hat_s_MetalKube_aims_to_make_Kubernetes_on_bare_machines_simple.html",
    "title": "Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple",
    "author" : "Pedro Ibáñez Requena",
    "tags" : "hybrid, cloud, metal3, baremetal, stack, edge, openstack, ironic",
    "body": "The Register; Raise some horns: Red Hat’s Metal³ aims to make Kubernetes on bare machines simple: Max Smolaks talks inthis article about the OpenInfra Days in the UK, 2019: where Metal³ wasrevealed earlier last week by Steve Hardy, Red Hat’s senior principalsoftware engineer. The Open Infrastructure Days in the UK is an eventorganized by the local Open Infrastructure community and supported bythe OpenStack Foundation. The Open-source software developers at Red Hatare working on a tool that would simplify the deployment and managementof Kubernetes clusters on bare-metal servers. Steve told The Register:  “In some situations, you won’t want to run a full OpenStackinfrastructure-as-a-service layer to provide, potentially, formultiple Kubernetes clusters”. Hardy is a notable contributor to OpenStack, having previously worked onHeat and TripleO projects. He said one of the reasons for choosingIronic was its active development – and when new features get added toIronic, the Metal³ team gets them “for free”.  “OpenStack has always been a modular set of projects, and people havealways had the opportunity to reuse components for differentapplications. This is just an example of where we are leveraging oneparticular component for infrastructure management, just as analternative to using a full infrastructure API,” Hardy said. Thierry Carrez, veep of engineering at the OpenStack Foundation also toldThe Register:  “I like the fact that the projects end up being reusable on their own,for the functions they bring to the table – this helps us integratewith adjacent communities”. Hardy also commented:  It’s still early days for Metal³ - the project has just sixcontributors, and there’s no telling when it might reach release. “It’s a very, very young project but we are keen to get more communityparticipation and feedback,”. For further detail, check out the full article atThe Register: Raise some horns: Red Hat’s MetalKube aims to make Kubernetes onbare machines simple. References:  Steve Hardy: Red Hat’s seniorprincipal software engineer.  Thierry Carrez: veep of engineering at theOpenStack Foundation.  The Register: Raise some horns: Red Hat’s MetalKube aims to make Kubernetes on bare machines simple"
    }, , {
    "id": 27,
    "url": "/blog/categories.html",
    "title": "Categories",
    "author" : "",
    "tags" : "",
    "body": " -   Blog  Read about the newest updates in the community. &lt;/section&gt;             Categories:        hybrid:               One cluster - multiple providers July 08, 2022                 Metal³: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019 January 20, 2020                 Introducing Metal³: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat - KubeCon NA, November 2019 December 04, 2019                 Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019 November 13, 2019                 The new stack Metal³ Uses OpenStack's Ironic for Declarative Bare Metal Kubernetes May 13, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             cloud:               Metal³: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019 January 20, 2020                 Introducing Metal³: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat - KubeCon NA, November 2019 December 04, 2019                 Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019 November 13, 2019                 The new stack Metal³ Uses OpenStack's Ironic for Declarative Bare Metal Kubernetes May 13, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             metal3:               Deploying OCI Container Images to Bare Metal with a Custom IPA Hardware Manager February 01, 2026                 Metal3. io Becomes a CNCF Incubating Project August 27, 2025                 Introducing Baremetal Operator end-to-end test suite December 13, 2024                 Scaling Kubernetes with Metal3: Simulating 1000 Clusters with Fake Ironic Agents October 24, 2024                 Scaling to 1000 clusters - Part 3 May 30, 2024                 Metal3 at KubeCon EU 2024 April 10, 2024                 How to run Metal3 website locally with Jekyll January 18, 2024                 Scaling to 1000 clusters - Part 2 May 17, 2023                 Scaling to 1000 clusters - Part 1 May 05, 2023                 One cluster - multiple providers July 08, 2022                 Metal3 Introduces Pivoting May 05, 2021                 Introducing the Metal3 IP Address Manager July 06, 2020                 Raw image streaming available in Metal3 July 05, 2020                 Metal³ development environment walkthrough part 2: Deploying a new bare metal cluster June 18, 2020                 Cluster API provider renaming March 05, 2020                 Metal³: Kubernetes Native Bare Metal Cluster Management - Maël Kimmerlin - Kubernetes and CNCF Finland Meetup February 27, 2020                 A detailed walkthrough of the Metal³ development environment February 18, 2020                 Metal³: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019 January 20, 2020                 Introducing Metal³: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat - KubeCon NA, November 2019 December 04, 2019                 Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019 November 13, 2019                 Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat November 07, 2019                 OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere - Chris Hoge, OpenStack Foundation; Julia Kreger, Red Hat October 31, 2019                 Baremetal Operator September 11, 2019                 Metal3 June 25, 2019                 The new stack Metal³ Uses OpenStack's Ironic for Declarative Bare Metal Kubernetes May 13, 2019                 Metal³: Baremetal Provisioning for Kubernetes April 30, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             baremetal:               Scaling Kubernetes with Metal3: Simulating 1000 Clusters with Fake Ironic Agents October 24, 2024                 How to run Metal3 website locally with Jekyll January 18, 2024                 Metal3 Introduces Pivoting May 05, 2021                 Introducing the Metal3 IP Address Manager July 06, 2020                 Raw image streaming available in Metal3 July 05, 2020                 Cluster API provider renaming March 05, 2020                 Metal³: Kubernetes Native Bare Metal Cluster Management - Maël Kimmerlin - Kubernetes and CNCF Finland Meetup February 27, 2020                 A detailed walkthrough of the Metal³ development environment February 18, 2020                 Metal³: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019 January 20, 2020                 Introducing Metal³: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat - KubeCon NA, November 2019 December 04, 2019                 Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019 November 13, 2019                 Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat November 07, 2019                 OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere - Chris Hoge, OpenStack Foundation; Julia Kreger, Red Hat October 31, 2019                 The new stack Metal³ Uses OpenStack's Ironic for Declarative Bare Metal Kubernetes May 13, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             stack:               The new stack Metal³ Uses OpenStack's Ironic for Declarative Bare Metal Kubernetes May 13, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             edge:               Introducing Baremetal Operator end-to-end test suite December 13, 2024                 Scaling to 1000 clusters - Part 3 May 30, 2024                 Scaling to 1000 clusters - Part 2 May 17, 2023                 Scaling to 1000 clusters - Part 1 May 05, 2023                 One cluster - multiple providers July 08, 2022                 Metal³: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019 January 20, 2020                 Introducing Metal³: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat - KubeCon NA, November 2019 December 04, 2019                 Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019 November 13, 2019                 The new stack Metal³ Uses OpenStack's Ironic for Declarative Bare Metal Kubernetes May 13, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             openstack:               Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat November 07, 2019                 OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere - Chris Hoge, OpenStack Foundation; Julia Kreger, Red Hat October 31, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             ironic:               Deploying OCI Container Images to Bare Metal with a Custom IPA Hardware Manager February 01, 2026                 Scaling Kubernetes with Metal3: Simulating 1000 Clusters with Fake Ironic Agents October 24, 2024                 Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat November 07, 2019                 The new stack Metal³ Uses OpenStack's Ironic for Declarative Bare Metal Kubernetes May 13, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             openshift:               Baremetal Operator September 11, 2019                 Metal3 June 25, 2019                 Metal³: Baremetal Provisioning for Kubernetes April 30, 2019             kubernetes:               Metal³ development environment walkthrough part 2: Deploying a new bare metal cluster June 18, 2020                 Metal³: Kubernetes Native Bare Metal Cluster Management - Maël Kimmerlin - Kubernetes and CNCF Finland Meetup February 27, 2020                 Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat November 07, 2019                 OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere - Chris Hoge, OpenStack Foundation; Julia Kreger, Red Hat October 31, 2019                 Baremetal Operator September 11, 2019                 Metal3 June 25, 2019                 Metal³: Baremetal Provisioning for Kubernetes April 30, 2019             OpenStack:               The new stack Metal³ Uses OpenStack's Ironic for Declarative Bare Metal Kubernetes May 13, 2019             operator:               Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat November 07, 2019                 OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere - Chris Hoge, OpenStack Foundation; Julia Kreger, Red Hat October 31, 2019                 Baremetal Operator September 11, 2019             summit:               Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019 November 13, 2019             kubecon:               Metal3 at KubeCon EU 2024 April 10, 2024                 Introducing Metal³: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat - KubeCon NA, November 2019 December 04, 2019             shiftdev:               Metal³: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019 January 20, 2020             metal3-dev-env:               How to run Metal3 website locally with Jekyll January 18, 2024                 Metal³ development environment walkthrough part 2: Deploying a new bare metal cluster June 18, 2020                 A detailed walkthrough of the Metal³ development environment February 18, 2020             documentation:               How to run Metal3 website locally with Jekyll January 18, 2024                 A detailed walkthrough of the Metal³ development environment February 18, 2020             development:               How to run Metal3 website locally with Jekyll January 18, 2024                 A detailed walkthrough of the Metal³ development environment February 18, 2020             talk:               Metal3 at KubeCon EU 2024 April 10, 2024                 Metal³: Kubernetes Native Bare Metal Cluster Management - Maël Kimmerlin - Kubernetes and CNCF Finland Meetup February 27, 2020             conference:               Metal3 at KubeCon EU 2024 April 10, 2024                 Metal³: Kubernetes Native Bare Metal Cluster Management - Maël Kimmerlin - Kubernetes and CNCF Finland Meetup February 27, 2020             meetup:               Metal³: Kubernetes Native Bare Metal Cluster Management - Maël Kimmerlin - Kubernetes and CNCF Finland Meetup February 27, 2020             cluster API:               Introducing Baremetal Operator end-to-end test suite December 13, 2024                 Scaling Kubernetes with Metal3: Simulating 1000 Clusters with Fake Ironic Agents October 24, 2024                 Scaling to 1000 clusters - Part 3 May 30, 2024                 Scaling to 1000 clusters - Part 2 May 17, 2023                 Scaling to 1000 clusters - Part 1 May 05, 2023                 One cluster - multiple providers July 08, 2022                 Metal³ development environment walkthrough part 2: Deploying a new bare metal cluster June 18, 2020                 Cluster API provider renaming March 05, 2020             provider:               Introducing Baremetal Operator end-to-end test suite December 13, 2024                 Scaling to 1000 clusters - Part 3 May 30, 2024                 Scaling to 1000 clusters - Part 2 May 17, 2023                 Scaling to 1000 clusters - Part 1 May 05, 2023                 One cluster - multiple providers July 08, 2022                 Cluster API provider renaming March 05, 2020             raw image:               Raw image streaming available in Metal3 July 05, 2020             image streaming:               Raw image streaming available in Metal3 July 05, 2020             IPAM:               Introducing the Metal3 IP Address Manager July 06, 2020             ip address manager:               Introducing the Metal3 IP Address Manager July 06, 2020             Pivoting:               Metal3 Introduces Pivoting May 05, 2021             Move:               Metal3 Introduces Pivoting May 05, 2021             scaling:               Scaling Kubernetes with Metal3: Simulating 1000 Clusters with Fake Ironic Agents October 24, 2024             cncf:               Metal3. io Becomes a CNCF Incubating Project August 27, 2025             community:               Metal3. io Becomes a CNCF Incubating Project August 27, 2025             announcement:               Metal3. io Becomes a CNCF Incubating Project August 27, 2025             IPA:               Deploying OCI Container Images to Bare Metal with a Custom IPA Hardware Manager February 01, 2026             OCI:               Deploying OCI Container Images to Bare Metal with a Custom IPA Hardware Manager February 01, 2026             deployment:               Deploying OCI Container Images to Bare Metal with a Custom IPA Hardware Manager February 01, 2026             bare metal:               Deploying OCI Container Images to Bare Metal with a Custom IPA Hardware Manager February 01, 2026                     Categories:             hybrid        cloud        metal3        baremetal        stack        edge        openstack        ironic        openshift        kubernetes        OpenStack        operator        summit        kubecon        shiftdev        metal3-dev-env        documentation        development        talk        conference        meetup        cluster API        provider        raw image        image streaming        IPAM        ip address manager        Pivoting        Move        scaling        cncf        community        announcement        IPA        OCI        deployment        bare metal      "
    }, {
    "id": 28,
    "url": "/community-resources.html",
    "title": "Community Resources",
    "author" : "",
    "tags" : "",
    "body": " -     Community Resources    Join conversations with the other people who are involved in the creation, maintenance, and future of Metal3. io. &lt;/section&gt;           Around the Web: Conference Talks:  Metal3: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019 Introducing Metal3 kubernetes native bare metal host management - Kubecon NA 2019 Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019 OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere - Chris Hoge, OpenStack Foundation; Julia Kreger, Red Hat Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red HatIn The News:  The New Stack: Metal3 Uses OpenStack’s Ironic for Declarative Bare Metal Kubernetes The Register: Raise some horns: Red Hat’s MetalKube aims to make Kubernetes on bare machines simpleBlog Posts:  Metal3 Blog postsCommunity Meetups:  Join Metal3 Team Meetups to engage in discussion with members and helpwith a deeper understanding of the project as well as the futurediscussion               Get Connected:                                                                                                                                                                                Mailing List                               Slack                     Twitter                                                                               GitHub                                                                                                       Blog                                                            YouTube                           Partnering Communities:                                                                                                                                         Ironic Website                                                                                                                          Ironic Community                                  Cluster API Community                  Providers of Testing Resources:                     Nordix. Details here.         Netlify Open Source Plan.           "
    }, {
    "id": 29,
    "url": "/contribute.html",
    "title": "Contribute",
    "author" : "",
    "tags" : "",
    "body": " - ContributeWant to contribute to the Metal3 Project? Here's everything you need to know. &lt;/section&gt;  &lt;main&gt;  &lt;section class= mk-main__section &gt;    &lt;header class= mk-main__header &gt;  &lt;h2 class= mk-heading mk-heading--xl mk-m-border &gt;About Metal3 Community&lt;/h2&gt;&lt;/header&gt; The Metal3 community is an open-source community dedicated to the advancement of the Metal3 project, a Kubernetes-based solution for managing bare metal infrastructure as code. Leveraging Kubernetes and tools like Ironic, Metal3 allows users to treat physical machines like virtual machines, simplifying provisioning, management, and scalability of bare metal resources. If you want to learn more about the Metal3 community or get involved in the project, you can visit their official community page at Community Resources.  How to Contribute: Familiarize yourself with the Metal3 repositories on GitHub. Follow the documentation to set up your development environment. This will allow you to test your changes locally before submitting them. Join community meetings and mailing list to engage in discussions with the members of the metal3 community. Join Ironic Community for steps on how to contribute. Check out community resources page to find out more.  Best Practices:   Review existing issues, feature requests, and ongoing discussions to find areas where you can contribute.  Introduce yourself and be patient while waiting for your PR to be reviewed.  If it's your first time contributing, look for issues with good first issue labels.  Ask questions and seek clarifications when needed.    Open Issues:     Review the Metal3 project's existing issues.   Review Ironic Bugs to contribute to the Ironic project. &lt;/section&gt;&lt;/main&gt; "
    }, {
    "id": 30,
    "url": "/documentation.html",
    "title": "Documentation",
    "author" : "",
    "tags" : "",
    "body": " -       Documentation     The Metal³ project (pronounced “metal cubed”) exists to provide components that allow you to do bare metal host management for Kubernetes. Metal³ works as a Kubernetes application, meaning it runs on Kubernetes and is managed through Kubernetes interfaces.      If you are looking for documentation about how to use Metal³, please check the user-guide.            Metal3 Component Overview:                     It is helpful to understand the high level architecture of of the Machine API Integration. Click on each step to learn more about that particular component.                   Machine controller:           Bare metal actuator                          The first component is the Bare Metal Actuator, which is an implementation of the Machine Actuator interface defined by the cluster-api project. This actuator reacts to changes to Machine objects and acts as a client of the BareMetalHost custom resources managed by the Bare Metal                           Bare metal operator:           With CRDs representing bare metal inventory with configuration needed by its bare metal management workers.                             The architecture also includes a new Bare Metal Operator, which includes the following:             A Controller for a new Custom Resource, BareMetalHost. This custom resource represents an inventory of known (configured or automatically discovered) bare metal hosts. When a Machine is created the Bare Metal Actuator will claim one of these hosts to be provisioned as a new Kubernetes node.             In response to BareMetalHost updates, the controller will perform bare metal host provisioning actions as necessary to reach the desired state.             The creation of the BareMetalHost inventory can be done in two ways:                           Manually via creating BareMetalHost objects.               Optionally, automatically created via a bare metal host discovery process.                 For more information about Operators, see the operator-sdk.                                         Bare metal management pods:                           The operator manages a set of tools for controlling the power on the host, monitoring the host status, and provisioning images to the host. These tools run inside the pod with the operator, and do not require any configuration by the user.                                       Around the Web: Conference Talks:  Metal3: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019 Introducing Metal3 kubernetes native bare metal host management - Kubecon NA 2019 Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019 OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere - Chris Hoge, OpenStack Foundation; Julia Kreger, Red Hat Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red HatIn The News:  The New Stack: Metal3 Uses OpenStack’s Ironic for Declarative Bare Metal Kubernetes The Register: Raise some horns: Red Hat’s MetalKube aims to make Kubernetes on bare machines simpleBlog Posts:  Metal3 Blog postsCommunity Meetups:  Join Metal3 Team Meetups to engage in discussion with members and helpwith a deeper understanding of the project as well as the futurediscussion  "
    }, {
    "id": 31,
    "url": "/faqs.html",
    "title": "FAQs",
    "author" : "",
    "tags" : "",
    "body": " -   Frequently Asked Questions  Here are some answers to common questions, discover more about Metal3                   What is the baremetal operator?    :          Baremetal Operator is a Kubernetes controller providing support forseveral custom resources, most importantly - BareMetalHosts.                  What kind of boot processes can be paired with specific BMC protocols?    :          Drivers with “virtual media” in their name can use the virtual mediatechnology to boot an ISO remotely. The other drivers require networkboot, more specifically - iPXE.                  What is Cluster API provider Metal3 (CAPM3)?    :          CAPM3 is aninfrastructure providerfor the Cluster API that uses Metal3 and Ironic to provision machinesfor your cluster.                  How does Metal3 relate to Cluster API (CAPI)?    :          The Metal3 project includes the Cluster API Provider Metal3 (CAPM3) - aninfrastructure provider for Cluster API.                  What CPU architectures are supported?    :          Both x86_64 (Intel) and AARCH64 (Arm) are supported. Mixed architectures(e. g. some hosts x86_64, some - aarch64) are not supported yet.                  What is IPMI?    :          IPMI is the acronym for Intelligent Platform Management Interfacewhich is used to monitor hardware health (fans, voltage, temperature,etc). The specification is available athereand was created by a joint effort by several manufacturers. It allows usto also define the boot order and power status of the hardware.                  What kinds of operating systems can be installed?    :          You can use any operating system that is available in a cloud format(e. g. qcow2). If you need first boot configuration, the image has tocontain cloud-init or a similar first-boot tool.                  Does Metal3 support provisioners other than Ironic?    :          While it’s technically possible to add more provisioners, only Ironic issupported now, and supporting other provisioners is not on the currentroadmap.                  How can one supply network configuration during provisioning?    :          You can put it to the BareMetalHost’s network Data field in theOpenStack network data format.                  Ironic is developed as part of OpenStack, does Metal3 require OpenStack?    :          Ironic can be used as a stand-alone service without any other OpenStackservices. In fact, Baremetal Operator does not support any otherOpenStack services.                  Can I use my own operating system installer with Metal3?    :          You can use the live ISO workflowto attach a bootable ISO to the machine using virtual media. Note thatBaremetal Operator will not track the installation process in this caseand will consider the host active once the ISO is booted.                  What is an out-of-band management controller?    :          Enterprise hardware usually has an integrated or optional controllerthat allows reaching the server even if it’s powered down, either viadedicated or shared nic. This controller allows some checks on theserver hardware and also perform some settings like changing powerstatus, changing Boot Order, etc. The Baremetal Operator uses it topower on, reboot and provision the physical servers to be used forrunning workloads on top. Commercial names include iDrac, iLO,iRMC, etc and most of them should support IPMI.                  Do I need to use the Metal3 with Cluster API or can I use Metal3 independently?    :          It is completely optional to use Cluster API. You can use only theBaremetal Operator and skip CAPM3 completely if all you need isbare-metal provisioning via Kubernetes API.                  What is Ironic and how does Metal3 relate to it?    :          Ironic is a bare metal provisioner, it handles provisioning of physicalmachines. Metal3 exposes a part of the Ironic functionality as aKubernetes native API via the Baremetal Operator. Ironic is not part ofMetal3 but Metal3 relies on Ironic to provision the bare metal hosts.                  What is an operator?    :          An Operator is a method of packaging, deploying and managing aKubernetes application. A Kubernetes application is an application thatis both deployed on Kubernetes and managed using the Kubernetes APIs andkubectl tooling. You can think of Operators as the runtime that managesthis type of application on Kubernetes. If you want to learn more aboutOperators you can check the Operator framework websitehttps://operatorframework. io/what/                  What is cleaning? Can I disable it?    :          Cleaning removes partitioning information from the disks to avoidconflicts with the new operating system. Seeautomated cleaning for details.                  What is inspection? Can I disable it?    :          Inspection is used to populate hardware information in the BareMetalHostobjects. You can disable it,but you may need to populate this information yourself. Do not blindlydisable inspection if it fails - chances are high the subsequentoperations fail the same way.                  What is iPXE?    :          The iPXE project develops firmware for booting machines over thenetwork. It’s a more feature-rich alternative to the well known PXE andcan be used as an add-on on top of PXE.                  What is virtual media?    :          Virtual media is a technology that allows booting an ISO on a remotemachine without resorting to network boot (e. g. PXE).                  Why use Ironic?    :          Ironic is an established service with a long history of production usageand good support for industry standards. By using it, Metal3 canconcentrate on providing the best integration with Kubernetes.            &lt;script&gt;  ! function(t, e, a) {     use strict ;    var r = {};    var i =  mk-faqs ,      l = i +  __question ,      n = i +  __answer ,      o =  [data-aria-faq-heading] ,      d =  [data-aria-faq-panel] ,      c = 0;    r. create = function() {      var t, a, s, u, A, g, h =  none ,        b = e. querySelectorAll( [data-aria-faq] );      for (c += 1, g = 0; g &lt; b. length; g++) {        var f;        if ((t = b[g]). hasAttribute( id ) || (t. id =  acc_  + c +  -  + g), t. classList. add(i), e. querySelectorAll( #  + t. id +  &gt; li ). length ? (a = e. querySelectorAll( #  + t. id +   li &gt;   + d), s = e. querySelectorAll( #  + t. id +   li &gt;   + o)) : (a = e. querySelectorAll( #  + t. id +   &gt;   + d), s = e. querySelectorAll( #  + t. id +   &gt;   + o)), t. hasAttribute( data-default ) &amp;&amp; (h = t. getAttribute( data-default )), A = t. hasAttribute( data-constant ), t. hasAttribute( data-multi ), t. hasAttribute( data-transition )) {          var y = t. querySelectorAll(d);          for (f = 0; f &lt; y. length; f++) y[f]. classList. add(n +  --transition )        }        for (r. setupPanels(t. id, a, h, A), r. setupHeadingButton(s, A), u = e. querySelectorAll( #  + t. id +  &gt; li ). length ? e. querySelectorAll( #  + t. id +   li &gt;   + o +   .   + l) : e. querySelectorAll( #  + t. id +   &gt;   + o +   .   + l), f = 0; f &lt; u. length; f++) u[f]. addEventListener( click , r. actions), u[f]. addEventListener( keydown , r. keytrolls)      }    }, r. setupPanels = function(t, e, a, r) {      var i, l, o, d, c;      for (i = 0; i &lt; e. length; i++) o = t +  _panel_  + (i + 1), d = a, c = r, (l = e[i]). setAttribute( id , o), s(e[0], !0), l. classList. add(n),  none  !== d &amp;&amp; NaN !== parseInt(d) &amp;&amp; (d &lt;= 1 ? s(e[0], !1) : d - 1 &gt;= e. length ? s(e[e. length - 1], !1) : s(e[d - 1], !1)), (c &amp;&amp;  none  === d || NaN === parseInt(d)) &amp;&amp; s(e[0], !1)    }, r. setupHeadingButton = function(t, a) {      var r, i, n, o, d, c;      for (c = 0; c &lt; t. length; c++) i = (r = t[c]). nextElementSibling. id, n = e. getElementById(i). getAttribute( aria-hidden ), o = e. createElement( button ), d = r. textContent, r. innerHTML =   , o. setAttribute( type ,  button ), o. setAttribute( aria-controls , i), o. setAttribute( id , i +  _question ), o. classList. add(l),  false  === n ? (u(o, !0), g(o, !0), a &amp;&amp; o. setAttribute( aria-disabled ,  true )) : (u(o, !1), g(o, !1)), r. appendChild(o), o. appendChild(e. createTextNode(d))    }, r. actions = function(t) {      var a, i = this. id. replace(/_panel. *$/g,   ),        n = e. getElementById(this. getAttribute( aria-controls ));      a = e. querySelectorAll( #  + i +  &gt; li ). length ? e. querySelectorAll( #  + i +   li &gt;   + o +   .   + l) : e. querySelectorAll( #  + i +   &gt;   + o +   .   + l), t. preventDefault(), r. togglePanel(t, i, n, a)    }, r. togglePanel = function(t, a, r, i) {      var l, n, o = t. target;      if ( true  !== o. getAttribute( aria-disabled ) &amp;&amp; (l = o. getAttribute( aria-controls ), g(o,  true ),  true  === o. getAttribute( aria-expanded ) ? (u(o,  false ), s(r,  true )) : (u(o,  true ), s(r,  false ), e. getElementById(a). hasAttribute( data-constant ) &amp;&amp; A(o,  true )), e. getElementById(a). hasAttribute( data-constant ) || !e. getElementById(a). hasAttribute( data-multi )))        for (n = 0; n &lt; i. length; n++) o !== i[n] &amp;&amp; (g(i[n],  false ), l = i[n]. getAttribute( aria-controls ), A(i[n],  false ), u(i[n],  false ), s(e. getElementById(l),  true ))    }, r. keytrolls = function(t) {      if (t. target. classList. contains(l)) {        var a, r = t. keyCode || t. which,          i = this. id. replace(/_panel. *$/g,   );        switch (a = e. querySelectorAll( #  + i +  &gt; li ). length ? e. querySelectorAll( #  + i +   li &gt;   + o +   .   + l) : e. querySelectorAll( #  + i +   &gt;   + o +   .   + l), r) {          case 35:            t. preventDefault(), a[a. length - 1]. focus();            break;          case 36:            t. preventDefault(), a[0]. focus()        }      }    }, r. init = function() {      r. create()    };    var s = function(t, e) {        t. setAttribute( aria-hidden , e)      },      u = function(t, e) {        t. setAttribute( aria-expanded , e)      },      A = function(t, e) {        t. setAttribute( aria-disabled , e)      },      g = function(t, e) {        t. setAttribute( data-current , e)      };    r. init()  }(window, document);  &lt;/script&gt;"
    }, , {
    "id": 32,
    "url": "/blog/",
    "title": "Blog",
    "author" : "",
    "tags" : "",
    "body": " -     Blog    Read about the newest updates in the community.     &lt;/section&gt;                   Deploying OCI Container Images to Bare Metal with a Custom IPA Hardware Manager:         Sunday, 1/02/2026        By Serhii Ivanov        What if you could deploy any OCI container image directly to bare metal, without building traditional disk images? Back in 2021, Dmitry Tantsur implemented custom deploy steps for Ironic, enabling alternative deployment methods beyond the standard image-based approach. This feature powers OpenShift’s bare metal provisioning with CoreOS, yet it remains. . .         Read More                    Metal3. io Becomes a CNCF Incubating Project:         Wednesday, 27/08/2025        By Honza Pokorný        We are pleased to share some incredible news with our community! The CNCF Technical Oversight Committee has officially voted to accept Metal3 as an incubating project. This milestone represents years of hard work, collaboration, and innovation, and we couldn’t be more excited about what lies ahead! Our Journey from Sandbox. . .         Read More                    Introducing Baremetal Operator end-to-end test suite:         Friday, 13/12/2024        By Lennart Jern        In the beginning, there was metal3-dev-env. It could set up a virtualized “baremetal” lab and test all the components together. As Metal3 matured, it grew in complexity and capabilities, with release branches, API versions, etc. Metal3-dev-env did everything from cloning the repositories and building the container images, to deploying the. . .         Read More                    Scaling Kubernetes with Metal3: Simulating 1000 Clusters with Fake Ironic Agents:         Thursday, 24/10/2024        By Huy Mai        If you’ve ever tried scaling out Kubernetes clusters in a bare-metal environment, you’ll know that large-scale testing comes with serious challenges. Most of us don’t have access to enough physical servers—or even virtual machines—to simulate the kinds of large-scale environments we need for stress testing, especially when deploying hundreds or. . .         Read More                    Scaling to 1000 clusters - Part 3:         Thursday, 30/05/2024        By Lennart Jern        In part 1, we introduced the Bare Metal Operator test mode and saw how it can be used to play with BareMetalHosts without Ironic and without any actual hosts. We continued in part 2 with how to fake workload clusters enough for convincing Cluster API’s controllers that they are healthy. . . .         Read More                                             1                            2                            3                            4                            5                            6                                                                                              Categories:             hybrid        cloud        metal3        baremetal        stack        edge        openstack        ironic        openshift        kubernetes        OpenStack        operator        summit        kubecon        shiftdev        metal3-dev-env        documentation        development        talk        conference        meetup        cluster API        provider        raw image        image streaming        IPAM        ip address manager        Pivoting        Move        scaling        cncf        community        announcement        IPA        OCI        deployment        bare metal      "
    }, {
    "id": 33,
    "url": "/privacy-statement.html",
    "title": "Privacy Statement",
    "author" : "",
    "tags" : "",
    "body": " -   Privacy Statement&lt;/section&gt;           Privacy Statement for the Metal3 Project: As Metal3. io and most of the infrastructure of the Metal3 Project arecurrently hosted by Red Hat Inc. , this site falls under theRed Hat Privacy Policy. All terms of that privacy policy apply to this site. Should we changeour hosting in the future, this Privacy Policy will be updated. How to Contact Us: If you have any questions about any of these practices or Metal3’s useof your personal information, please feel free to contactus or file anIssuein our GitHub repo. Metal3 will work with you to resolve any concerns you may have aboutthis Statement. Changes to this Privacy Statement: Metal3 reserves the right to change this policy from time to time. If wedo make changes, the revised Privacy Statement will be posted on thissite. A notice will be posted on our blog and/or mailing lists wheneverthis privacy statement is changed in a material way. This Privacy Statement was last amended on September 25, 2019. "
    }, , , , {
    "id": 34,
    "url": "/AGENTS.html",
    "title": "Metal3 Website - AI Agent Instructions",
    "author" : "",
    "tags" : "",
    "body": " - Metal3 Website - AI Agent InstructionsInstructions for AI coding agents. For content guidelines, see GUIDELINES. md. Overview: Public-facing website for Metal3 at https://metal3. io. Built withJekyll static site generator, hosted on GitHub Pages. Repository Structure:       Directory   Purpose         _posts/   Blog posts (dated markdown files)       _layouts/   Jekyll page templates       _includes/   Reusable HTML components       _faqs/   FAQ entries       assets/   Images, CSS, JS       hack/   CI scripts (markdownlint, spellcheck, shellcheck)   Testing Standards: CI uses GitHub Actions. Run locally before PRs:       Command   Purpose         make lint   Run all linters       make serve   Serve locally on port 4000   Pre-commit Hooks: This repository uses pre-commit for automated checks. Config in . pre-commit-config. yaml.       Command   Purpose         . /hack/pre-commit. sh   Run pre-commit in container       pre-commit install   Install hooks locally       pre-commit run --all-files   Run all hooks manually   Hooks include: prettier (CSS/JS/JSON), black (Python), trailing whitespace,YAML/JSON validation, and merge conflict detection. Code Conventions:  Markdown: Config in . markdownlint-cli2. yaml Spelling: Custom dictionary in . cspell-config. json Links: Checked by lychee (. lycheeignore for exceptions)Adding Content: New blog post: Create _posts/YYYY-MM-DD-title. md with frontmatter: ---layout: posttitle:  Post Title date: YYYY-MM-DDauthor:  Your Name ---Code Review Guidelines: When reviewing pull requests:  Visual review - Preview locally with make serve Spelling - Add technical terms to . cspell-config. json Links - No broken links Images - Place in assets/images/POST_TITLE/AI Agent Guidelines:  Run make lint before committing Update . cspell-config. json for new technical terms Follow GUIDELINES. md for content styleRelated Documentation:  Metal3 Book metal3-docs"
    }, {
    "id": 35,
    "url": "/CLAUDE.html",
    "title": "Metal3 Website - AI Agent Instructions",
    "author" : "",
    "tags" : "",
    "body": " - Metal3 Website - AI Agent InstructionsInstructions for AI coding agents. For content guidelines, see GUIDELINES. md. Overview: Public-facing website for Metal3 at https://metal3. io. Built withJekyll static site generator, hosted on GitHub Pages. Repository Structure:       Directory   Purpose         _posts/   Blog posts (dated markdown files)       _layouts/   Jekyll page templates       _includes/   Reusable HTML components       _faqs/   FAQ entries       assets/   Images, CSS, JS       hack/   CI scripts (markdownlint, spellcheck, shellcheck)   Testing Standards: CI uses GitHub Actions. Run locally before PRs:       Command   Purpose         make lint   Run all linters       make serve   Serve locally on port 4000   Pre-commit Hooks: This repository uses pre-commit for automated checks. Config in . pre-commit-config. yaml.       Command   Purpose         . /hack/pre-commit. sh   Run pre-commit in container       pre-commit install   Install hooks locally       pre-commit run --all-files   Run all hooks manually   Hooks include: prettier (CSS/JS/JSON), black (Python), trailing whitespace,YAML/JSON validation, and merge conflict detection. Code Conventions:  Markdown: Config in . markdownlint-cli2. yaml Spelling: Custom dictionary in . cspell-config. json Links: Checked by lychee (. lycheeignore for exceptions)Adding Content: New blog post: Create _posts/YYYY-MM-DD-title. md with frontmatter: ---layout: posttitle:  Post Title date: YYYY-MM-DDauthor:  Your Name ---Code Review Guidelines: When reviewing pull requests:  Visual review - Preview locally with make serve Spelling - Add technical terms to . cspell-config. json Links - No broken links Images - Place in assets/images/POST_TITLE/AI Agent Guidelines:  Run make lint before committing Update . cspell-config. json for new technical terms Follow GUIDELINES. md for content styleRelated Documentation:  Metal3 Book metal3-docs"
    }, , , {
    "id": 36,
    "url": "/blog/page2/",
    "title": "Blog",
    "author" : "",
    "tags" : "",
    "body": " -     Blog    Read about the newest updates in the community.     &lt;/section&gt;       {% for post in paginator. posts %}            {{ post. title }}:         {{ post. date | date:  %A, %-d/%m/%Y  }}        By {{ post. author }}        {{ post. content | strip_html | truncatewords: 50 }}         Read More        {% endfor %}     {% if paginator. total_pages &gt; 1 %}           {% if paginator. previous_page %}                                                        {% endif %}      {% for page in (1. . paginator. total_pages) %}       {% if page == paginator. page %}        {{ page }}       {% elsif page == 1 %}        {{ page }}       {% else %}        {{ page }}       {% endif %}      {% endfor %}      {% if paginator. next_page %}                                                     {% endif %}          {% endif %}    {% include blog-aside. html %}"
    }, {
    "id": 37,
    "url": "/blog/page3/",
    "title": "Blog",
    "author" : "",
    "tags" : "",
    "body": " -     Blog    Read about the newest updates in the community.     &lt;/section&gt;       {% for post in paginator. posts %}            {{ post. title }}:         {{ post. date | date:  %A, %-d/%m/%Y  }}        By {{ post. author }}        {{ post. content | strip_html | truncatewords: 50 }}         Read More        {% endfor %}     {% if paginator. total_pages &gt; 1 %}           {% if paginator. previous_page %}                                                        {% endif %}      {% for page in (1. . paginator. total_pages) %}       {% if page == paginator. page %}        {{ page }}       {% elsif page == 1 %}        {{ page }}       {% else %}        {{ page }}       {% endif %}      {% endfor %}      {% if paginator. next_page %}                                                     {% endif %}          {% endif %}    {% include blog-aside. html %}"
    }, {
    "id": 38,
    "url": "/blog/page4/",
    "title": "Blog",
    "author" : "",
    "tags" : "",
    "body": " -     Blog    Read about the newest updates in the community.     &lt;/section&gt;       {% for post in paginator. posts %}            {{ post. title }}:         {{ post. date | date:  %A, %-d/%m/%Y  }}        By {{ post. author }}        {{ post. content | strip_html | truncatewords: 50 }}         Read More        {% endfor %}     {% if paginator. total_pages &gt; 1 %}           {% if paginator. previous_page %}                                                        {% endif %}      {% for page in (1. . paginator. total_pages) %}       {% if page == paginator. page %}        {{ page }}       {% elsif page == 1 %}        {{ page }}       {% else %}        {{ page }}       {% endif %}      {% endfor %}      {% if paginator. next_page %}                                                     {% endif %}          {% endif %}    {% include blog-aside. html %}"
    }, {
    "id": 39,
    "url": "/blog/page5/",
    "title": "Blog",
    "author" : "",
    "tags" : "",
    "body": " -     Blog    Read about the newest updates in the community.     &lt;/section&gt;       {% for post in paginator. posts %}            {{ post. title }}:         {{ post. date | date:  %A, %-d/%m/%Y  }}        By {{ post. author }}        {{ post. content | strip_html | truncatewords: 50 }}         Read More        {% endfor %}     {% if paginator. total_pages &gt; 1 %}           {% if paginator. previous_page %}                                                        {% endif %}      {% for page in (1. . paginator. total_pages) %}       {% if page == paginator. page %}        {{ page }}       {% elsif page == 1 %}        {{ page }}       {% else %}        {{ page }}       {% endif %}      {% endfor %}      {% if paginator. next_page %}                                                     {% endif %}          {% endif %}    {% include blog-aside. html %}"
    }, {
    "id": 40,
    "url": "/blog/page6/",
    "title": "Blog",
    "author" : "",
    "tags" : "",
    "body": " -     Blog    Read about the newest updates in the community.     &lt;/section&gt;       {% for post in paginator. posts %}            {{ post. title }}:         {{ post. date | date:  %A, %-d/%m/%Y  }}        By {{ post. author }}        {{ post. content | strip_html | truncatewords: 50 }}         Read More        {% endfor %}     {% if paginator. total_pages &gt; 1 %}           {% if paginator. previous_page %}                                                        {% endif %}      {% for page in (1. . paginator. total_pages) %}       {% if page == paginator. page %}        {{ page }}       {% elsif page == 1 %}        {{ page }}       {% else %}        {{ page }}       {% endif %}      {% endfor %}      {% if paginator. next_page %}                                                     {% endif %}          {% endif %}    {% include blog-aside. html %}"
    }, , ];

var idx = lunr(function () {
    this.ref('id')
    this.field('title', { boost: 2 })
    this.field('body')
    this.field('author')
    this.field('url')
    this.field('tags', { boost: 2 })
    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results </p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}

function getQueryVariable(variable) {
  var query = window.location.search.substring(1);
  var vars = query.split('&');

  for (var i = 0; i < vars.length; i++) {
    var pair = vars[i].split('=');

    if (pair[0] === variable) {
      return decodeURIComponent(pair[1].replace(/\+/g, '%20'));
    }
  }
}


var searchTerm = getQueryVariable('query');
if (searchTerm) {
  lunr_search(searchTerm)
}

</script>
<style>
    #lunrsearchresults {padding-top: 0.2rem;}
    .lunrsearchresult {padding-bottom: 1rem;}
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>
</main>
<footer class="mk-main-footer">
  <div>
    <div class="mk-cncf-footer">
      <p>We are a <a href="https://cncf.io/">Cloud Native Computing Foundation</a> sandbox project.</p>
      <p><img id= "cncf-image" src="/assets/images/cncf-color.png"/></p>
      <p>Copyright 2026 The Metal³ Contributors - <a href="/privacy-statement.html">Privacy Statement</a></p>
      <p>Copyright 2026 The Linux Foundation. All Rights Reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a> page.</p>
    </div>
      <div class="mk-icons-footer">
        <p>
<!--           <a href="https://twitter.com/metal3_io" aria-label="Visit us on Twitter">
            <i class="fab fa-twitter fa-lg"></i>
          </a> -->
          <a href="https://kubernetes.slack.com/messages/CHD49TLE7" data-placement="top" title="Join our Slack channel">
            <i class="fab fa-slack fa-lg"></i>
          </a>
          <a href="https://github.com/metal3-io" aria-label="View our repo on GitHub">
            <i class="fab fa-github fa-lg"></i>
          </a>
          <a href="https://groups.google.com/g/metal3-dev" aria-label="Send us an email">
            <i class="fas fa-envelope fa-lg"></i>
          </a>
          <a href="https://www.youtube.com/channel/UC_xneeYbo-Dl4g-U78xW15g/videos" aria-label="See our YouTube channel">
            <i class="fab fa-youtube fa-lg"></i>
          </a>
        </p>
      </div>
  </div>
</footer>
</div><!--wrapper-->
<script>
var toggle = document.querySelector('#toggle');
var menu = document.querySelector('#main_nav');
var menuItems = document.querySelectorAll('#main_nav li a');

toggle.addEventListener('click', function(){
if (menu.classList.contains('is-active')) {
  this.setAttribute('aria-expanded', 'false');
  menu.classList.remove('is-active');
} else {
  menu.classList.add('is-active');
  this.setAttribute('aria-expanded', 'true');
  //menuItems[0].focus();
}
});
</script>
    <script src="/assets/js/copy.js"></script>
    <!-- This comes from DTM/DPAL and must be latest entry in body-->
<script>
  let pageLocation = window.location.href
  let footerIcons = document.querySelector(".mk-icons-footer")

  if(pageLocation.includes("community-resources.html")){
    footerIcons.style.display = "none"
  }else {null}

</script>
    <script type="text/javascript">
        if (("undefined" !== typeof _satellite) && ("function" === typeof _satellite.pageBottom)) {
            _satellite.pageBottom();
        }
    </script>
</body>
</html>

