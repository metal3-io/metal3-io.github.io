<!doctype html>
<html class="no-js" lang="en">

<head>
    <script id="dpal" src="//www.redhat.com/ma/dpal.js" type="text/javascript"></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="/assets/css/styles.css">
    <link rel="shortcut icon" type="image/png" href="/favicon.png">
    <meta name="theme-color" content="#008585">
    
    <title>Metal³ - Metal Kubed</title>
    <!-- # Opengraph protocol properties: https://ogp.me/ -->
    <meta name="author" content="The Metal³ - Metal Kubed website team, " >
    
    <meta name="twitter:card" content="summary">
    <meta name="description" content="Metal3.io aims to build on baremetal host provisioning technologies to provide a Kubernetes native API for managing bare metal hosts via a provisioning stack that is also running on Kubernetes.">
    <meta name="keywords" content="hybrid, cloud, metal3, baremetal, stack, edge, openstack, ironic, openshift, kubernetes, openstack, operator, summit, kubecon, shiftdev, metal3-dev-env, documentation, development, talk, conference, meetup, cluster-api, provider, raw-image, image-streaming, ipam, ip-address-manager, pivoting, move, scaling, cncf, community, announcement, " >
    <meta property="og:title" content="Metal³ - Metal Kubed">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://metal3.io/search.html" >
    <meta property="og:image" content="https://metal3.io/assets/images/metal3logo.png">
    <meta property="og:description" content="Metal3.io aims to build on baremetal host provisioning technologies to provide a Kubernetes native API for managing bare metal hosts via a provisioning stack that is also running on Kubernetes." >
    <meta property="og:site_name" content="Metal³ - Metal Kubed" >
    <meta property="og:article:author" content="The Metal³ - Metal Kubed website team" >
    <meta property="og:article:published_time" content="2025-10-31 19:27:28 -0500" >
    <meta name="twitter:title" content="Metal³ - Metal Kubed">
    <meta name="twitter:description" content="Metal3.io aims to build on baremetal host provisioning technologies to provide a Kubernetes native API for managing bare metal hosts via a provisioning stack that is also running on Kubernetes.">

    <link type="application/atom+xml" rel="alternate" href="https://metal3.io/feed.xml" title="Metal³ - Metal Kubed" />
    <meta name="google-site-verification" content="HCdbGknTOCTKQVt7m-VxTG4BEYXxSqm-sDb-iklqrB0" />
  <link href="https://fonts.googleapis.com/css?family=Nunito:200,400&display=swap" rel="stylesheet">
  <script defer src="https://use.fontawesome.com/releases/v5.1.0/js/all.js" integrity="sha384-3LK/3kTpDE/Pkp8gTNp2gR/2gOiwQ6QaO7Td0zV76UFJVhqLl4Vl3KL1We6q6wR9" crossorigin="anonymous"></script>
  <!-- Photoswipe.com gallery-->

  <!-- Core CSS file -->
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe.min.css">

  <!-- Skin CSS file (styling of UI - buttons, caption, etc.)
      In the folder of skin CSS file there are also:
      - .png and .svg icons sprite,
      - preloader.gif (for browsers that do not support CSS animations) -->
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/default-skin/default-skin.min.css">
</head>
<body>
    <!--[if IE]>
      <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
    <![endif]-->

<div class="mk-wrapper">
    <section class="mk-masthead mk-masthead--sub">
<header class="mk-main-header">
    <a href="/" class="mk-main-header__brand">
        <svg version="1.1" viewBox="0 0 557 540" xmlns="http://www.w3.org/2000/svg">
          <g fill="none" fill-rule="evenodd">
            <g transform="translate(-1)" fill-rule="nonzero">
            <path d="m181.91 539.68h-0.7c-0.76 0-1.44-0.11-2-0.17h-0.14l-1.62-0.2c-15.204-1.867-29.364-8.7129-40.27-19.47l-1.07-1.06-49.46-61.26-73.34-90.59-0.5-0.72c-2.8927-4.0899-5.2989-8.503-7.17-13.15-1.0257-2.532-1.8875-5.1274-2.58-7.77v-0.11c-0.22-0.85-0.43-1.69-0.62-2.56v-0.14c-0.8042-3.5966-1.2861-7.2578-1.44-10.94v-0.47-0.48c-0.067687-4.136 0.26722-8.2688 1-12.34l0.11-0.61 14.51-63.64 28.72-126c4.0017-17.442 15.802-32.074 32-39.68l178.2-85.93 3.34-0.67c5.6926-1.1418 11.484-1.7201 17.29-1.7201h2.83 0.57c8.4518-0.016309 16.808 1.7879 24.5 5.2901l0.47 0.22 175.82 84.2 0.48 0.25c7.1101 3.7526 13.491 8.7481 18.84 14.75 2.7886 3.1018 5.2639 6.4715 7.39 10.06l0.17 0.29c2.1776 3.7964 3.9314 7.8205 5.23 12l0.3 1 44.23 190.25 0.17 1.42c2.0399 16.443-2.1677 33.052-11.79 46.54l-0.48 0.68-121.46 150.24c-7.2792 9.604-17.475 16.59-29.06 19.91-0.93 0.27-1.87 0.52-2.81 0.75l-0.3 0.07c-5.0328 1.1701-10.183 1.76-15.35 1.76h-194.01z" fill="#fff"/>
            <path d="m492 131.65c-0.75221-2.3458-1.7582-4.6025-3-6.73-1.2507-2.1148-2.7114-4.0982-4.36-5.92-3.3032-3.7145-7.2456-6.8067-11.64-9.13l-179.82-86c-4.3569-1.9816-9.0938-2.9883-13.88-2.95h-0.77c-4.9428-0.19294-9.8909 0.20318-14.74 1.18l-179.72 86.6c-8.9642 4.1124-15.498 12.17-17.67 21.79l-3.69 16.16 216.29 117.67 0.34-0.18 217.22-112.72-4.56-19.77z" fill="#00E0C1"/>
            <path d="m279 264.32l-216.29-117.67-25.77 113.1-14.73 64.63c-0.44744 2.4671-0.64178 4.9734-0.58 7.48v0.29c0.072639 2.1493 0.33702 4.2878 0.79 6.39 0.12 0.56 0.26 1.1 0.4 1.65 0.41228 1.5719 0.92671 3.1151 1.54 4.62 1.1152 2.7748 2.5517 5.4095 4.28 7.85l23.69 29.27 51 63 49.67 61.55c6.7982 6.7311 15.643 11.009 25.14 12.16 0.67 0 1.31 0.18 2 0.21h99.17v-254.34l-0.31-0.19z" fill="#00EEC4"/>
            <path d="m536.75 324.38l-40.19-173-217.23 112.76v254.71h98.82c3.2616 0.017 6.5139-0.34884 9.69-1.0906 0.62-0.15 1.23-0.31 1.84-0.49 6.2438-1.7629 11.72-5.5604 15.56-10.79l66.09-81.75 31.31-38.73 26.94-33.33c5.8432-8.2018 8.4013-18.295 7.17-28.29z" fill="#00D1BD"/>
            <path d="m120.94 369l137 75.89c1.3702 0.76284 3.0421 0.74251 4.3933-0.05344s2.1796-2.2483 2.1767-3.8166v-161.02c0-5.718-3.1489-10.971-8.19-13.67l-1.64-0.87-134.68-71.99c-0.8041-0.43178-1.7757-0.41032-2.56 0.056543-0.78426 0.46687-1.2663 1.3108-1.27 2.2235l-0.94 163.17c0.02 3.63 2.77 8.44 5.71 10.08z" fill="#fff"/>
            <path d="m282.61 103.85c-4.0372-0.033083-8.0333 0.81323-11.71 2.481l-134.2 60.47c-0.91184 0.40637-1.512 1.2973-1.5476 2.295-0.032512 0.99771 0.50554 1.9274 1.3876 2.395l135.72 72.51c0.15 0.09 0.31 0.16 0.47 0.24l0.59 0.29 0.26 0.11 0.8 0.34h0.09c4.9879 1.8704 10.539 1.5061 15.24-1l139.14-73.94c1.1079-0.5822 1.7814-1.7504 1.7328-3.0009-0.054039-1.2505-0.82096-2.3596-1.9728-2.8491l-135.06-58.05c-3.4545-1.4945-7.1761-2.2735-10.94-2.291z" fill="#fff"/>
            <path d="m442.82 192.61c-1.08-0.49333-2.4133-0.29667-4 0.59l-24.52 13.54c-3.6117 1.9922-6.3845 5.2194-7.81 9.09l-37.49 87.55-37.31-46.2c-1.59-2.29-4.2-2.45-7.81-0.45l-24.51 13.54c-1.6358 0.9266-3.0116 2.2508-4 3.85-1.0039 1.4454-1.5667 3.151-1.62 4.91v166.83c0 1.59 0.55 2.59 1.63 3s2.42 0.19 4-0.69l27.34-15.1c1.6143-0.90735 2.9864-2.1902 4-3.74 1.0178-1.3976 1.5863-3.0717 1.63-4.8v-105l23.21 30.12c2.1667 2.1267 4.6967 2.3933 7.59 0.8l11.67-6.45c3.18-1.7467 5.71-4.8067 7.59-9.18l23.43-55.9 0.25 97.1 0.17 8.31c-0.10795 1.217 0.57186 2.3675 1.69 2.86 1.2747 0.44018 2.6836 0.23517 3.78-0.55l27.27-15.83c1.5869-0.9387 2.9245-2.2455 3.9-3.81 0.9881-1.4207 1.5184-3.1095 1.5212-4.84v-166.43c0.028815-1.6-0.51118-2.64-1.6012-3.12z" fill="#fff"/>
            </g>
          </g>
        </svg>
      </a>
      <div role="navigation" class="mk-main-header__nav-wrapper">
        <button class="mk-main-header__toggle" id="toggle" aria-controls="main_nav" aria-expanded="false" aria-label="navigation toggle" >
          <svg version="1.1" viewBox="0 0 512 448" xmlns="http://www.w3.org/2000/svg">
          <g>
          <path d="m296 0h192c13.255 0 24 10.745 24 24v160c0 13.255-10.745 24-24 24h-192c-13.255 0-24-10.745-24-24v-160c0-13.255 10.745-24 24-24zm-80 0h-192c-13.255 0-24 10.745-24 24v160c0 13.255 10.745 24 24 24h192c13.255 0 24-10.745 24-24v-160c0-13.255-10.745-24-24-24zm-216 264v160c0 13.255 10.745 24 24 24h192c13.255 0 24-10.745 24-24v-160c0-13.255-10.745-24-24-24h-192c-13.255 0-24 10.745-24 24zm296 184h192c13.255 0 24-10.745 24-24v-160c0-13.255-10.745-24-24-24h-192c-13.255 0-24 10.745-24 24v160c0 13.255 10.745 24 24 24z"/>
          </g>
          </svg>
          <span class="mk-main-header__toggle-text">menu</span>
        </button>
        </div>
        <ul id="main_nav" class="mk-main-nav">
          <li ><a class="mk-main-nav__item" href="/blog/index.html">Blog</a></li>
          <li ><a class="mk-main-nav__item" href="/community-resources.html">Community Resources</a></li>
          <li ><a class="mk-main-nav__item" href="https://book.metal3.io">Documentation</a></li>
          <li ><a class="mk-main-nav__item" href="/contribute.html">Contribute</a></li>
          <li ><a class="mk-main-nav__item" href="/faqs.html">FAQs</a></li>
          <li ><a class="mk-main-nav__item" href="https://book.metal3.io/developer_environment/tryit">Try It!</a></li>
          <li  class="active"  id="mk-main-nav__search">
            <form action="/search.html" method="get" autocomplete="off" class="mk-search-form">
              <div class="autocomplete" style="width:150px;">
                <input type="text" id="search-input" class="docs-search--input" placeholder="Search Term" name="query">
              </div>
              <button type="submit" id = "search-button" class = "search-button" disabled = 'true' >
                <img src="/assets/images/search.png" style="height: 20px;" alt="">
              </button>
                <div id="mode-toggle">
                  <img src="/assets/images/moon-outline.png" id="mode-icon" style="height: 20px; margin-left: 10px;"/>
                </div>
            </form>
          </li>

        </ul>
  </header>
  
<script>
function autocomplete(inp, arr) {
  /*the autocomplete function takes two arguments,
  the text field element and an array of possible autocompleted values:*/
  var currentFocus;
  /*execute a function when someone writes in the text field:*/
  inp.addEventListener("input", function(e) {
      var a, b, i, val = this.value;
      /*close any already open lists of autocompleted values*/
      closeAllLists();
      if (!val) { return false;}
      currentFocus = -1;
      /*create a DIV element that will contain the items (values):*/
      a = document.createElement("DIV");
      a.setAttribute("id", this.id + "autocomplete-list");
      a.setAttribute("class", "autocomplete-items");
      /*append the DIV element as a child of the autocomplete container:*/
      this.parentNode.appendChild(a);
      /*for each item in the array...*/
      for (i = 0; i < arr.length; i++) {
        /*check if the item starts with the same letters as the text field value:*/
        if (arr[i].substr(0, val.length).toUpperCase() == val.toUpperCase()) {
          /*create a DIV element for each matching element:*/
          b = document.createElement("DIV");
          /*make the matching letters bold:*/
          b.innerHTML = "<strong>" + arr[i].substr(0, val.length) + "</strong>";
          b.innerHTML += arr[i].substr(val.length);
          /*insert a input field that will hold the current array item's value:*/
          b.innerHTML += "<input type='hidden' value='" + arr[i] + "'>";
          /*execute a function when someone clicks on the item value (DIV element):*/
              b.addEventListener("click", function(e) {
              /*insert the value for the autocomplete text field:*/
              inp.value = this.getElementsByTagName("input")[0].value;
              /*close the list of autocompleted values,
              (or any other open lists of autocompleted values:*/
              closeAllLists();
          });
          a.appendChild(b);
        }
      }
  });
  /*execute a function presses a key on the keyboard:*/
  inp.addEventListener("keydown", function(e) {
      document.getElementById("search-button").disabled= undefined;
      var x = document.getElementById(this.id + "autocomplete-list");
      if (x) x = x.getElementsByTagName("div");
      if (e.keyCode == 40) {
        /*If the arrow DOWN key is pressed,
        increase the currentFocus variable:*/
        currentFocus++;
        /*and and make the current item more visible:*/
        addActive(x);
      } else if (e.keyCode == 38) { //up
        /*If the arrow UP key is pressed,
        decrease the currentFocus variable:*/
        currentFocus--;
        /*and and make the current item more visible:*/
        addActive(x);
      } else if (e.keyCode == 13) {
        /*If the ENTER key is pressed, prevent the form from being submitted,*/
        if (currentFocus > -1) {
          /*and simulate a click on the "active" item:*/
          if (x) {
            x[currentFocus].click();
            e.preventDefault();
          }
        }
        if (document.getElementById("search-input").value == "") {
          e.preventDefault();
        }
      }
  });
  function addActive(x) {
    /*a function to classify an item as "active":*/
    if (!x) return false;
    /*start by removing the "active" class on all items:*/
    removeActive(x);
    if (currentFocus >= x.length) currentFocus = 0;
    if (currentFocus < 0) currentFocus = (x.length - 1);
    /*add class "autocomplete-active":*/
    x[currentFocus].classList.add("autocomplete-active");
  }
  function removeActive(x) {
    /*a function to remove the "active" class from all autocomplete items:*/
    for (var i = 0; i < x.length; i++) {
      x[i].classList.remove("autocomplete-active");
    }
  }
  function closeAllLists(elmnt) {
    /*close all autocomplete lists in the document,
    except the one passed as an argument:*/
    var x = document.getElementsByClassName("autocomplete-items");
    for (var i = 0; i < x.length; i++) {
      if (elmnt != x[i] && elmnt != inp) {
      x[i].parentNode.removeChild(x[i]);
    }
  }
}
/*execute a function when someone clicks in the document:*/
document.addEventListener("click", function (e) {
    closeAllLists(e.target);
});
}
</script>
<script>
  document.addEventListener("DOMContentLoaded", function(){
  let iconMode = document.getElementById("mode-icon")
  let toggleMode = document.getElementById("mode-toggle")
  let cncfImage = document.getElementById("cncf-image")
let isToggled = localStorage.getItem("currentMode") === "true";
updateMode();
toggleMode.addEventListener("click", () => {
  isToggled = !isToggled;
  localStorage.setItem("currentMode", isToggled);
  updateMode();
});
function updateMode() {
  let mastHead = document.querySelector(".mk-masthead");
  let h1 = document.querySelectorAll("h1")
  let h2 = document.querySelectorAll("h2")
  let h3 = document.querySelectorAll("h3")
  let li = document.querySelectorAll("li")
  let sections = document.querySelectorAll(".mk-main__section")
  let body = document.querySelector("body")
  let whyCards = document.querySelectorAll(".mk-why-baremetal__card")
  let blogCards = document.querySelectorAll(".mk-blog-meta__card")
  let questions = document.querySelectorAll(".mk-faqs__question")
  let subHeadings = document.querySelectorAll(".mk-sub-heading")
  let p = document.querySelectorAll("p")
  if (isToggled) {

    iconMode.src = "/assets/images/moon-outline.png";
    cncfImage.src = "/assets/images/cncf-white.svg";
    mastHead.style.backgroundColor = "var(--mk--BackgroundColor--500)";
    body.style.backgroundColor = "var(--mk--BackgroundColor--500)";
    body.style.color = "var(--mk--Color--200)";
    h1.forEach((eachH1)=>{
      eachH1.style.color = "var(--mk--Color--200)"
    })
    h2.forEach((eachH2)=>{
      eachH2.style.color = "var(--mk--Color--200)"
    })
    h3.forEach((eachH3)=>{
      eachH3.style.color = "var(--mk--Color--200)"
    })
    li.forEach((eachLi)=>{
      eachLi.style.color = "var(--mk--Color--200)"
    })
    sections.forEach((section)=>{
      section.style.backgroundColor = "var(--mk--BackgroundColor--500)";
    })
    p.forEach((eachP)=>{
      eachP.style.color = "var(--mk--Color--200)"
    })
    whyCards.forEach((whyCard)=>{
    whyCard.querySelector("h3").style.color = "var(--mk--BackgroundColor--150)"
      whyCard.style.backgroundColor = "var(--mk--BackgroundColor--175)"
    })
    blogCards.forEach((blogCard)=>{
      blogCard.style.backgroundColor = "var(--mk--color-brand--400)";
    })
    questions.forEach((question)=>{
      question.style.color = "var(--mk--Color--200)"
    })
    subHeadings.forEach((subHeading)=>{
      subHeading.style.color = "var(--mk--Color--500)"
    })
  } else {
    iconMode.src = "/assets/images/moon.png";
    cncfImage.src = "/assets/images/cncf-color.svg";
    mastHead.style.backgroundColor = "";
    body.style.backgroundColor = "";
    body.style.color = "var(--mk--Color--400)";


    h1.forEach((eachH1)=>{
      eachH1.style.color = ""
    })
    h2.forEach((eachH2)=>{
      eachH2.style.color = ""
    })
    h3.forEach((eachH3)=>{
      eachH3.style.color = ""
    })
    sections.forEach((section)=>{
      section.style.backgroundColor = "var(--mk--BackgroundColor--250)";
    })
    li.forEach((eachLi)=>{
      eachLi.style.color = ""
    })
    p.forEach((eachP)=>{
      eachP.style.color = ""
    })
    whyCards.forEach((whyCard)=>{
      whyCard.querySelector("h3").style.color = ""
      whyCard.style.backgroundColor = "white"
    })
    blogCards.forEach((blogCard)=>{
      blogCard.style.backgroundColor = ""
    })
    questions.forEach((question)=>{
      question.style.color = ""
    })
    subHeadings.forEach((subHeading)=>{
      subHeading.style.color = "var(--mk--Color--500)"
    })


  }
}
})
</script>
<script>
var mykeywords = ["hybrid", "cloud", "metal3", "baremetal", "stack", "edge", "openstack", "ironic", "openshift", "kubernetes", "OpenStack", "operator", "summit", "kubecon", "shiftdev", "metal3-dev-env", "documentation", "development", "talk", "conference", "meetup", "cluster API", "provider", "raw image", "image streaming", "IPAM", "ip address manager", "Pivoting", "Move", "scaling", "cncf", "community", "announcement", ]
autocomplete(document.getElementById("search-input"), mykeywords);
</script>
<script src="/assets/js/clipboard.min.js"></script>
<!-- Photoswipe -->
<!-- Core JS file -->
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe.min.js"></script>
<!-- UI JS file -->
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>

<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="/assets/js/lunr.min.js"></script>

<div class="mk-masthead__content--sub">
        <h1 class="mk-masthead__content--sub__title">Search results</h1>
</div>
</section>
<main class="mk-main mk-blog">
            <article class="mk-main__section mk-main__content mk-main__section__content">
    <div class="container post">
      <h1 class="page-title"></h1>
      <article class="post-content">
        <div id="lunrsearchresults">
            <ul></ul>
        </div>
      </article>
    </div>

</article>
<nav class="mk-pagination">
        
        
</nav>

    </main>



<script>

var documents = [{
    "id": 0,
    "url": "/blog/2025/08/27/metal3-becomes-cncf-incubating-project.html",
    "title": "Metal3.io Becomes a CNCF Incubating Project",
    "author" : "Honza Pokorný",
    "tags" : "metal3, cncf, community, announcement",
    "body": "We are pleased to share some incredible news with our community! The CNCFTechnical Oversight Committee has officially voted to accept Metal3 as anincubating project. This milestone represents years of hard work, collaboration,and innovation, and we couldn’t be more excited about what lies ahead! Our Journey from Sandbox to Incubation: What started as a collaboration between Red Hat and Ericsson in 2019 hasblossomed into something truly special. When we joined the CNCF sandbox inSeptember 2020, we knew we had something powerful: a way to make bare metalinfrastructure as Kubernetes-native as any cloud platform. Today, that visionhas grown far beyond our initial dreams. The Numbers Tell Our Story: We’re incredibly proud of what our community has accomplished together:  57 active contributing organizations from around the globe 186 amazing contributors who’ve shaped our project 8,368 merged pull requests representing countless hours of collaboration 1,523 GitHub stars from supporters worldwide 187 releases of continuous improvementBut beyond the numbers, what makes us truly happy is seeing organizations likeFujitsu, Ikea, SUSE, Ericsson, and Red Hat successfully deploying Metal3 inproduction environments. What Makes Us Proud: Metal3 has evolved into so much more than a bare metal provisioning tool. We’vebuilt a comprehensive platform that:  Seamlessly integrates with Cluster API for Kubernetes lifecycle management Provides robust IP address management through our IPAM component Offers enterprise-grade security with automated vulnerability scanning Supports firmware management and day-2 operations Runs entirely on Kubernetes using native APIsOur new Ironic Standalone Operator has revolutionized deployment simplicity,making it easier than ever for teams to get started with Metal3. Looking Forward with Excitement: The roadmap ahead fills us with anticipation! In 2025, we’re planning:  Enhanced multi-tenancy support ARM architecture support beyond x86_64 Improved DHCP-less provisioning capabilities New API revisions across our components Continued simplification of the user experienceThe Adventure Continues: Joining CNCF incubation isn’t the end of our journey – it’s an exciting newchapter! With the foundation’s support and our amazing community behind us,we’re more energized than ever to push the boundaries of what’s possible withbare metal Kubernetes infrastructure. Thank you for being part of this incredible adventure. Here’s to making baremetal as cloud-native as the clouds themselves! "
    }, {
    "id": 1,
    "url": "/blog/2024/12/13/Introducing-BMO-E2E.html",
    "title": "Introducing Baremetal Operator end-to-end test suite",
    "author" : "Lennart Jern",
    "tags" : "metal3, cluster API, provider, edge",
    "body": "In the beginning, there wasmetal3-dev-env. It could set up avirtualized “baremetal” lab and test all the components together. As Metal3matured, it grew in complexity and capabilities, with release branches, APIversions, etc. Metal3-dev-env did everything from cloning the repositories andbuilding the container images, to deploying the controllers and running tests,on top of setting up the virtual machines and the networks, of course. Needlessto say, it became hard to understand and easy to misuse. We tried reducing the scope a bit by introducing end to end tests directly inthe Cluster API providerMetal3(CAPM3). However, metal3-dev-env was still very much entangled with CAPM3. Itwas at this point that I got tired of trying to gradually fix it and took theinitiative to start from scratch with end to end tests in Baremetal Operator(BMO) instead. Up until that point, we had been testing BMO through CAPM3 and the cluster APIflow. It worked, but it was very inefficient. From the perspective on theBaremetal Operator, a test could look something like this:  Register 5 BareMetalHosts Inspect the 5 BareMetalHosts Provision the 5 BareMetalHosts all with the same image Deprovision 1 BareMetalHost Provision it again with another image Deprovision another BareMetalHost Provision it again with the other image Continue in the same way with the rest of the BareMetalHosts… Deprovision all BareMetalHostsAs you can see, it is very repetitive, constantly doing the same thing again andagain. As a consequence of this and the complexity of metal3-dev-env, it wasquite an effort to thoroughly test something related to BMO code. I wasconstantly questioning myself and the test environment. “Is it testing the codeI wrote?” “Is it doing the relevant scenario?” “Is the configuration correct?” Baremetal Operator end to end tests are born: Sometimes it is easier to start from scratch, so this is what wedid. The BaremetalOperator end to end tests started out as a small script that only set upminikube, some VMs and a baseboard management controller (BMC) emulator. Thegoal was simple: do the minimum required to simulate a baremetal lab. From this,it was quite easy to build a test module that was responsible for deploying thenecessary controllers and running some tests. Notice the separation of concerns here! The test module expects a baremetal labenvironment to be already existing and the script that sets up the environmentis not involved in anyway with the tests or deployment of the controllers. Thisdesign is deliberate, with a clear goal that the test module should be usefulacross multiple environments. It should be possible to run the test suiteagainst real baremetal labs with multiple different configurations. I am hopingthat we will get a chance next year to try it for real in a baremetal lab. How does it work?: The flexibility of the end to end module is possible through a configurationfile. It can be used to configure everything from the image URL and checksum tothe timeout limits. Since Ironic can be deployed in many different ways, it wasalso necessary to make this flexible. The user can optionally set up Ironicbefore the test, or provide a kustomization that will be applied automatically. A separate configuration file declares the BMCs that should be used in thetests. The configuration that we use inCIshows how these files look like. As a proof of concept for the flexibility ofthe tests, it can be noted that we already have two different configurations. One for running the tests with Ironic and one for running them with BMO infixture mode. The first is the “normal” mode, the latter means that BMO does notcommunicate with Ironic at all, it just pretends. While that obviously isn’tuseful for any thorough tests, it still provides a quick and light weight testsuite, and ensures that we do not get too attached to one particularconfiguration. The test suite itself is made with Ginkgo and Gomega. Instead of building a longchain of checks and scenarios we have attempted to do small, isolated tests. This makes it possible to run multiple in parallel and shorten the test suiteduration, as well as easily identify where exactly errors occur. In order toaccomplish this, we make heavy use of the statusannotation so that we can skipinspection when possible. Where are we today?: It is already several months since we switched over to the BMO e2e test suite asthe primary, and only required tests for pull requests in the BMO repository. Werun the end to end test suite as GitHubworkflowsand it covers more than the metal3-dev-env and CAPM3 based tests from BMOperspective. That does not mean that we are done though. At the time of writing,there are several GitHubissues for improving andextending the tests. The progress has significantly slowed though, as canperhaps be expected, since the most essentials parts were implemented. The future: In the future we hope to make the BMO end to end module and tooling more usefulfor local development and testing. It should be easy to spin up a minimalenvironment and test specific scenarios, also using Tilt. Additionally, we wantto “rebase” the CAPM3 end to end tests on this work. It should be possible toreuse the code and tooling for simulating a baremetal lab so that we can get ridof the entanglement with metal3-dev-env. "
    }, {
    "id": 2,
    "url": "/blog/2024/10/24/Scaling-Kubernetes-with-Metal3-on-Fake-Node.html",
    "title": "Scaling Kubernetes with Metal3: Simulating 1000 Clusters with Fake Ironic Agents",
    "author" : "Huy Mai",
    "tags" : "metal3, cluster API, ironic, baremetal, scaling",
    "body": "If you’ve ever tried scaling out Kubernetes clusters in a bare-metalenvironment, you’ll know that large-scale testing comes with serious challenges. Most of us don’t have access to enough physical servers—or even virtualmachines—to simulate the kinds of large-scale environments we need for stresstesting, especially when deploying hundreds or thousands of clusters. That’s where this experiment comes in. Using Metal3, we simulated a massive environment—provisioning 1000 single-nodeKubernetes clusters—without any actual hardware. The trick? A combination ofFake Ironic Python Agents (IPA) and Fake Kubernetes API servers. These toolsallowed us to run an entirely realistic Metal3 provisioning workflow whilesimulating thousands of nodes and clusters, all without needing a single realmachine. The motivation behind this was simple: to create a scalable testing environmentthat lets us validate Metal3’s performance, workflow, and reliability withoutneeding an expensive hardware lab or virtual machine fleet. By simulating nodesand clusters, we could push the limits of Metal3’s provisioning processcost-effectively and time-efficiently. In this post, I’ll explain exactly how it all works, from setting up multipleIronic services to faking hardware nodes and clusters and sharing the lessonslearned. Whether you’re a Metal3 user or just curious about how to testlarge-scale Kubernetes environments, it’ll surely be a good read. Let’s getstarted! Prerequisites &amp; Setup: Before diving into the fun stuff, let’s ensure we’re on the same page. You don’tneed to be a Metal3 expert to follow along, but having a bit of background willhelp! What You’ll Need to Know: Let’s start by ensuring you’re familiar with some essential tools and conceptsthat power Metal3 workflow. If you’re confident in your Metal3 skills, pleasefeel free to skip this part. A typical Metal3 Workflow: The following diagram explains a typical Metal3 workflow. We will, then, go intodetails of every component.  Cluster API (CAPI): CAPI is a project that simplifies the deployment and management of Kubernetesclusters. It provides a consistent way to create, update, and scale clustersthrough Kubernetes-native APIs. The magic of CAPI is that it abstracts away manyof the underlying details so that you can manage clusters on different platforms(cloud, bare metal, etc. ) in a unified way. Cluster API Provider Metal3 (CAPM3): CAPM3 extends CAPI to work specifically with Metal3 environments. It connectsthe dots between CAPI, BMO, and Ironic, allowing Kubernetes clusters to bedeployed on bare-metal infrastructure. It handles tasks like provisioning newnodes, registering them with Kubernetes, and scaling clusters. Bare Metal Operator (BMO): BMO is a controller that runs inside a Kubernetes cluster and works alongsideIronic to manage bare-metal infrastructure. It automates the lifecycle ofbare-metal hosts, managing things like registering new hosts, powering them onor off, and monitoring their status. Bare Metal Host (BMH)A BMH is the Kubernetes representation of a bare-metal node. It containsinformation about how to reach the node it represents, and BMO monitors itsdesired state closely. When BMO notices that a BMH object state is requested tochange (either by a human user or CAPM3), it will decide what needs to be doneand tell Ironic. Ironic &amp; Ironic Python Agent (IPA):  Ironic is a bare-metal provisioning tool that handles tasks like bootingservers, deploying bootable media (e. g. , operating systems) to disk, andconfiguring hardware. Think of Ironic as the piece of software that managesactual physical servers. In a Metal3 workflow, Ironic receives orders from BMOand translates them into actionable steps. Ironic has multiple ways to interactwith the machines, and one of them is the so-called “ agent-based direct deploy”method, which is commonly used by BMO. The agent mentioned is called IronicPython Agent (IPA), which is a piece of software that runs on each bare-metalnode and carries out Ironic’s instructions. It interacts with the hardwaredirectly, like wiping disks, configuring networks, and handling boot processes. In a typical Metal3 workflow, BMO reads the desired state of the node from theBMH object, translates the Kubernetes reconciling logic to concrete actions, andforwards them to Ironic, which, as part of the provisioning process, tells IPAthe exact steps it needs to perform to get the nodes to desired states. Duringthe first boot after node image installation, Kubernetes components will beinstalled on the nodes by cloud-init, and once the process succeeds, Ironicand IPA finish the provisioning process, and CAPI and CAPM3 will verify thehealth of the newly provisioned Kubernetes cluster(s). The Experiment: Simulating 1000 Kubernetes Clusters: This experiment aimed to push Metal3 to simulate 1000 single-node Kubernetesclusters on fake hardware. Instead of provisioning real machines, we used FakeIronic Python Agents (Fake IP) and Fake Kubernetes API Servers (FKAS) tosimulate nodes and control planes, respectively. This setup allowed us to test amassive environment without the need for physical infrastructure. Since our goal is to verify the Metal3 limit, our setup will let all the Metal3components (except for IPA, which runs inside and will be scaled with the nodes)to keep working as they do in a typical workflow. In fact, none of thecomponents should be aware that they are running with fake hardware. Take the figure we had earlier as a base, here is the revised workflow with fakenodes.  Step 1: Setting Up the environment: As you may have known, a typical Metal3 workflow requires several components:bootstrap Kubernetes cluster, possible external networks, bare-metal nodes, etc. As we are working on simulating the environment, we will start with a newlyspawned Ubuntu VM, create a cluster with minikube, add networks with libvirt,and so on (If you’re familiar with Metal3’s dev-env, this step is similar towhat script01,02and a part of03do). We will not discuss this part, but you can find the related setup fromthisscriptif interested. Note: If you intend to follow along, note that going to 1000 nodes requiresa large environment and will take a long time. In our setup, we had a VM with 24cores and 32GB of RAM, of which we assigned 14 cores and 20GB of RAM to theminikube VM, and the process took roughly 48 hours. If your environment is lesspowerful, consider reducing the nodes you want to provision. Something like 100nodes will require minimal resources and time while still being impressive. Step 2: Install BMO and Ironic: In Metal3’s typical workflow, we usually rely on Kustomize to install Ironic andBMO. Kustomize helps us define configurations for Kubernetes resources, makingit easier to customize and deploy services. However, our current Kustomizeoverlay for Metal3 configures only a single Ironic instance. This setup workswell for smaller environments, but it becomes a bottleneck when scaling up andhandling thousands of nodes. That’s where Ironic’s special mode comes into play. Ironic has the abilityto run multiple Ironic conductors while sharing the same database. The bestpart? Workload balancing between conductors happens automatically, which meansthat no matter which Ironic conductor receives a request, the load is evenlydistributed across all conductors, ensuring efficient provisioning. Achievingthis requires separating ironic conductor from the database, which allows usto scale up the conductor part. Each conductor will have its ownPROVISIONING_IP, hence the need to have a specialized configMap. We used Helm for this purpose. In our Helm chart, theIronic conductor container and HTTP server (httpd) container areseparated into a new pod, and the rest of the ironic package (mostlyMariaDB-ironic database) stays in another pod. A list of PROVISIONING_IPs isprovided by the chart’s values. yaml, and for each IP, an ironic conductorpod is created, along with a config map whose values are rendered with the IP’svalue. This way, we can dynamically scale up/down ironic (or, more specifically,ironic conductors) by simply adding/removing ips. Another piece of information that we need to keep in mind is the ipa-downloadercontainer. In our current metal3-dev-env, the IPA-downloader container runs asan init Container for ironic, and its job is to download the IPA image to aPersistent Volume. This image contains the Ironic Python Agent, and it isassumed to exist by Ironic. For the multiple-conductor scenario, running thesame init-container for all the conductors, at the same time, could be slowand/or fail due to network issue. To make it work, we made a small “hack” in thechart: the ipa image will exist in a specific location inside the minikube host,and all the conductor pods will mount to that same location. In production, amore throughout solution might be to keep the IPA-downloader as aninit-container, but points the image to the local image server, which we set upin the previous step. BMO, on the other hand, still works well with kustomize, as we do not need toscale it. As with typical metal3 workflow, BMO and Ironic must share someauthentication to work with TLS. You can check out the full Ironic helm charthere. Step 3: Creating Fake Nodes with Fake Ironic Python Agents: As we mentioned at the beginning, instead of using real hardware, we will use anew tool called Fake Ironic Python Agent, or Fake IPA to simulate thenodes. Setting up Fake IPA is relatively straightforward, as Fake IPA runs ascontainers on the host machine, but first, we need to create the list of “nodes”that we will use (Fake IPA requires to have that list ready when it starts). A“node” typically looks like this {    uuid : $uuid,    name : $node_name,    power_state :  Off ,    external_notifier :  True ,    nics : [    { mac : $macaddr,  ip :  192. 168. 0. 100 }   ],}All of the variables (uuid, node_name, macaddress) can be dynamicallygenerated in any way you want (check thisscriptout if you need an idea). Still, we must store this information to generate theBMH objects that match those “nodes. ” The ip is, on the other hand, notessential. It could be anything. We must also start up the sushy-tools container in this step. It is a toolthat simulates the Baseboard ManagementControllerfor non-bare-metal hardware, and we have been using it extensively inside Metal3dev-env and CI to control and provision VMs as if they are bare-metal nodes. Ina bare-metal setup, Ironic will ask the BMC to install IPA on the node, and inour setup, sushy-tools will get the same request, but it will simply fakethe installation and, in the end, forward Ironic traffic to the Fake IPAcontainer. Another piece of information we will need is the cert that Ironic will usein its communication with IPA. IPA is supposed to get it from Ironic, but asFake IPA cannot do that (at least not yet), we must get the cert and provideit in Fake IPA config. mkdir certkubectl get secret -n baremetal-operator-system ironic-cert -o json \ -o=jsonpath= {. data. ca\. crt}  | base64 -d &gt;cert/ironic-ca. crtAlso note that one set of sushy-tools and Fake IPA containers won’t beenough to provision 1000 nodes. Just like Ironic, they need to be scaled upextensively (about 20-30 pairs will be sufficient for 1000 nodes), butfortunately, the scaling is straightforward: We just need to give them differentports. Both of these components also require a Python-based config file. Forconvenience, in this setup, we create a big file and provide it to both of them,using the following shell script: for i in $(seq 1  $N_SUSHY ); do container_conf_dir= $SUSHY_CONF_DIR/sushy-$i  # Use round-robin to choose fake-ipa and sushy-tools containers for the node fake_ipa_port=$((9901 + (($i % ${N_FAKE_IPA:-1})))) sushy_tools_port=$((8000 + i)) ports+=(${sushy_tools_port}) # This is only so that we have the list of the needed ports for other # purposes, like configuring the firewalls.  ports+=(${fake_ipa_port}) mkdir -p  ${container_conf_dir}  # Generate the htpasswd file, which is required by sushy-tools cat &lt;&lt;'EOF' &gt; ${container_conf_dir} /htpasswdadmin:$2b$12$/dVOBNatORwKpF. ss99KB. vESjfyONOxyH. UgRwNyZi1Xs/W2pGVSEOF # Set configuration options cat &lt;&lt;EOF &gt; ${container_conf_dir} /conf. pyimport collectionsSUSHY_EMULATOR_LIBVIRT_URI =  ${LIBVIRT_URI} SUSHY_EMULATOR_IGNORE_BOOT_DEVICE = FalseSUSHY_EMULATOR_VMEDIA_VERIFY_SSL = FalseSUSHY_EMULATOR_AUTH_FILE =  /root/sushy/htpasswd SUSHY_EMULATOR_FAKE_DRIVER = TrueSUSHY_EMULATOR_LISTEN_PORT =  ${sushy_tools_port} EXTERNAL_NOTIFICATION_URL =  http://${ADVERTISE_HOST}:${fake_ipa_port} FAKE_IPA_API_URL =  ${API_URL} FAKE_IPA_URL =  http://${ADVERTISE_HOST}:${fake_ipa_port} FAKE_IPA_INSPECTION_CALLBACK_URL =  ${CALLBACK_URL} FAKE_IPA_ADVERTISE_ADDRESS_IP =  ${ADVERTISE_HOST} FAKE_IPA_ADVERTISE_ADDRESS_PORT =  ${fake_ipa_port} FAKE_IPA_CAFILE =  /root/cert/ironic-ca. crt SUSHY_FAKE_IPA_LISTEN_IP =  ${ADVERTISE_HOST} SUSHY_FAKE_IPA_LISTEN_PORT =  ${fake_ipa_port} SUSHY_EMULATOR_FAKE_IPA = TrueSUSHY_EMULATOR_FAKE_SYSTEMS = $(cat nodes. json)EOF # Start sushy-tools docker run -d --net host --name  sushy-tools-${i}  \  -v  ${container_conf_dir} :/root/sushy \   ${SUSHY_TOOLS_IMAGE}  # Start fake-ipa docker run \  -d --net host --name fake-ipa-${i} \  -v  ${container_conf_dir} :/app \  -v  $(realpath cert) :/root/cert \   ${FAKEIPA_IMAGE} doneIn this setup, we made it so that all the sushy-tools containers willlisten on the port range running from 8001, 8002,…, while the Fake IPAcontainers have ports 9001, 9002,… Step 4: Add the BMH objects: Now that we have sushy-tools and Fake IPA containers running, we canalready generate the manifest for BMH objects, and apply them to the cluster. ABMH object will look like this ---apiVersion: v1kind: Secretmetadata: name: {name}-bmc-secret labels:   environment. metal3. io: baremetaltype: Opaquedata: username: YWRtaW4= password: cGFzc3dvcmQ=---apiVersion: metal3. io/v1alpha1kind: BareMetalHostmetadata: name: {name}spec: online: true bmc:  address: redfish+http://192. 168. 222. 1:{port}/redfish/v1/Systems/{uuid}  credentialsName: {name}-bmc-secret bootMACAddress: {random_mac} bootMode: legacyIn this manifest:  name is the node name we generated in the previous step.  uuid is the random uuid we generated for the same node.  random_mac is a random mac address for the boot. It’s NOT the same as theNIC mac address we generated for the node.  port is the listening port on one of the sushy-tools containers wecreated in the previous step. Since every sushy-tools and Fake IPAcontainer has information about ALL the nodes, we can decide what container tolocate the “node”. In general, it’s a good idea to spread them out, so allcontainers are loaded equally. We can now run kubectl apply -f on one (or all of) the BMH manifests. What youexpect to see is that a BMH object is created, and its state will change fromregistering to available after a while. It means ironic acknowledgedthat the node is valid, in good state and ready to be provisioned. Step 5: Deploy the fake nodes to kubernetes clusters: Before provisioning our clusters, let’s init the process, so that we have CAPIand CAPM3 installed clusterctl init --infrastructure=metal3After a while, we should see that CAPI, CAPM3, and IPAM pods become available. In a standard Metal3 workflow, after having the BMH objects in an availablestate, we can provision new Kubernetes clusters with clusterctl. However, withfake nodes, things get a tiny bit more complex. At the end of the provisioningprocess, Cluster API expects that there is a new kubernetes API servercreated for the new cluster, from which it will check if all nodes are up, allthe control planes have apiserver, etcd, etc. pods up and running, and soon. It is where the Fake Kubernetes API Server(FKAS)comes in. As the FKAS README linked above already described how it works, we won’t gointo details. We simply need to send FKAS a register POST request (withthe new cluster’s namespace and cluster name), and it will give us an IP and aport, which we can plug into our cluster template and then run clusterctlgenerate cluster. Under the hood, FKAS generates unique API servers for different clusters. Each of the fake API servers does the following jobs:  Mimicking API Calls: The Fake Kubernetes API server was set up to respond tothe essential Kubernetes API calls made during provisioning.  Node Registration: When CAPM3 registered nodes, the Fake API server returnedsuccess responses, making Metal3 believe the nodes had joined a real Kubernetescluster.  Cluster Health and Status: The Fake API responded with “healthy” statuses,allowing CAPI/CAPM3 to continue its workflow without interruption.  Node Creation and Deletion: When CAPI queried for node status or attempted toadd/remove nodes, the Fake API server responded realistically, ensuring theprovisioning process continued smoothly.  Pretending to Host Kubelet: The Fake API server also simulated kubeletresponses, which allowed CAPI/CAPM3 to interact with the fake clusters as thoughthey were managing actual nodes. Note that in this experiment, we provisioned every one of the 1000 fake nodes toa single-node cluster, but it’s possible to increase the number of controlplanes and worker nodes by changing the --control-plane-machine-count andworker-machine-count parameters in the clusterctl generate cluster command. However, you will need to ensure that all clusters’ total nodes do not exceedthe number of BMHs. As a glance, the whole simulation looks like this: It will likely take some time, but once the BMHs are all provisioned, we shouldbe able to verify that all, or at least, most of the clusters are in good shape: # This will list the clusters. kubectl get clusters -A# This will determine the clusters' readiness. kubectl get kcp -A For each cluster, it’s also a good idea to perform a clusterctlcheck. Accessing the fake cluster: A rather interesting (but not essential for our goal) check we can perform onthe fake clusters is to try accessing them. Let’s start with fetching acluster’s kubeconfig: clusterctl -n &lt;cluster-namespace&gt; get kubeconfig &lt;cluster-name&gt; &gt; kubeconfig-&lt;cluster-name&gt;. yamlAs usual, clusterctl will generate a kubeconfig file, but we cannot use itjust yet. Recall that we generated the API endpoint using FKAS; the address wehave now will be a combination of a port with FKAS’s IP address, which isn’taccessible from outside the cluster. What we should do now is:  Edit the kubeconfig-&lt;cluster-name&gt;. yaml so that the endpoint is in the formlocalhost:&lt;port&gt;.  Port-forward the FKAS Pod to the same port the kubeconfig has shown. And voila, now we can access the fake cluster with kubectl --kubeconfigkubeconfig-&lt;cluster-name&gt;. yaml. You can inspect its state and check theresources (nodes, pods, etc. ), but we won’t be able to run any workload on it asit’s fake. Results: In this post, we have demonstrated how it is possible to “generate”bare-metal-based Kubernetes clusters from thin air (or rather, a bunch of nodesthat do not exist). Of course, these “clusters” are not very useful. Still,successfully provisioning them without letting any of our main components(CAPI, CAPM3, BMO, and Ironic) know they are working with fakehardware proves that Metal3 is capable of handling a heavy workload andprovision multiple nodes/clusters. If interested, you could also check (and try out) the experiment by yourselfhere. "
    }, {
    "id": 3,
    "url": "/blog/2024/05/30/Scaling_part_3.html",
    "title": "Scaling to 1000 clusters - Part 3",
    "author" : "Lennart Jern",
    "tags" : "metal3, cluster API, provider, edge",
    "body": "In part 1, we introduced theBare Metal Operator test mode and saw how it can be used to play withBareMetalHosts without Ironic and without any actual hosts. We continued inpart 2 with how to fakeworkload clusters enough for convincing Cluster API’s controllers that they arehealthy. These two pieces together allowed us to run scaling tests and reach ourtarget of 1000 single node clusters. In this final part of the blog post series,we will take a look at the results, the issues that we encountered and theimprovements that have been made. Issues encountered and lessons learned: As part of this work we have learned a lot. We found genuine bugs andperformance issues, but we also learned about relevant configuration options forCluster API and controllers in general. One of the first things we hit was this bug in Bare MetalOperator thatcaused endless requeues for some deleted objects. It was not a big deal, barelynoticeable, at small scale. However, at larger scales things like this become aproblem. The logs become unreadable as they are filled with “spam” fromrequeuing deleted objects and the controller is wasting resources trying toreconcile them. As mentioned, we also learned a lot from this experiment. For example, that allthe controllers have flags for setting their concurrency, i. e. how many objectsthey reconcile in parallel. The default is 10, which works well in most cases,but for larger scales it may be necessary to tune this in order to speed up thereconciliation process. The next thing we hit was rate limits! Bothclient-goandcontroller-runtimehave default rate limits of 10 and 20 QPS (Queries Per Second) respectively thatthe controllers inherit unless overridden. In general, this is a good thing, asit prevents controllers from overloading the API server. They obviously becomean issue once you scale far enough though. For us that happened when we got to600 clusters. Why 600? The number was actually a good clue, and the reason we managed figureout what was wrong! Let’s break it down. By default, the Cluster API controllerwill reconcile objects every 10 minutes (=600 seconds) in addition to reactingto events. Each reconciliation will normally involve one or more API calls, soat 600 clusters, we would have at least one API call per second just from theperiodic sync. In other words, the controllers would at this point use up alarge part of their budget on periodic reconciliation and quickly reach theirlimit when adding reactions to events, such as the creation of a new cluster. At the time, these rate limits were not configurable in the Cluster APIcontrollers, so we had to patch the controllers to increase the limits. We havesince then added flags to the controllers to make this configurable. If youfound this interesting, you can read more about it in thisissue. With concurrency and rate limits taken care of, we managed to reach our targetof 1000 clusters in reasonable time. However, there was still a problem withresource usage. The Kubeadm control plane controller was unreasonably CPUhungry! Luckily, Cluster API has excellent debugging and monitoring toolsavailable so it was easyto collect data and profile the controllers. A quick look at the dashboardconfirmed that the Kubeadm control plane controller was indeed the culprit, witha CPU usage far higher than the other controllers.  We then collected some profiling data and found the cause of the CPU usage. Itwas generating new private keys for accessing the workload cluster API serverevery time it needed to access it. This is a CPU intensive operation, and ithappened four times per reconciliation! The flame graph seen below clearly showsthe four key generation operations, and makes it obvious that this is what takesup most of the time spent on the CPU for the controller.  Improvements: All issues mentioned in the previous section have been addressed. The Bare MetalOperator is no longer re-queuing deleted objects. All controllers have flags forsetting their concurrency and rate limits, and the Kubeadm control planecontroller is now caching and reusing the private keys instead of generating newones every time. The impact of all of this is that  the Bare Metal Operator has more readable logs and lower CPU usage, users can configure rate limits for all Cluster API and Metal3 controllers ifnecessary, and the Kubeadm control plane controller has a much lower CPU usage and fasterreconciliation times. Results: When we set out, it was simply not possible to reach a scale of 1000 clusters ina reasonable time. With the collaboration, help from maintainers and othercommunity members, we managed to reach our target. It is now possible to managethousands of workload clusters through a single Cluster API management cluster. The discussions and efforts also resulted in a deep dive presentation atKubeCon NA2023from the Cluster API maintainers. Cluster API itself now also has an in-memoryproviderwhich makes it almost trivial to test large scale scenarios. However, it must benoted that it can only be used to test the core, bootstrap and control planeproviders. If you want to try it out, you can use the following script. Pleasenote that this will still be CPU intensive, despite the improvements mentionedabove. Creating 1000 clusters is no small task! kind create clusterexport CLUSTER_TOPOLOGY=trueclusterctl init --core=cluster-api:v1. 7. 2 --bootstrap=kubeadm:v1. 7. 2 --control-plane=kubeadm:v1. 7. 2 --infrastructure=in-memory:v1. 7. 2# Patch the controllers to increase the rate limits and concurrencykubectl -n capi-system patch deployment capi-controller-manager \ --type=json -p='[  { op :  add ,  path :  /spec/template/spec/containers/0/args/- ,  value :  --kube-api-qps=100 },  { op :  add ,  path :  /spec/template/spec/containers/0/args/- ,  value :  --kube-api-burst=200 },  { op :  add ,  path :  /spec/template/spec/containers/0/args/- ,  value :  --cluster-concurrency=100 },  { op :  add ,  path :  /spec/template/spec/containers/0/args/- ,  value :  --machine-concurrency=100 } ]'kubectl -n capi-kubeadm-control-plane-system patch deployment capi-kubeadm-control-plane-controller-manager \ --type=json -p='[  { op :  add ,  path :  /spec/template/spec/containers/0/args/- ,  value :  --kube-api-qps=100 },  { op :  add ,  path :  /spec/template/spec/containers/0/args/- ,  value :  --kube-api-burst=200 },  { op :  add ,  path :  /spec/template/spec/containers/0/args/- ,  value :  --kubeadmcontrolplane-concurrency=100 } ]'kubectl -n capi-kubeadm-bootstrap-system patch deployment capi-kubeadm-bootstrap-controller-manager \ --type=json -p='[  { op :  add ,  path :  /spec/template/spec/containers/0/args/- ,  value :  --kube-api-qps=100 },  { op :  add ,  path :  /spec/template/spec/containers/0/args/- ,  value :  --kube-api-burst=200 },  { op :  add ,  path :  /spec/template/spec/containers/0/args/- ,  value :  --kubeadmconfig-concurrency=100 },  { op :  add ,  path :  /spec/template/spec/containers/0/args/- ,  value :  --cluster-concurrency=100 } ]'# Create a ClusterClass and save a Cluster manifestkubectl apply -f https://github. com/kubernetes-sigs/cluster-api/releases/download/v1. 7. 2/clusterclass-in-memory-quick-start. yamlclusterctl generate cluster in-memory-test --flavor=in-memory-development --kubernetes-version=v1. 30. 0 &gt; in-memory-cluster. yaml# Create 1000 clustersSTART=0NUM=1000for ((i=START; i&lt;NUM; i++))do name= test-$(printf  %03d\n   $i )  sed  s/in-memory-test/${name}/g  in-memory-cluster. yaml | kubectl apply -f -doneThis should result in 1000 ready in-memory clusters (and a pretty hot laptop ifyou run it locally). On a laptop with an i9-12900H CPU, it took about 15 minutesuntil all clusters were ready. Conclusion and next steps: We are very happy with the results we achieved. The community has been veryhelpful and responsive, and we are very grateful for all the help we received. Going forward, we will hopefully be able to run scale tests periodically toensure that we are not regressing. Even small scale tests can be enough todetect performance regressions as long as we keep track of the performancemetrics. This is something we hope to incorporate into the CI system in thefuture. "
    }, {
    "id": 4,
    "url": "/blog/2024/04/10/Metal3_at_KubeCon_EU_2024.html",
    "title": "Metal3 at KubeCon EU 2024",
    "author" : "Lennart Jern",
    "tags" : "metal3, talk, conference, kubecon",
    "body": "The Metal3 project was present at KubeCon EU 2024 with multiple maintainers,contributors and users! For many of us, this was the first time we met in thephysical world, despite working together for years already. This was veryvaluable and appreciated by many of us, I am sure. We had time to casuallydiscuss ideas and proposals, hack together on theironic-standalone-operatorand simply get to know each other.  Photo by Michael Captain. As a project, we had the opportunity to give an update through a lightningtalk on Tuesday! On Wednesday we continued with a contribfest sessionwhere we gave an introduction to the project for potential new contributors. Wehad prepared a number of good-first-issue’s that people could choose from ifthey wanted. Perhaps more important though, was that we had time to answerquestions, discuss use-cases, issues and features with the attendees. The newquick-start page was also launched just intime for the contribfest. It should hopefully make it easier to get started withthe project and we encourage everyone to run through it and report or fix anyissues found.  Photo from the official CNCF Flickr. More photoshere. Finally, just like previous, we had a table in the Project Pavilion. There was alot of interest in Metal3, more than last year I would say. Even with fivemaintainers working in parallel, we still had a hard time keeping up with theamount of people stopping by to ask questions! My takeaway from this event isthat we still have work to do on explaining what Metal3 is and how it works. Itis quite uncommon that people know about baseboard management controllers (BMCs)and this of course makes it harder to grasp what Metal3 is all about. However,the interest is there, so we just need to get the information out there so thatpeople can learn! Another takeaway is that Cluster API in general seems toreally take off. Many people that came by our kiosk knew about Cluster API andwere interested in Metal3 because of the integration with have with it. For those of you who couldn’t attend, I hope this post gives an idea about whathappened at KubeCon related to Metal3. Did you miss the contribfest? Maybe youwould like to contribute but don’t know where to start? Check out thegood-first-issue’s!There are still plenty to choose from, and we will keep adding more. "
    }, {
    "id": 5,
    "url": "/blog/2024/01/18/How_to_run_Metal3_website_locally_with_Jekyll.html",
    "title": "How to run Metal3 website locally with Jekyll",
    "author" : "Salima Rabiu",
    "tags" : "metal3, baremetal, metal3-dev-env, documentation, development",
    "body": "Introduction: If you’re a developer or contributor to the Metal3 project, you may needto run the Metal3 website locally to test changes and ensure everythinglooks as expected before deploying them. In this guide, we’ll walk youthrough the process of setting up and running Metal3’s website locallyon your machine using Jekyll. Prerequisites: Before we begin, make sure you have the following prerequisitesinstalled on your system:    Ruby: Jekyll, the static site generator used by Metal3, is built withRuby. Install Ruby and its development tools by running the followingcommand in your terminal:   sudo apt install ruby-full   Setting up Metal3’s Website: Once Ruby is installed, we can proceed to set up Metal3’s website andits dependencies. Follow these steps:    Clone the Metal3 website repository from GitHub. Open your terminaland navigate to the directory where you want to clone the repository,then run the following command:   git clone https://github. com/metal3-io/metal3-io. github. io. git      Change to the cloned directory:   cd metal3-io. github. io      Install the required gems and dependencies using Bundler. Run thefollowing command:   bundle install   Running the Metal3 Website Locally: With Metal3’s website and its dependencies installed, you can now start the localdevelopment server to view and test the website. In the terminal, navigate to theproject’s root directory (metal3-io. github. io) and run the following command: bundle exec jekyll serveThis command tells Jekyll to build the website and start a local server. Once the server is running, you’ll see output indicating the localaddress where the Metal3 website is being served, typicallyhttp://localhost:4000. Open your web browser and enter the provided address. Congratulations!You should now see the Metal3 website running locally, allowing you topreview your changes and ensure everything is working as expected. Conclusion: Running Metal3’s website locally using Jekyll is a great way to testchanges and ensure the site functions properly before deploying them. Byfollowing the steps outlined in this guide, you’ve successfully set upand run Metal3’s website locally. Feel free to explore the Metal3documentation and contribute to the project further. "
    }, {
    "id": 6,
    "url": "/blog/2023/05/17/Scaling_part_2.html",
    "title": "Scaling to 1000 clusters - Part 2",
    "author" : "Lennart Jern",
    "tags" : "metal3, cluster API, provider, edge",
    "body": "In part 1, we introduced the Bare Metal Operator test mode and saw how it can be used to play with BareMetalHosts without Ironic and without any actual hosts. Now we will take a look at the other end of the stack and how we can fake the workload cluster API’s. Test setup: The end goal is to have one management cluster where the Cluster API and Metal3 controllers run. In this cluster we would generate BareMetalHosts and create Clusters, Metal3Clusters, etc to benchmark the controllers. To give them a realistic test, we also need to fake the workload cluster API’s. These will run separately in “backing” clusters to avoid interfering with the test (e. g. by using up all the resources in the management cluster). Here is a diagram that describes the setup: How are we going to fake the workload cluster API’s then?The most obvious solution is to just run the real deal, i. e. the kube-apiserver. This is what would be run in a real workload cluster, together with the other components that make up the Kubernetes control plane. If you want to follow along and try to set this up yourself, you will need at least the following tools installed:  kind kubectl kubeadm clusterctl openssl curl wgetThis has been tested with Kubernetes v1. 25, kind v0. 19 and clusterctl v1. 4. 2. All script snippets are assumed to be for the bash shell. Running the Kubernetes API server: There are many misconceptions, maybe even superstitions, about the Kubernetes control plane. The fact is that it is in no way special. It consists of a few programs that can be run in any way you want: in a container, as a systemd unit or directly executed at the command line. They can run on a Node or outside of the cluster. You can even run multiple instances on the same host as long as you avoid port collisions. For our purposes we basically want to run as little as possible of the control plane components. We just need the API to be available and possible for us to populate with data that the controllers expect to be there. In other words, we need the API server and etcd. The scheduler is not necessary since we won’t run any actual workload (we are just pretending the Nodes are there anyway) and the controller manager would just get in the way when we want to fake resources. It would, for example, try to update the status of the (fake) Nodes that we want to create. The API server will need an etcd instance to connect to. It will also need some TLS configuration, both for connecting to etcd and for handling service accounts. One simple way to generate the needed certificates is to use kubeadm. But before we get there we need to think about how the configuration should look like. For simplicity, we will simply run the API server and etcd in a kind cluster for now. It would then be easy to run them in some other Kubernetes cluster later if needed. Let’s create it right away: kind create cluster# Note: This has been tested with node image# kindest/node:v1. 26. 3@sha256:61b92f38dff6ccc29969e7aa154d34e38b89443af1a2c14e6cfbd2df6419c66fTo try to cut down on the resources required, we will also use a single multi-tenant etcd instance instead of one per API server. We can rely on the internal service discovery so the API server can find etcd via an address like etcd-server. etd-system. svc. cluster. local, instead of using IP addresses. Finally, we will need an endpoint where the API is exposed to the cluster where the controllers are running, but for now we can focus on just getting it up and running with 127. 0. 0. 1:6443 as the endpoint. Based on the above, we can create a kubeadm-config. yaml file like this: apiVersion: kubeadm. k8s. io/v1beta3kind: ClusterConfigurationapiServer: certSANs:  - 127. 0. 0. 1clusterName: testcontrolPlaneEndpoint: 127. 0. 0. 1:6443etcd: local:  serverCertSANs:   - etcd-server. etcd-system. svc. cluster. local  peerCertSANs:   - etcd-0. etcd. etcd-system. svc. cluster. localkubernetesVersion: v1. 25. 3certificatesDir: /tmp/test/pkiWe can now use this to generate some certificates and upload them to the cluster: # Generate CA certificateskubeadm init phase certs etcd-ca --config kubeadm-config. yamlkubeadm init phase certs ca --config kubeadm-config. yaml# Generate etcd peer and server certificateskubeadm init phase certs etcd-peer --config kubeadm-config. yamlkubeadm init phase certs etcd-server --config kubeadm-config. yaml# Upload certificateskubectl create namespace etcd-systemkubectl -n etcd-system create secret tls test-etcd --cert /tmp/test/pki/etcd/ca. crt --key /tmp/test/pki/etcd/ca. keykubectl -n etcd-system create secret tls etcd-peer --cert /tmp/test/pki/etcd/peer. crt --key /tmp/test/pki/etcd/peer. keykubectl -n etcd-system create secret tls etcd-server --cert /tmp/test/pki/etcd/server. crt --key /tmp/test/pki/etcd/server. keyDeploying a multi-tenant etcd instance: Now it is time to deploy etcd! curl -L https://github. com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/manifests/v2/etcd. yaml \ | sed  s/CLUSTER/test/g  | kubectl -n etcd-system apply -f -kubectl -n etcd-system wait sts/etcd --for=jsonpath= {. status. availableReplicas} =1As mentioned before, we want to create a multi-tenant etcd that many API servers can share. For this reason, we will need to create a root user and enable authentication for etcd: # Create root rolekubectl -n etcd-system exec etcd-0 -- etcdctl \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ role add root# Create root userkubectl -n etcd-system exec etcd-0 -- etcdctl \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ user add root --new-user-password= rootpw kubectl -n etcd-system exec etcd-0 -- etcdctl \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ user grant-role root root# Enable authenticationkubectl -n etcd-system exec etcd-0 -- etcdctl \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ auth enableAt this point we have a working etcd instance with authentication and TLS enabled. Each client will need to have an etcd user to interact with this instance so we need to create an etcd user for the API server. We already created a root user before so this should look familiar. ## Create etcd tenant# Create userkubectl -n etcd-system exec etcd-0 -- etcdctl --user root:rootpw \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ user add test --new-user-password=test# Create rolekubectl -n etcd-system exec etcd-0 -- etcdctl --user root:rootpw \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ role add test# Add read/write permissions for prefix to the rolekubectl -n etcd-system exec etcd-0 -- etcdctl --user root:rootpw \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ role grant-permission test --prefix=true readwrite  /test/ # Give the user permissions from the rolekubectl -n etcd-system exec etcd-0 -- etcdctl --user root:rootpw \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ user grant-role test testFrom etcd’s point of view, everything is now ready. The API server could theoretically use etcdctl and authenticate with the username and password that we created for it. However, that is not how the API server works. It expects to be able to authenticate using client certificates. Luckily, etcd supports this so we just have to generate the certificates and sign them so that etcd trusts them. The key thing is to set the common name in the certificate to the name of the user we want to authenticate as. Since kubeadm always sets the same common name, we will here use openssl to generate the client certificates so that we get control over it. # Generate etcd client certificateopenssl req -newkey rsa:2048 -nodes -subj  /CN=test  \ -keyout  /tmp/test/pki/apiserver-etcd-client. key  -out  /tmp/test/pki/apiserver-etcd-client. csr openssl x509 -req -in  /tmp/test/pki/apiserver-etcd-client. csr  \ -CA /tmp/test/pki/etcd/ca. crt -CAkey /tmp/test/pki/etcd/ca. key -CAcreateserial \ -out  /tmp/test/pki/apiserver-etcd-client. crt  -days 365Deploying the API server: In order to deploy the API server, we will first need to generate some more certificates. The client certificates for connecting to etcd are already ready, but it also needs certificates to secure the exposed API itself, and a few other things. Then we will also need to create secrets from all of these certificates: kubeadm init phase certs ca --config kubeadm-config. yamlkubeadm init phase certs apiserver --config kubeadm-config. yamlkubeadm init phase certs sa --cert-dir /tmp/test/pkikubectl create ns workload-apikubectl -n workload-api create secret tls test-ca --cert /tmp/test/pki/ca. crt --key /tmp/test/pki/ca. keykubectl -n workload-api create secret tls test-etcd --cert /tmp/test/pki/etcd/ca. crt --key /tmp/test/pki/etcd/ca. keykubectl -n workload-api create secret tls  test-apiserver-etcd-client  \ --cert  /tmp/test/pki/apiserver-etcd-client. crt  \ --key  /tmp/test/pki/apiserver-etcd-client. key kubectl -n workload-api create secret tls apiserver \ --cert  /tmp/test/pki/apiserver. crt  \ --key  /tmp/test/pki/apiserver. key kubectl -n workload-api create secret generic test-sa \ --from-file=tls. crt= /tmp/test/pki/sa. pub  \ --from-file=tls. key= /tmp/test/pki/sa. key With all that out of the way, we can finally deploy the API server!For this we will use a normal Deployment. # Deploy API servercurl -L https://github. com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/manifests/v2/kube-apiserver-deployment. yaml | sed  s/CLUSTER/test/g  | kubectl -n workload-api apply -f -kubectl -n workload-api wait --for=condition=Available deploy/test-kube-apiserverTime to check if it worked!We can use port-forwarding to access the API, but of course we will need some authentication method for it to be useful. With kubeadm we can generate a kubeconfig based on the certificates we already have. kubeadm kubeconfig user --client-name kubernetes-admin --org system:masters \ --config kubeadm-config. yaml &gt; kubeconfig. yamlNow open another terminal and set up port-forwarding to the API server: kubectl -n workload-api port-forward svc/test-kube-apiserver 6443Back in the original terminal, you should now be able to reach the workload API server: kubectl --kubeconfig kubeconfig. yaml cluster-infoNote that it won’t have any Nodes or Pods running. It is completely empty since it is running on its own. There is no kubelet that registered as a Node or applied static manifests, there is no scheduler or controller manager. Exactly like we want it. Faking Nodes and other resources: Let’s take a step back and think about what we have done so far. We have deployed a Kubernetes API server and a multi-tenant etcd instance. More API servers can be added in the same way, so it is straight forward to scale. All of it runs in a kind cluster, which means that it is easy to set up and we can switch to any other Kubernetes cluster if needed later. Through Kubernetes we also get an easy way to access the API servers by using port-forwarding, without exposing all of them separately. The time has now come to think about what we need to put in the workload cluster API to convince the Cluster API and Metal3 controllers that it is healthy. First of all they will expect to see Nodes that match the Machines and that they have a provider ID set. Secondly, they will expect to see healthy control plane Pods. Finally, they will try to check on the etcd cluster. The final point is a problem, but we can work around it for now by configuring external etcd. It will lead to a different code path for the bootstrap and control plane controllers, but until we have something better it will be a good enough test. Creating the Nodes and control plane Pods is really easy though. We are just adding resources and there are no controllers or validating web hooks that can interfere. Try it out! # Create a Nodekubectl --kubeconfig=kubeconfig. yaml create -f https://github. com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/fake-node. yaml# Check that it workedkubectl --kubeconfig=kubeconfig. yaml get nodes# Maybe label it as part of the control plane?kubectl --kubeconfig=kubeconfig. yaml label node fake-node node-role. kubernetes. io/control-plane=  Now add a Pod: kubectl --kubeconfig=kubeconfig. yaml create -f https://github. com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-apiserver-pod. yaml# Set status on the pods (it is not added when using create/apply). curl -L https://github. com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-apiserver-pod-status. yaml | kubectl --kubeconfig=kubeconfig. yaml -n kube-system patch pod kube-apiserver-node-name \  --subresource=status --patch-file=/dev/stdinYou should be able to see something like this: $ kubectl --kubeconfig kubeconfig. yaml get pods -ANAMESPACE   NAME            READY  STATUS  RESTARTS  AGEkube-system  kube-apiserver-node-name  1/1   Running  0     16h$ kubectl --kubeconfig kubeconfig. yaml get nodesNAME    STATUS  ROLES  AGE  VERSIONfake-node  Ready  &lt;none&gt;  16h  v1. 25. 3Now all we have to do is to ensure that the API returns information that the controllers expect. Hooking up the API server to a Cluster API cluster: We will now set up a fresh cluster where we can run the Cluster API and Metal3 controllers. # Delete the previous clusterkind delete cluster# Create a fresh new clusterkind create cluster# Initialize Cluster API with Metal3clusterctl init --infrastructure metal3## Deploy the Bare Metal Opearator# Create the namespace where it will runkubectl create ns baremetal-operator-system# Deploy it in normal modekubectl apply -k https://github. com/metal3-io/baremetal-operator/config/default# Patch it to run in test modekubectl patch -n baremetal-operator-system deploy baremetal-operator-controller-manager --type=json \ -p='[{ op :  add ,  path :  /spec/template/spec/containers/0/args/- ,  value :  --test-mode }]'You should now have a cluster with the Cluster API, Metal3 provider and Bare Metal Operator running. Next, we will prepare some files that will come in handy later, namely a cluster template, BareMetalHost manifest and Kubeadm configuration file. # Download cluster-templateCLUSTER_TEMPLATE=/tmp/cluster-template. yaml# https://github. com/metal3-io/cluster-api-provider-metal3/blob/main/examples/clusterctl-templates/clusterctl-cluster. yamlCLUSTER_TEMPLATE_URL= https://raw. githubusercontent. com/metal3-io/cluster-api-provider-metal3/main/examples/clusterctl-templates/clusterctl-cluster. yaml wget -O  ${CLUSTER_TEMPLATE}   ${CLUSTER_TEMPLATE_URL} # Save a manifest of a BareMetalHostcat &lt;&lt; EOF &gt; /tmp/test-hosts. yaml---apiVersion: v1kind: Secretmetadata: name: worker-1-bmc-secrettype: Opaquedata: username: YWRtaW4= password: cGFzc3dvcmQ=---apiVersion: metal3. io/v1alpha1kind: BareMetalHostmetadata: name: worker-1spec: online: true bmc:  address: libvirt://192. 168. 122. 1:6233/  credentialsName: worker-1-bmc-secret bootMACAddress:  00:60:2F:10:E9:A7 EOF# Save a kubeadm config templatecat &lt;&lt; EOF &gt; /tmp/kubeadm-config-template. yamlapiVersion: kubeadm. k8s. io/v1beta3kind: ClusterConfigurationapiServer: certSANs:  - localhost  - 127. 0. 0. 1  - 0. 0. 0. 0  - HOSTclusterName: testcontrolPlaneEndpoint: HOST:6443etcd: local:  serverCertSANs:   - etcd-server. etcd-system. svc. cluster. local  peerCertSANs:   - etcd-0. etcd. etcd-system. svc. cluster. localkubernetesVersion: v1. 25. 3certificatesDir: /tmp/CLUSTER/pkiEOFWith this we have enough to start creating the workload cluster. First, we need to set up some certificates. This should look very familiar from earlier when we created certificates for the Kubernetes API server and etcd. mkdir -p /tmp/pki/etcdCLUSTER= test NAMESPACE=etcd-systemCLUSTER_APIENDPOINT_HOST= test-kube-apiserver. ${NAMESPACE}. svc. cluster. local sed -e  s/NAMESPACE/${NAMESPACE}/g  -e  s/\/CLUSTER//g  -e  s/HOST/${CLUSTER_APIENDPOINT_HOST}/g  \ /tmp/kubeadm-config-template. yaml &gt;  /tmp/kubeadm-config-${CLUSTER}. yaml # Generate CA certificateskubeadm init phase certs etcd-ca --config  /tmp/kubeadm-config-${CLUSTER}. yaml kubeadm init phase certs ca --config  /tmp/kubeadm-config-${CLUSTER}. yaml # Generate etcd peer and server certificateskubeadm init phase certs etcd-peer --config  /tmp/kubeadm-config-${CLUSTER}. yaml kubeadm init phase certs etcd-server --config  /tmp/kubeadm-config-${CLUSTER}. yaml Next, we create the namespace, the BareMetalHost and secrets from the certificates: CLUSTER=test-1NAMESPACE=test-1kubectl create namespace  ${NAMESPACE} kubectl -n  ${NAMESPACE}  apply -f /tmp/test-hosts. yamlkubectl -n  ${NAMESPACE}  create secret tls  ${CLUSTER}-etcd  --cert /tmp/pki/etcd/ca. crt --key /tmp/pki/etcd/ca. keykubectl -n  ${NAMESPACE}  create secret tls  ${CLUSTER}-ca  --cert /tmp/pki/ca. crt --key /tmp/pki/ca. keyWe are now ready to create the cluster!We just need a few variables for the template. The important part here is the CLUSTER_APIENDPOINT_HOST and CLUSTER_APIENDPOINT_PORT, since this will be used by the controllers to connect to the workload cluster API. You should set the IP to the private IP of the test machine or similar. This way we can use port-forwarding to expose the API on this IP, which the controllers can then reach. The port just have to be one not in use, and preferably something that is easy to remember and associate with the correct cluster. For example, cluster 1 gets port 10001, cluster 2 gets 10002, etc. export IMAGE_CHECKSUM= 97830b21ed272a3d854615beb54cf004 export IMAGE_CHECKSUM_TYPE= md5 export IMAGE_FORMAT= raw export IMAGE_URL= http://172. 22. 0. 1/images/rhcos-ootpa-latest. qcow2 export KUBERNETES_VERSION= v1. 25. 3 export WORKERS_KUBEADM_EXTRA_CONFIG=  export CLUSTER_APIENDPOINT_HOST= 172. 17. 0. 2 export CLUSTER_APIENDPOINT_PORT= 10001 export CTLPLANE_KUBEADM_EXTRA_CONFIG=   clusterConfiguration:   controlPlaneEndpoint: ${CLUSTER_APIENDPOINT_HOST}:${CLUSTER_APIENDPOINT_PORT}   apiServer:    certSANs:    - localhost    - 127. 0. 0. 1    - 0. 0. 0. 0    - ${CLUSTER_APIENDPOINT_HOST}   etcd:    external:     endpoints:      - https://etcd-server:2379     caFile: /etc/kubernetes/pki/etcd/ca. crt     certFile: /etc/kubernetes/pki/apiserver-etcd-client. crt     keyFile: /etc/kubernetes/pki/apiserver-etcd-client. key Create the cluster! clusterctl generate cluster  ${CLUSTER}  \  --from  ${CLUSTER_TEMPLATE}  \  --target-namespace  ${NAMESPACE}  | kubectl apply -f -This will give you a cluster and all the templates and other resources that are needed. However, we will need to fill in for the non-existent hardware and create the workload cluster API server, like we practiced before. This time it is slightly different, because some of the steps are handled by the Cluster API. We just need to take care of what would happen on the node, plus the etcd part since we are using external etcd configuration. mkdir -p  /tmp/${CLUSTER}/pki/etcd # Generate etcd client certificateopenssl req -newkey rsa:2048 -nodes -subj  /CN=${CLUSTER}  \ -keyout  /tmp/${CLUSTER}/pki/apiserver-etcd-client. key  -out  /tmp/${CLUSTER}/pki/apiserver-etcd-client. csr openssl x509 -req -in  /tmp/${CLUSTER}/pki/apiserver-etcd-client. csr  \ -CA /tmp/pki/etcd/ca. crt -CAkey /tmp/pki/etcd/ca. key -CAcreateserial \ -out  /tmp/${CLUSTER}/pki/apiserver-etcd-client. crt  -days 365# Get the k8s ca certificate and key. # This is used by kubeadm to generate the api server certificateskubectl -n  ${NAMESPACE}  get secrets  ${CLUSTER}-ca  -o jsonpath= {. data. tls\. crt}  | base64 -d &gt;  /tmp/${CLUSTER}/pki/ca. crt kubectl -n  ${NAMESPACE}  get secrets  ${CLUSTER}-ca  -o jsonpath= {. data. tls\. key}  | base64 -d &gt;  /tmp/${CLUSTER}/pki/ca. key # Generate certificatessed -e  s/NAMESPACE/${NAMESPACE}/g  -e  s/CLUSTER/${CLUSTER}/g  -e  s/HOST/${CLUSTER_APIENDPOINT_HOST}/g  \ /tmp/kubeadm-config-template. yaml &gt;  /tmp/kubeadm-config-${CLUSTER}. yaml kubeadm init phase certs apiserver --config  /tmp/kubeadm-config-${CLUSTER}. yaml # Create secretskubectl -n  ${NAMESPACE}  create secret tls  ${CLUSTER}-apiserver-etcd-client  --cert  /tmp/${CLUSTER}/pki/apiserver-etcd-client. crt  --key  /tmp/${CLUSTER}/pki/apiserver-etcd-client. key kubectl -n  ${NAMESPACE}  create secret tls apiserver --cert  /tmp/${CLUSTER}/pki/apiserver. crt  --key  /tmp/${CLUSTER}/pki/apiserver. key Now we will need to set up the fake cluster resources. For this we will create a second kind cluster and set up etcd, just like we did before. # Note: This will create a kubeconfig context named kind-backing-cluster-1,# i. e.  kind-  is prefixed to the name. kind create cluster --name backing-cluster-1# Setup central etcdCLUSTER= test NAMESPACE=etcd-systemkubectl create namespace  ${NAMESPACE} # Upload certificateskubectl -n  ${NAMESPACE}  create secret tls  ${CLUSTER}-etcd  --cert /tmp/pki/etcd/ca. crt --key /tmp/pki/etcd/ca. keykubectl -n  ${NAMESPACE}  create secret tls etcd-peer --cert /tmp/pki/etcd/peer. crt --key /tmp/pki/etcd/peer. keykubectl -n  ${NAMESPACE}  create secret tls etcd-server --cert /tmp/pki/etcd/server. crt --key /tmp/pki/etcd/server. key# Deploy ETCDcurl -L https://github. com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/manifests/v2/etcd. yaml \ | sed  s/CLUSTER/${CLUSTER}/g  | kubectl -n  ${NAMESPACE}  apply -f -kubectl -n etcd-system wait sts/etcd --for=jsonpath= {. status. availableReplicas} =1# Create root rolekubectl -n etcd-system exec etcd-0 -- etcdctl \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ role add root# Create root userkubectl -n etcd-system exec etcd-0 -- etcdctl \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ user add root --new-user-password= rootpw kubectl -n etcd-system exec etcd-0 -- etcdctl \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ user grant-role root root# Enable authenticationkubectl -n etcd-system exec etcd-0 -- etcdctl \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ auth enableSwitch the context back to the first cluster with kubectl config use-context kind-kind so we don’t get confused about which is the main cluster. We will now need to put all the expected certificates for the fake cluster in the kind-backing-cluster-1 so that they can be used by the API server that we will deploy there. CLUSTER=test-1NAMESPACE=test-1# Setup fake resources for cluster test-1kubectl --context=kind-backing-cluster-1 create namespace  ${NAMESPACE} kubectl --context=kind-backing-cluster-1 -n  ${NAMESPACE}  create secret tls  ${CLUSTER}-etcd  --cert /tmp/pki/etcd/ca. crt --key /tmp/pki/etcd/ca. keykubectl --context=kind-backing-cluster-1 -n  ${NAMESPACE}  create secret tls  ${CLUSTER}-ca  --cert /tmp/pki/ca. crt --key /tmp/pki/ca. keykubectl --context=kind-backing-cluster-1 -n  ${NAMESPACE}  create secret tls  ${CLUSTER}-apiserver-etcd-client  --cert  /tmp/${CLUSTER}/pki/apiserver-etcd-client. crt  --key  /tmp/${CLUSTER}/pki/apiserver-etcd-client. key kubectl --context=kind-backing-cluster-1 -n  ${NAMESPACE}  create secret tls apiserver --cert  /tmp/${CLUSTER}/pki/apiserver. crt  --key  /tmp/${CLUSTER}/pki/apiserver. key kubectl -n  ${NAMESPACE}  get secrets  ${CLUSTER}-sa  -o yaml | kubectl --context=kind-backing-cluster-1 create -f -## Create etcd tenant# Create userkubectl --context=kind-backing-cluster-1 -n etcd-system exec etcd-0 -- etcdctl --user root:rootpw \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ user add  ${CLUSTER}  --new-user-password= ${CLUSTER} # Create rolekubectl --context=kind-backing-cluster-1 -n etcd-system exec etcd-0 -- etcdctl --user root:rootpw \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ role add  ${CLUSTER} # Add read/write permissions for prefix to the rolekubectl --context=kind-backing-cluster-1 -n etcd-system exec etcd-0 -- etcdctl --user root:rootpw \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ role grant-permission  ${CLUSTER}  --prefix=true readwrite  /${CLUSTER}/ # Give the user permissions from the rolekubectl --context=kind-backing-cluster-1 -n etcd-system exec etcd-0 -- etcdctl --user root:rootpw \ --key=/etc/kubernetes/pki/etcd/tls. key --cert=/etc/kubernetes/pki/etcd/tls. crt --cacert /etc/kubernetes/pki/ca/tls. crt \ user grant-role  ${CLUSTER}   ${CLUSTER} Check that the Metal3Machine is associated with a BareMetalHost. Deploy the API server. # Deploy API servercurl -L https://github. com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/manifests/v2/kube-apiserver-deployment. yaml | sed -e  s/CLUSTER/${CLUSTER}/g  | kubectl --context=kind-backing-cluster-1 -n  ${NAMESPACE}  apply -f -kubectl --context=kind-backing-cluster-1 -n  ${NAMESPACE}  wait --for=condition=Available deploy/test-kube-apiserver# Get kubeconfigclusterctl -n  ${NAMESPACE}  get kubeconfig  ${CLUSTER}  &gt;  /tmp/kubeconfig-${CLUSTER}. yaml # Edit kubeconfig to point to 127. 0. 0. 1:${CLUSTER_APIENDPOINT_PORT}sed -i -e  s/${CLUSTER_APIENDPOINT_HOST}/127. 0. 0. 1/  -e  s/:6443/:${CLUSTER_APIENDPOINT_PORT}/   /tmp/kubeconfig-${CLUSTER}. yaml # Port forward for accessing the APIkubectl --context=kind-backing-cluster-1 -n  ${NAMESPACE}  port-forward \   --address  ${CLUSTER_APIENDPOINT_HOST},127. 0. 0. 1  svc/test-kube-apiserver  ${CLUSTER_APIENDPOINT_PORT} :6443 &amp;# Check that it is workingkubectl --kubeconfig= /tmp/kubeconfig-${CLUSTER}. yaml  cluster-infoNow that we have a working API for the workload cluster, the only remaining thing is to put everything that the controllers expect in it. This includes adding a Node to match the Machine as well as static pods that Cluster API expects to be there. Let’s start with the Node!The Node must have the correct name and a label with the BareMetalHost UID so that the controllers can put the correct provider ID on it. We have only created 1 BareMetalHost so it is easy to pick the correct one. The name of the Node should be the same as the Machine, which is also only a single one. machine= $(kubectl -n  ${NAMESPACE}  get machine -o jsonpath= {. items[0]. metadata. name} ) bmh_uid= $(kubectl -n  ${NAMESPACE}  get bmh -o jsonpath= {. items[0]. metadata. uid} ) curl -L https://github. com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/fake-node. yaml | sed -e  s/fake-node/${machine}/g  -e  s/fake-uuid/${bmh_uid}/g  | \ kubectl --kubeconfig= /tmp/kubeconfig-${CLUSTER}. yaml  create -f -# Label it as control-plane since this is a control-plane node. kubectl --kubeconfig= /tmp/kubeconfig-${CLUSTER}. yaml  label node  ${machine}  node-role. kubernetes. io/control-plane=  # Upload kubeadm config to configmap. This will mark the KCP as initialized. kubectl --kubeconfig= /tmp/kubeconfig-${CLUSTER}. yaml  -n kube-system create cm kubeadm-config \ --from-file=ClusterConfiguration= /tmp/kubeadm-config-${CLUSTER}. yaml This should be enough to make the Machines healthy!You should be able to see something similar to this: $ clusterctl -n test-1 describe cluster test-1NAME                      READY SEVERITY REASON SINCE MESSAGECluster/test-1                 True           46s├─ClusterInfrastructure - Metal3Cluster/test-1 True           114m└─ControlPlane - KubeadmControlPlane/test-1   True           46s └─Machine/test-1-f2nw2            True           47sHowever, if you check the KubeadmControlPlane more carefully, you will notice that it is still complaining about control plane components. This is because we have not created the static pods yet, and it is also unable to check the certificate expiration date for the Machine. Let’s fix it: # Add static pods to make kubeadm control plane manager happycurl -L https://github. com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-apiserver-pod. yaml | sed  s/node-name/${machine}/g  | kubectl --kubeconfig= /tmp/kubeconfig-${CLUSTER}. yaml  create -f -curl -L https://github. com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-controller-manager-pod. yaml | sed  s/node-name/${machine}/g  | kubectl --kubeconfig= /tmp/kubeconfig-${CLUSTER}. yaml  create -f -curl -L https://github. com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-scheduler-pod. yaml | sed  s/node-name/${machine}/g  | kubectl --kubeconfig= /tmp/kubeconfig-${CLUSTER}. yaml  create -f -# Set status on the pods (it is not added when using create/apply). curl -L https://github. com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-apiserver-pod-status. yaml | kubectl --kubeconfig= /tmp/kubeconfig-${CLUSTER}. yaml  -n kube-system patch pod  kube-apiserver-${machine}  \  --subresource=status --patch-file=/dev/stdincurl -L https://github. com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-controller-manager-pod-status. yaml | kubectl --kubeconfig= /tmp/kubeconfig-${CLUSTER}. yaml  -n kube-system patch pod  kube-controller-manager-${machine}  \  --subresource=status --patch-file=/dev/stdincurl -L https://github. com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-scheduler-pod-status. yaml | kubectl --kubeconfig= /tmp/kubeconfig-${CLUSTER}. yaml  -n kube-system patch pod  kube-scheduler-${machine}  \  --subresource=status --patch-file=/dev/stdin# Add certificate expiry annotations to make kubeadm control plane manager happyCERT_EXPIRY_ANNOTATION= machine. cluster. x-k8s. io/certificates-expiry EXPIRY_TEXT= $(kubectl -n  ${NAMESPACE}  get secret apiserver -o jsonpath= {. data. tls\. crt}  | base64 -d | openssl x509 -enddate -noout | cut -d= -f 2) EXPIRY= $(date --date= ${EXPIRY_TEXT}  --iso-8601=seconds) kubectl -n  ${NAMESPACE}  annotate machine  ${machine}   ${CERT_EXPIRY_ANNOTATION}=${EXPIRY} kubectl -n  ${NAMESPACE}  annotate kubeadmconfig --all  ${CERT_EXPIRY_ANNOTATION}=${EXPIRY} Now we finally have a completely healthy cluster as far as the controllers are concerned. Conclusions and summary: We now have all the tools necessary to start experimenting.  With the BareMetal Operator running in test mode, we can skip Ironic and still work with BareMetalHosts that act like normal.  We can set up separate “backing” clusters where we run etcd and multiple API servers to fake the workload cluster API’s.  Fake Nodes and Pods can be easily added to the workload cluster API’s, and configured as we want.  The workload cluster API’s can be exposed to the controllers in the test cluster using port-forwarding. In this post we have not automated all of this, but if you want to see a scripted setup, take a look at this. It is what we used to scale to 1000 clusters. Just remember that it may need some tweaking for your specific environment if you want to try it out! Specifically we used 10 “backing” clusters, i. e. 10 separate cloud VMs with kind clusters where we run etcd and the workload cluster API’s. Each one would hold 100 API servers. The test cluster was on its own separate VM also running a kind cluster with all the controllers and all the Cluster objects, etc. In the next and final blog post of this series we will take a look at the results of all this. What issues did we run into along the way?How did we fix or work around them?We will also take a look at what is going on in the community related to this and discuss potential future work in the area. "
    }, {
    "id": 7,
    "url": "/blog/2023/05/05/Scaling_part_1.html",
    "title": "Scaling to 1000 clusters - Part 1",
    "author" : "Lennart Jern",
    "tags" : "metal3, cluster API, provider, edge",
    "body": "We want to ensure that Metal3 can scale to thousands of nodes and clusters. However, running tests with thousands of real servers is expensive and we don’t have access to any such large environment in the project. So instead we have been focusing on faking the hardware while trying to keep things as realistic as possible for the controllers. In this first part we will take a look at the Bare Metal Operator and the test mode it offers. The next part will be about how to fake the Kubernetes API of the workload clusters. In the final post we will take a look at the issues we ran into and what is being done in the community to address them so that we can keep scaling! Some background on how to fool the controllers: With the full Metal3 stack, from Ironic to Cluster API, we have the following controllers that operate on Kubernetes APIs:  Cluster API Kubeadm control plane controller Cluster API Kubeadm bootstrap controller Cluster API controller Cluster API provider for Metal3 controller IP address manager controller Bare Metal Operator controllerWe will first focus on the controllers that interact with Nodes, Machines, Metal3Machines and BareMetalHosts, i. e. objects related to actual physical machines that we need to fake. In other words, we are skipping the IP address manager for now. What do these controllers care about really?What do we need to do to fool them?At the Cluster API level, the controllers just care about the Kubernetes resources in the management cluster (e. g. Clusters and Machines) and some resources in the workload cluster (e. g. Nodes and the etcd Pods). The controllers will try to connect to the workload clusters in order to check the status of the resources there, so if there is no real workload cluster, this is something we will need to fake if we want to fool the controllers. When it comes to Cluster API provider for Metal3, it connects the abstract high level objects with the BareMetalHosts, so here we will need to make the BareMetalHosts to behave realistically in order to provide a good test. This is where the Bare Metal Operator test mode comes in. If we can fake the workload cluster API and the BareMetalHosts, then all the Cluster API controllers and the Metal3 provider will get a realistic test that we can use when working on scalability. Bare Metal Operator test mode: The Bare Metal Operator has a test mode, in which it doesn’t talk to Ironic. Instead it just pretends that everything is fine and all actions succeed. In this mode the BareMetalHosts will move through the state diagram just like they normally would (but quite a bit faster). To enable it, all you have to do is add the -test-mode flag when running the Bare Metal Operator controller. For convenience there is also a make target (make run-test-mode) that will run the Bare Metal Operator directly on the host in test mode. Here is an example of how to use it. You will need kind and kubectl installed for this to work, but you don’t need the Bare Metal Operator repository cloned.    Create a kind cluster and deploy cert-manager (needed for web hook certificates):   kind create cluster# Install cert-managerkubectl apply -f https://github. com/cert-manager/cert-manager/releases/download/v1. 11. 0/cert-manager. yaml      Deploy the Bare Metal Operator in test mode:   # Create the namespace where it will runkubectl create ns baremetal-operator-system# Deploy it in normal modekubectl apply -k https://github. com/metal3-io/baremetal-operator/config/default# Patch it to run in test modekubectl patch -n baremetal-operator-system deploy baremetal-operator-controller-manager --type=json \ -p='[{ op :  add ,  path :  /spec/template/spec/containers/0/args/- ,  value :  --test-mode }]'      In a separate terminal, create a BareMetalHost from the example manifests:   kubectl apply -f https://github. com/metal3-io/baremetal-operator/raw/main/examples/example-host. yaml   After applying the BareMetalHost, it will quickly go through registering and become available. $ kubectl get bmhNAME          STATE     CONSUMER  ONLINE  ERROR  AGEexample-baremetalhost  registering       true       2s$ kubectl get bmhNAME          STATE    CONSUMER  ONLINE  ERROR  AGEexample-baremetalhost  available       true       6sWe can now provision the BareMetalHost, turn it off, deprovision, etc. Just like normal, except that the machine doesn’t exist. Let’s try provisioning it! kubectl patch bmh example-baremetalhost --type=merge --patch-file=/dev/stdin &lt;&lt;EOFspec: image:  url:  http://example. com/totally-fake-image. vmdk   checksum:  made-up-checksum   format: vmdkEOFYou will see it go through provisioning and end up in provisioned state: $ kubectl get bmhNAME          STATE     CONSUMER  ONLINE  ERROR  AGEexample-baremetalhost  provisioning       true       7m20s$ kubectl get bmhNAME          STATE     CONSUMER  ONLINE  ERROR  AGEexample-baremetalhost  provisioned       true       7m22sWrapping up: With Bare Metal Operator in test mode, we have the foundation for starting our scalability journey. We can easily create BareMetalHost objects and they behave similar to what they would in a real scenario. A simple bash script will at this point allow us to create as many BareMetalHosts as we would like. To wrap things up, we will now do just that: put together a script and try generating a few BareMetalHosts. The script will do the same thing we did before when creating the example BareMetalHost, but it will also give them different names so we don’t get naming collisions. Here it is: #!/usr/bin/env bashset -eucreate_bmhs() { n= ${1}  for (( i = 1; i &lt;= n; ++i )); do  cat &lt;&lt; EOF---apiVersion: v1kind: Secretmetadata: name: worker-$i-bmc-secrettype: Opaquedata: username: YWRtaW4= password: cGFzc3dvcmQ=---apiVersion: metal3. io/v1alpha1kind: BareMetalHostmetadata: name: worker-$ispec: online: true bmc:  address: libvirt://192. 168. 122. $i:6233/  credentialsName: worker-$i-bmc-secret bootMACAddress:  $(printf '00:60:2F:%02X:%02X:%02X\n' $((RANDOM%256)) $((RANDOM%256)) $((RANDOM%256))) EOF done}NUM= ${1:-10} create_bmhs  ${NUM} Save it as produce-available-hosts. sh and try it out: $ . /produce-available-hosts. sh 10 | kubectl apply -f -secret/worker-1-bmc-secret createdbaremetalhost. metal3. io/worker-1 createdsecret/worker-2-bmc-secret createdbaremetalhost. metal3. io/worker-2 createdsecret/worker-3-bmc-secret createdbaremetalhost. metal3. io/worker-3 createdsecret/worker-4-bmc-secret createdbaremetalhost. metal3. io/worker-4 createdsecret/worker-5-bmc-secret createdbaremetalhost. metal3. io/worker-5 createdsecret/worker-6-bmc-secret createdbaremetalhost. metal3. io/worker-6 createdsecret/worker-7-bmc-secret createdbaremetalhost. metal3. io/worker-7 createdsecret/worker-8-bmc-secret createdbaremetalhost. metal3. io/worker-8 createdsecret/worker-9-bmc-secret createdbaremetalhost. metal3. io/worker-9 createdsecret/worker-10-bmc-secret createdbaremetalhost. metal3. io/worker-10 created$ kubectl get bmhNAME    STATE     CONSUMER  ONLINE  ERROR  AGEworker-1  registering       true       2sworker-10  available        true       2sworker-2  available        true       2sworker-3  available        true       2sworker-4  available        true       2sworker-5  available        true       2sworker-6  registering       true       2sworker-7  available        true       2sworker-8  available        true       2sworker-9  available        true       2sWith this we conclude the first part of the scaling series. In the next post, we will take a look at how to fake the other end of the stack: the workload cluster API. "
    }, {
    "id": 8,
    "url": "/blog/2022/07/08/One_cluster_multiple_providers.html",
    "title": "One cluster - multiple providers",
    "author" : "Lennart Jern",
    "tags" : "metal3, cluster API, provider, hybrid, edge",
    "body": "Running on bare metal has both benefits and drawbacks. You can get thebest performance possible out of the hardware, but it can also be quiteexpensive and maybe not necessary for all workloads. Perhaps a hybridcluster could give you the best of both? Raw power for the workload thatneeds it, and cheap virtualized commodity for the rest. This blog postwill show how to set up a cluster like this using the Cluster API backedby the Metal3 and BYOH providers. The problem: Imagine that you have some bare metal servers that you want to use forsome specific workload. Maybe the workload benefits from the specifichardware or there are some requirements that make it necessary to run itthere. The rest of the organization already uses Kubernetes and thecluster API everywhere so of course you want the same for this as well. Perfect, grab Metal³ and start working! But hold on, this would mean that you use some of the servers forrunning the Kubernetes control plane and possibly all the cluster APIcontrollers. If there are enough servers this is probably not an issue,but do you really want to “waste” these servers on such genericworkloads that could be running anywhere? This can become especiallypainful if you need multiple control plane nodes. Each server isprobably powerful enough to run all the control planes and controllers,but it would be a single point of failure… What if there was a way to use a different cluster API infrastructureprovider for some nodes? For example, use the Openstack infrastructureprovider for the control plane and Metal³ for the workers. Let’s do anexperiment! Setting up the experiment environment: This blog post will use the Bring your ownhost(BYOH) provider together with Metal³ as a proof of concept to show whatis currently possible. The BYOH provider was chosen as the second provider for two reasons:  Due to its design (you provision the host yourself), it is very easyto adapt it to the test (e. g. use a VM in the same network that themetal3-dev-env uses).  It is one of the providers that is known to work when combiningmultiple providers for a single cluster. We will be using themetal3-dev-env on Ubuntuas a starting point for this experiment. Note that it makes substantialchanges to the machine where it is running, so you may want to use adedicated lab machine instead of your laptop for this. If you have notdone so already, clone it and run make. This should give you amanagement cluster with the Metal³ provider installed and twoBareMetalHosts ready for provisioning. The next step is to add the BYOH provider and a ByoHost. clusterctl init --infrastructure byohFor the ByoHost we will use Vagrant. You can install it with sudo apt install vagrant. Then copy the Vagrantfile below to a new folder and run vagrant up. # -*- mode: ruby -*-hosts = {   control-plane1  =&gt; {  memory  =&gt; 2048,  ip  =&gt;  192. 168. 10. 10 },  #  control-plane2  =&gt; {  memory  =&gt; 2048,  ip  =&gt;  192. 168. 10. 11 },  #  control-plane3  =&gt; {  memory  =&gt; 2048,  ip  =&gt;  192. 168. 10. 12 },}Vagrant. configure( 2 ) do |config|  # Choose which box you want below  config. vm. box =  generic/ubuntu2004   config. vm. synced_folder  .  ,  /vagrant , disabled: true  config. vm. provider :libvirt do |libvirt|   # QEMU system connection is required for private network configuration   libvirt. qemu_use_session = false  end  # Loop over all machine names  hosts. each_key do |host|    config. vm. define host, primary: host == hosts. keys. first do |node|      node. vm. hostname = host      node. vm. network :private_network, ip: hosts[host][ ip ],       libvirt__forward_mode:  route       node. vm. provider :libvirt do |lv|        lv. memory = hosts[host][ memory ]        lv. cpus = 2      end    end  endendVagrant should now have created a new VM to use as a ByoHost. Now wejust need to run the BYOH agent in the VM to make it register as aByoHost in the management cluster. The BYOH agent needs a kubeconfigfile to do this, so we start by copying it to the VM: cp ~/. kube/config ~/. kube/management-cluster. conf# Ensure that the correct IP is used (not localhost)export KIND_IP=$(docker inspect -f '{{range . NetworkSettings. Networks}}{{. IPAddress}}{{end}}' kind-control-plane)sed -i 's/  server\:. */  server\: https\:\/\/' $KIND_IP '\:6443/g' ~/. kube/management-cluster. confscp -i . vagrant/machines/control-plane1/libvirt/private_key \ /home/ubuntu/. kube/management-cluster. conf vagrant@192. 168. 10. 10:management-cluster. confNext, install the prerequisites and host agent in the VM and run it. vagrant sshsudo apt install -y socat ebtables ethtool conntrackwget https://github. com/vmware-tanzu/cluster-api-provider-bringyourownhost/releases/download/v0. 2. 0/byoh-hostagent-linux-amd64mv byoh-hostagent-linux-amd64 byoh-hostagentchmod +x byoh-hostagentsudo . /byoh-hostagent --namespace metal3 --kubeconfig management-cluster. confYou should now have a management cluster with both the Metal³ and BYOHproviders installed, as well as two BareMetalHosts and one ByoHost. $ kubectl -n metal3 get baremetalhosts,byohostsNAME               STATE    CONSUMER  ONLINE  ERROR  AGEbaremetalhost. metal3. io/node-0  available       true       18mbaremetalhost. metal3. io/node-1  available       true       18mNAME                           AGEbyohost. infrastructure. cluster. x-k8s. io/control-plane1  73sCreating a multi-provider cluster: The trick is to create both a Metal3Cluster and a ByoCluster that areowned by one common Cluster. We will use the ByoCluster for the controlplane in this case. First the Cluster: apiVersion: cluster. x-k8s. io/v1beta1kind: Clustermetadata: labels:  cni: mixed-cluster-crs-0  crs:  true  name: mixed-clusterspec: clusterNetwork:  pods:   cidrBlocks:    - 192. 168. 0. 0/16  serviceDomain: cluster. local  services:   cidrBlocks:    - 10. 128. 0. 0/12 controlPlaneRef:  apiVersion: controlplane. cluster. x-k8s. io/v1beta1  kind: KubeadmControlPlane  name: mixed-cluster-control-plane infrastructureRef:  apiVersion: infrastructure. cluster. x-k8s. io/v1beta1  kind: ByoCluster  name: mixed-clusterAdd the rest of the BYOH manifests to get a control plane. The code is collapsed here for easier reading. Please click on the line below to expand it.  KubeadmControlPlane, ByoCluster and ByoMachineTemplate      apiVersion: controlplane. cluster. x-k8s. io/v1beta1kind: KubeadmControlPlanemetadata: labels:  nodepool: pool0 name: mixed-cluster-control-planespec: kubeadmConfigSpec:  clusterConfiguration:   apiServer:    certSANs:     - localhost     - 127. 0. 0. 1     - 0. 0. 0. 0     - host. docker. internal   controllerManager:    extraArgs:     enable-hostpath-provisioner:  true   files:   - content: |     apiVersion: v1     kind: Pod     metadata:      creationTimestamp: null      name: kube-vip      namespace: kube-system     spec:      containers:      - args:       - start       env:       - name: vip_arp        value:  true        - name: vip_leaderelection        value:  true        - name: vip_address        value: 192. 168. 10. 20       - name: vip_interface        value: {{ . DefaultNetworkInterfaceName }}       - name: vip_leaseduration        value:  15        - name: vip_renewdeadline        value:  10        - name: vip_retryperiod        value:  2        image: ghcr. io/kube-vip/kube-vip:v0. 3. 5       imagePullPolicy: IfNotPresent       name: kube-vip       resources: {}       securityContext:        capabilities:         add:         - NET_ADMIN         - SYS_TIME       volumeMounts:       - mountPath: /etc/kubernetes/admin. conf        name: kubeconfig      hostNetwork: true      volumes:      - hostPath:        path: /etc/kubernetes/admin. conf        type: FileOrCreate       name: kubeconfig     status: {}    owner: root:root    path: /etc/kubernetes/manifests/kube-vip. yaml  initConfiguration:   nodeRegistration:    criSocket: /var/run/containerd/containerd. sock    ignorePreflightErrors:     - Swap     - DirAvailable--etc-kubernetes-manifests     - FileAvailable--etc-kubernetes-kubelet. conf  joinConfiguration:   nodeRegistration:    criSocket: /var/run/containerd/containerd. sock    ignorePreflightErrors:     - Swap     - DirAvailable--etc-kubernetes-manifests     - FileAvailable--etc-kubernetes-kubelet. conf machineTemplate:  infrastructureRef:   apiVersion: infrastructure. cluster. x-k8s. io/v1beta1   kind: ByoMachineTemplate   name: mixed-cluster-control-plane replicas: 1 version: v1. 23. 5---apiVersion: infrastructure. cluster. x-k8s. io/v1beta1kind: ByoClustermetadata: name: mixed-clusterspec: bundleLookupBaseRegistry: projects. registry. vmware. com/cluster_api_provider_bringyourownhost bundleLookupTag: v1. 23. 5 controlPlaneEndpoint:  host: 192. 168. 10. 20  port: 6443---apiVersion: infrastructure. cluster. x-k8s. io/v1beta1kind: ByoMachineTemplatemetadata: name: mixed-cluster-control-planespec: template:  spec: {}   So far this is a “normal” Cluster backed by the BYOH provider. But nowit is time to do something different. Instead of adding more ByoHosts asworkers, we will add a Metal3Cluster and MachineDeployment backed byBareMetalHosts! Note that the controlPlaneEndpoint of theMetal3Cluster must point to the same endpoint that the ByoCluster isusing. apiVersion: infrastructure. cluster. x-k8s. io/v1beta1kind: Metal3Clustermetadata: name: mixed-clusterspec: controlPlaneEndpoint:  host: 192. 168. 10. 20  port: 6443 noCloudProvider: true IPPools     apiVersion: ipam. metal3. io/v1alpha1kind: IPPoolmetadata: name: provisioning-poolspec: clusterName: mixed-cluster namePrefix: test1-prov pools:  - end: 172. 22. 0. 200   start: 172. 22. 0. 100 prefix: 24---apiVersion: ipam. metal3. io/v1alpha1kind: IPPoolmetadata: name: baremetalv4-poolspec: clusterName: mixed-cluster gateway: 192. 168. 111. 1 namePrefix: test1-bmv4 pools:  - end: 192. 168. 111. 200   start: 192. 168. 111. 100 prefix: 24   These manifests are quite large but they are just the same as would beused by the metal3-dev-env with some name changes here and there. Thekey thing to note is that all references to a Cluster are to the one wedefined above. Here is the MachineDeployment: apiVersion: cluster. x-k8s. io/v1beta1kind: MachineDeploymentmetadata: labels:  cluster. x-k8s. io/cluster-name: mixed-cluster  nodepool: nodepool-0 name: test1spec: clusterName: mixed-cluster replicas: 1 selector:  matchLabels:   cluster. x-k8s. io/cluster-name: mixed-cluster   nodepool: nodepool-0 template:  metadata:   labels:    cluster. x-k8s. io/cluster-name: mixed-cluster    nodepool: nodepool-0  spec:   bootstrap:    configRef:     apiVersion: bootstrap. cluster. x-k8s. io/v1beta1     kind: KubeadmConfigTemplate     name: test1-workers   clusterName: mixed-cluster   infrastructureRef:    apiVersion: infrastructure. cluster. x-k8s. io/v1beta1    kind: Metal3MachineTemplate    name: test1-workers   nodeDrainTimeout: 0s   version: v1. 23. 5Finally, we add the Metal3MachineTemplate, Metal3DataTemplate andKubeadmConfigTemplate. Here you may want to add your public ssh key inthe KubeadmConfigTemplate (the last few lines).  Metal3MachineTemplate, Metal3DataTemplate and KubeadmConfigTemplate      apiVersion: infrastructure. cluster. x-k8s. io/v1beta1kind: Metal3MachineTemplatemetadata: name: test1-workersspec: template:  spec:   dataTemplate:    name: test1-workers-template   image:    checksum: http://172. 22. 0. 1/images/UBUNTU_22. 04_NODE_IMAGE_K8S_v1. 23. 5-raw. img. md5sum    checksumType: md5    format: raw    url: http://172. 22. 0. 1/images/UBUNTU_22. 04_NODE_IMAGE_K8S_v1. 23. 5-raw. img---apiVersion: infrastructure. cluster. x-k8s. io/v1beta1kind: Metal3DataTemplatemetadata: name: test1-workers-template namespace: metal3spec: clusterName: mixed-cluster metaData:  ipAddressesFromIPPool:   - key: provisioningIP    name: provisioning-pool  objectNames:   - key: name    object: machine   - key: local-hostname    object: machine   - key: local_hostname    object: machine  prefixesFromIPPool:   - key: provisioningCIDR    name: provisioning-pool networkData:  links:   ethernets:    - id: enp1s0     macAddress:      fromHostInterface: enp1s0     type: phy    - id: enp2s0     macAddress:      fromHostInterface: enp2s0     type: phy  networks:   ipv4:    - id: baremetalv4     ipAddressFromIPPool: baremetalv4-pool     link: enp2s0     routes:      - gateway:        fromIPPool: baremetalv4-pool       network: 0. 0. 0. 0       prefix: 0  services:   dns:    - 8. 8. 8. 8---apiVersion: bootstrap. cluster. x-k8s. io/v1beta1kind: KubeadmConfigTemplatemetadata: name: test1-workersspec: template:  spec:   files:    - content: |      network:       version: 2       renderer: networkd       bridges:        ironicendpoint:         interfaces: [enp1s0]         addresses:         - {{ ds. meta_data. provisioningIP }}/{{ ds. meta_data. provisioningCIDR }}     owner: root:root     path: /etc/netplan/52-ironicendpoint. yaml     permissions:  0644     - content: |      [registries. search]      registries = ['docker. io']      [registries. insecure]      registries = ['192. 168. 111. 1:5000']     path: /etc/containers/registries. conf   joinConfiguration:    nodeRegistration:     kubeletExtraArgs:      cgroup-driver: systemd      container-runtime: remote      container-runtime-endpoint: unix:///var/run/crio/crio. sock      feature-gates: AllAlpha=false      node-labels: metal3. io/uuid={{ ds. meta_data. uuid }}      provider-id: metal3://{{ ds. meta_data. uuid }}      runtime-request-timeout: 5m     name:  {{ ds. meta_data. name }}    preKubeadmCommands:    - netplan apply    - systemctl enable --now crio kubelet   users:    - name: metal3     # sshAuthorizedKeys:     # - add your public key here for debugging     sudo: ALL=(ALL) NOPASSWD:ALL   The result of all this is a Cluster with two Machines, one from theMetal³ provider and one from the BYOH provider. $ k -n metal3 get machineNAME                CLUSTER     NODENAME        PROVIDERID                   PHASE   AGE   VERSIONmixed-cluster-control-plane-48qmm  mixed-cluster  control-plane1     byoh://control-plane1/jf5uye          Running  7m41s  v1. 23. 5test1-8767dbccd-24cl5        mixed-cluster  test1-8767dbccd-24cl5  metal3://0642d832-3a7c-4ce9-833e-a629a60a455c  Running  7m18s  v1. 23. 5Let’s also check that the workload cluster is functioning as expected. Get the kubeconfig and add Calico as CNI. clusterctl get kubeconfig -n metal3 mixed-cluster &gt; kubeconfig. yamlexport KUBECONFIG=kubeconfig. yamlkubectl apply -f https://docs. projectcalico. org/v3. 20/manifests/calico. yamlNow check the nodes. $ kubectl get nodesNAME          STATUS  ROLES         AGE  VERSIONcontrol-plane1     Ready  control-plane,master  88m  v1. 23. 5test1-8767dbccd-24cl5  Ready  &lt;none&gt;         82m  v1. 23. 5Going back to the management cluster, we can inspect the state of thecluster API resources. $ clusterctl -n metal3 describe cluster mixed-clusterNAME                                    READY SEVERITY REASON SINCE MESSAGECluster/mixed-cluster                            True           13m├─ClusterInfrastructure - ByoCluster/mixed-cluster├─ControlPlane - KubeadmControlPlane/mixed-cluster-control-plane      True           13m│ └─Machine/mixed-cluster-control-plane-hp2fp                True           13m│  └─MachineInfrastructure - ByoMachine/mixed-cluster-control-plane-vxft5└─Workers └─MachineDeployment/test1                         True           3m57s  └─Machine/test1-7f77dfb7c8-j7x4q                    True           9m32sConclusion: As we have seen in this post, it is possible to combine at least someinfrastructure providers when creating a single cluster. This can beuseful for example if a provider has a high cost or limited resources. Furthermore, the use case is not addressed by MachineDeployments sincethey would all be from the same provider (even though they can havedifferent properties). There is some room for development and improvement though. The mostobvious thing is perhaps that Clusters only have oneinfrastructureRef. This means that the cluster API controllers are notaware of the “secondary” infrastructure provider(s). Another thing that may be less obvious is the reliance on Nodes andMachines in the Kubeadm control plane provider. It is not an issue inthe example we have seen here since both Metal³ and BYOH creates Nodes. However, there are some projects where Nodes are unnecessary. See forexample Kamaji, which aims tointegrate with the cluster API. The idea here is to run the controlplane components in the management cluster as Pods. Naturally, therewould not be any control plane Nodes or Machines in this case. (A secondprovider would be used to add workers. ) But the Kubeadm control planeprovider expects there to be both Machines and Nodes for the controlplane, so a new provider is likely needed to make this work as desired. This issue can already be seen in thevclusterprovider, where the Cluster stays in Provisioning state because it is“Waiting for the first control plane machine to have itsstatus. nodeRef set”. The idea with vcluster is to reuse the Nodes ofthe management cluster but provide a separate control plane. This givesusers better isolation than just namespaces without the need for another“real” cluster. It is for example possible to have different customresource definitions in each vcluster. But since vcluster runs all thepods (including the control plane) in the management cluster, there willnever be a control plane Machine or nodeRef. There is already one implementation of a control plane provider withoutNodes, i. e. the EKS provider. Perhaps this is the way forward. Oneimplementation for each specific case. It would be nice if it waspossible to do it in a more generic way though, similar to how theKubeadm control plane provider is used by almost all infrastructureproviders. To summarize, there is already some support for mixed clusters withmultiple providers. However, there are some issues that make itunnecessarily awkward. Two things that could be improved in the clusterAPI would be the following:  Make the cluster. infrastructureRef into a list to allow multipleinfrastructure providers to be registered.  Drop the assumption that there will always be control plane Machinesand Nodes (e. g. by implementing a new control plane provider). "
    }, {
    "id": 9,
    "url": "/blog/2021/05/05/Pivoting.html",
    "title": "Metal3 Introduces Pivoting",
    "author" : "Kashif Nizam Khan",
    "tags" : "metal3, baremetal, Pivoting, Move",
    "body": "Metal3 project has introduced pivoting in its CI workflow. The motivation forpivoting is to move all the objects from the ephemeral/managementcluster to a target cluster. This blog post will briefly introduce the conceptof pivoting and the impact it has on the overall CI workflow. For the rest ofthis blog, we refer ephemeral/management cluster as an ephemeral cluster. What is Pivoting?: In the context of Metal3 Provider, Pivoting is the process of movingCluster-API and Metal3 objects from the ephemeral k8s cluster to a targetcluster. In Metal3, this process is performed using theclusterctl toolprovided by Cluster-API. clusterctl recognizes pivoting as a move. During thepivot process, clusterctl pauses any reconciliation of Cluster-API objects andthis gets propagated to Cluster-api-provider-metal3 (CAPM3) objects as well. Once all the objects are paused, the objects are created on the other side onthe target cluster and deleted from the ephemeral cluster. Prerequisites: Prior to the actual pivot process, the target cluster should already have theprovider components, ironic containers and CNI installed and running. To performpivot outside metal3-dev-env, specifically, the following points need to beaddressed:  clusterctl is used to initialize both the ephemeral and target cluster.  BMH objects have correct status annotation.  Maintain connectivity towards the provisioning network.  Baremetal Operator(BMO) is deployed as part of CAPM3.  Objects should have a proper owner reference chain. For a detailed explanation of the above-mentioned prerequisites please read thepivoting documentation. Pivoting workflow in CI: The Metal3 CI currently includes pivoting as part of the deploymentprocess both for Ubuntu and CentOS-based jobs. This essentially means allthe PRs that go in, are tested through the pivoting workflow. Here is theCI deployment workflow:  make the metal3-dev-env. It gives us the ephemeral cluster with all the necessary controllers runningwithin it. The corresponding metal3-dev-env command is make provision target cluster. For normal integration tests, this step deploysa control-plane node and a worker in the target cluster. For, feature-testand feature-test-upgrade the provision step deploys three control-planes anda worker. The corresponding metal3-dev-env commands are (normal integrationtest workflow):. /scripts/provision/cluster. sh. /scripts/provision/controlplane. sh. /scripts/provision/worker. sh Initialize the provider components on the target cluster. This installs allthe controllers and associated components related to cluster-api ,cluster-api-provider-metal3, baremetal-operator and ironic. Since it isnecessary to have only one set of ironic deployment/containers in the picture,this step also deletes the ironic deployment/containers fromephemeral cluster.  Move all the objects from ephemeral to the target cluster.  Check the status of the objects to verify whether the objects are beingreconciled correctly by the controllers in the target cluster. This stepverifies and finalizes the pivoting process. The corresponding metal3-dev-envthe command that performs this and the previous two steps is :. /scripts/feature_tests/pivoting/pivot. sh Move the objects back to the ephemeral cluster. This step alsoremoves the ironic deployment from the target cluster and reinstates theironic deployment/containers in the ephemeral cluster. Since we donot delete the provider components in the ephemeral cluster,installing them again is not necessary. The corresponding metal3-dev-env commandthat performs this step is :. /scripts/feature_tests/pivoting/repivot. sh De-provision the BMHs and delete the target cluster. The correspondingmetal3-dev-env commands to de-provision worker, controlplane and the clusteris as follows:. /scripts/deprovision/worker. sh. /scripts/deprovision/controlplane. sh. /scripts/deprovision/cluster. shNote that, if we de-provision cluster, that would de-provision worker andcontrolplane automatically. Pivoting in Metal3: The pivoting process described above is realized in ansible scriptsmove. ymlandmove_back. yml. Under the hood, pivoting uses the move command fromclusterctlprovided by Cluster-API. As stated earlier, all the PRs that go into any Metal3 repository where theintegration tests are run, the code change introduced in the PR is verified withpivoting also in the integration tests now. Moreover, the upgrade workflow inMetal3 performs all the upgrade operations in Metal3 after pivoting to thetarget cluster. "
    }, {
    "id": 10,
    "url": "/blog/2020/07/06/IP_address_manager.html",
    "title": "Introducing the Metal3 IP Address Manager",
    "author" : "Maël Kimmerlin",
    "tags" : "metal3, baremetal, IPAM, ip address manager",
    "body": "As a part of developing the Cluster API Provider Metal3 (CAPM3) v1alpha4release, the Metal3 crew introduced a new project: its own IP Address Manager. This blog post will go through the motivations behind such a project, thefeatures that it brings, its use in Metal3 and future work. What is the IP Address Manager?: The IP Address Manager (IPAM) is a controller that provides IP addresses andmanages the allocations of IP subnets. It is not a DHCP server in that it onlyreconciles Kubernetes objects and does not answer any DHCP queries. Itallocates IP addresses on request but does not handle any use of thoseaddresses. This sounds like the description of any IPAM system, no? Well, the twistis that this manager is based on Kubernetes to specifically handle someconstraints from Metal3. We will go through the different issues that thisproject tackles. When deploying nodes in a bare metal environment, there are a lot of possiblevariations. This project specifically aims to solve cases where staticIP address configurations are needed. It is designed to specifically addressthis in the Cluster API (CAPI) context. CAPI addresses the deployment of Kubernetes clusters and nodes, usingthe Kubernetes API. As such, it uses objects such as Machine Deployments(similar to deployments for pods) that takes care of creating the requestednumber of machines, based on templates. The replicas can be increased by theuser, triggering the creation of new machines based on the provided templates. This mechanism does not allow for flexibility to be able to provide staticaddresses for each machine. The manager adds this flexibility by providingthe address right before provisioning the node. In addition, all the resources from the source cluster must support the CAPIpivoting, i. e. being copied and recreated in the target cluster. This meansthat all objects must contain all needed information in their spec field torecreate the status in the target cluster without losing information. Allobjects must, through a tree of owner references, be attached to the clusterobject, for the pivoting to proceed properly. In a nutshell, the manager provides an IP Address allocation service, basedon Kubernetes API and fulfilling the needs of Metal3, specifically therequirements of CAPI. How does it work?: The manager follows the same logic as the volume allocation in Kubernetes,with a claim and an object created for that claim. There are three types ofobjects defined, the IPPool, the IPClaim and the IPAddress objects. The IPPool objects contain the definition of the IP subnets from which theAddresses are allocated. It supports both IPv4 and IPv6. The subnets can eitherbe defined as such or given as start and end IP addresses with a prefix. It also supports pre-allocating IP addresses. The following is an example IPPool definition : apiVersion: ipam. metal3. io/v1alpha1kind: IPPoolmetadata: name: pool1spec: clusterName: cluster1 pools:  - start: 192. 168. 0. 10   end: 192. 168. 0. 30   prefix: 25   gateway: 192. 168. 0. 1  - subnet: 192. 168. 1. 1/26  - subnet: 192. 168. 1. 128/25 prefix: 24 gateway: 192. 168. 1. 1 preAllocations:  claim2: 192. 168. 0. 12An IPv6 IPPool would be defined similarly : apiVersion: ipam. metal3. io/v1alpha1kind: IPPoolmetadata: name: pool1spec: clusterName: cluster1 pools:  - start: 2001:0db8:85a3:0000:0000:8a2e::10   end: 2001:0db8:85a3:0000:0000:8a2e:ffff:fff0   prefix: 96   gateway: 12001:0db8:85a3:0000:0000:8a2e::1  - subnet: 2001:0db8:85a3:0000:0000:8a2d::/96 prefix: 96 gateway: 2001:0db8:85a3:0000:0000:8a2d::1Whenever something requires an IP address from the IPPool, it will create anIPClaim. The IPClaim contains a pointer to the IPPool and an owner referenceto the object that created it. The following is an example of an IPClaim: apiVersion: ipam. metal3. io/v1alpha1kind: IPClaimmetadata: name: claim1spec: pool:  Name: pool1status: address:  Name: pool1-192-168-0-13The controller will then reconcile this object and allocate an IP address. Itwill create an IPAddress object representing the allocated address. It willthen update the IPPool status to list the IP Address and the IPClaim statusto point to the IPAddress. The following is an example of an IPAddress: apiVersion: ipam. metal3. io/v1alpha1kind: IPAddressmetadata: name: pool1-192-168-0-13spec: pool:  Name: pool1 claim:  Name: claim1 address: 192. 168. 0. 13 prefix: 24 gateway: 192. 168. 0. 1After this allocation, the IPPool will be looking like this: apiVersion: ipam. metal3. io/v1alpha1kind: IPPoolmetadata: name: pool1spec: clusterName: cluster1 pools:  - start: 192. 168. 0. 10   end: 192. 168. 0. 30   prefix: 25   gateway: 192. 168. 0. 1  - subnet: 192. 168. 1. 1/26  - subnet: 192. 168. 1. 128/25 prefix: 24 gateway: 192. 168. 1. 1 preAllocations:  claim2: 192. 168. 0. 12status: indexes:  claim1: 192. 168. 0. 13  claim2: 192. 168. 0. 12Use in Metal3: The IP Address Manager is used in Metal3 together with the metadata and networkdata templates feature. Each Metal3Machine (M3M) and Metal3MachineTemplate(M3MT) is associated with a Metal3DataTemplate that contains metadata and /or a network data template that will be rendered for each Metal3Machine. Therendered data will then be provided to Ironic. Those templates referenceIPPool objects. For each Metal3Machine, an IPClaim is created for eachIPPool, and the templates are rendered with the allocated IPAddress. This is how we achieve dynamic IP Address allocations in setups thatrequire static configuration, allowing us to use Machine Deployment and KubeadmControl Plane objects from CAPI in hardware labs where DHCP is not supported. Since each IPAddress has an owner reference set to its IPClaim object, andIPClaim objects have an owner reference set to the Metal3Data object createdfrom the Metal3DataTemplate, the owner reference chain links a Metal3Machine toall the IPClaim and IPAddress objects were created for it, allowing for CAPIpivoting. What now?: The project is fulfilling its basic requirements, but we are looking intoextending it and covering more use cases. For example, we are looking atadding integration with Infoblox and other external IPAM services. Do nothesitate to open an issue if you have some ideas for new features! The project can be foundhere. "
    }, {
    "id": 11,
    "url": "/blog/2020/07/05/raw-image-streaming.html",
    "title": "Raw image streaming available in Metal3",
    "author" : "Maël Kimmerlin",
    "tags" : "metal3, baremetal, raw image, image streaming",
    "body": "Metal3 supports multiple types of images for deployment, the mostpopular being QCOW2. We have recently added support for a feature of Ironicthat improves deployments on constrained environments, raw image streaming. We’ll first dive into how Ironic deploys the images on the target hosts, andhow raw image streaming improves this process. Afterwards, we will point outthe changes to take this into use in Metal3. Image deployments with Ironic: In Metal3, the image deployment is performed by the Ironic Python Agent (IPA)image running on the target host. In order to deploy an image, Ironic willfirst boot the target node with an IPA image over iPXE. IPA will run in memory. Once IPA runs on the target node, Ironic will instruct it to download thetarget image. In Metal3, we use HTTP(S) for the download of the image. IPA willdownload the image and, depending on the format of the image, prepare it towrite on the disk. This means that the image is downloaded in memory anddecompressed, two steps that can be both time and memory consuming. In order to improve this process, Ironic implemented a feature called raw imagestreaming. What is raw image streaming?: The target image format when writing to disk is raw. That’s why the images informats like QCOW2 must be processed before being written to disk. However, ifthe image that is downloaded is already in raw format, then no processing isneeded. Ironic leverages this, and instead of first downloading the image and thenprocessing it before writing it to disk, it will directly write thedownloaded image to the disk. This feature is known as image streaming. Image streaming can only be performed with images in raw format. Since the downloaded image when streamed is directly written to disk, thememory size requirements change. For any other format than raw, the targethost needs to have sufficient memory to both run IPA (4GB) anddownload the image in memory. However, with raw images, the only constrainton memory is to run IPA (so 4GB). For example, in order to deploy an Ubuntuimage (around 700MB, QCOW2), the requirement is 8GB when in QCOW2 format, whileit is only 4GB (as for any other image) when streamed as raw. This allowsthe deployment of images that are bigger than the available memory onconstrained nodes. However, this shifts the load on the network, since the raw images are usuallymuch bigger than other formats. Using this feature in network constrainedenvironment is not recommended. Raw image streaming in Metal3: In order to use raw image streaming in Metal3, a couple of steps are needed. The first one is to convert the image to raw and make it available in anHTTP server. This can be achieved by running : qemu-img convert -O raw  ${IMAGE_NAME}   ${IMAGE_RAW_NAME} Once converted the image format needs to be provided to Ironic through theBareMetalHost (BMH) image spec field. If not provided, Ironic will assume thatthe format is unspecified and download it in memory first. The following is an example of the BMH image spec field in Metal3 Dev Env. apiVersion: metal3. io/v1alpha1kind: BareMetalHostspec: image:  format: raw  url: http://172. 22. 0. 1/images/bionic-server-cloudimg-amd64-raw. img  checksum: http://172. 22. 0. 1/images/bionic-server-cloudimg-amd64-raw. img. md5sum  checksumType: md5If deploying with Cluster API provider Metal3 (CAPM3), CAPM3 takes care ofsetting the image field of BMH properly, based on the image field values inthe Metal3Machine (M3M), which might be based on a Metal3MachineTemplate (M3MT). So in order to use raw image streaming, the format of the image must beprovided in the image spec field of the Metal3Machine or Metal3MachineTemplate. The following is an example of the M3M image spec field in metal3-dev-env : apiVersion: infrastructure. cluster. x-k8s. io/v1alpha3kind: Metal3Machinespec: image:  format: raw  url: http://172. 22. 0. 1/images/bionic-server-cloudimg-amd64-raw. img  checksum: http://172. 22. 0. 1/images/bionic-server-cloudimg-amd64-raw. img. md5sum  checksumType: md5The following is for a M3MT in metal3-dev-env : apiVersion: infrastructure. cluster. x-k8s. io/v1alpha3kind: Metal3MachineTemplatespec: template:  spec:   image:    format: raw    url: http://172. 22. 0. 1/images/bionic-server-cloudimg-amd64-raw. img    checksum: http://172. 22. 0. 1/images/bionic-server-cloudimg-amd64-raw. img. md5sum    checksumType: md5This will enable raw image streaming. By default, metal3-dev-env uses the raw imagestreaming, in order to minimize the resource requirements of the environment. In a nutshell: With the addition of raw image streaming, Metal3 now supports a wider range ofhardware, specifically, the memory-constrained nodes and speeds up deployments. Metal3 still supports all the other formats it supported until now. This newfeature changes the way raw images are deployed for better efficiency. "
    }, {
    "id": 12,
    "url": "/blog/2020/06/18/Metal3-dev-env-BareMetal-Cluster-Deployment.html",
    "title": "Metal³ development environment walkthrough part 2: Deploying a new bare metal cluster",
    "author" : "Himanshu Roy",
    "tags" : "metal3, kubernetes, cluster API, metal3-dev-env",
    "body": "Introduction: This blog post describes how to deploy a bare metal cluster, a virtualone for simplicity, usingMetal³/metal3-dev-env. Wewill briefly discuss the steps involved in setting up the cluster aswell as some of the customization available. If you want to know moreabout the architecture of Metal³, this blogpost can be helpful. This post builds upon the detailed metal3-dev-env walkthroughblogpostwhich describes in detail the steps involved in the environment set-upand management cluster configuration. Here we will use that environmentto deploy a new Kubernetes cluster using Metal³. Before we get started, there are a couple of requirements we areexpecting to be fulfilled. Requirements:  Metal³ is already deployed and working, if not please follow theinstructions in the previously mentioned detailed metal3-dev-envwalkthrough blogpost.  The appropriate environment variables are setup via shell or in theconfig_${user}. sh file, for example     CAPM3_VERSION   NUM_NODES   CLUSTER_NAME   Overview of Config and Resource types: In this section, we give a brief overview of the important config filesand resources used as part of the bare metal cluster deployment. Thefollowing sub-sections show the config files and resources that arecreated and give a brief description of some of them. This will help youunderstand the technical details of the cluster deployment. You can alsochoose to skip this section, visit the next section about provisioningfirst and then revisit this. Config Files and Resources Types:  info “Information” Among these the config files are rendered under thepathhttps://github. com/metal3-io/metal3-dev-env/tree/master/vm-setup/roles/v1aX_integration_test/filesas part of the provisioning process. A description of some of the files part of provisioning a cluster, in acentos-based environment:       Name   Description   Path         provisioning scripts   Scripts to trigger provisioning of cluster, control plane or worker   ${metal3-dev-env}/scripts/provision/       deprovisioning scripts   Scripts to trigger deprovisioning of cluster, control plane or worker   ${metal3-dev-env}/scripts/deprovision/       templates directory   Templates for cluster, control plane, worker definitions   ${metal3-dev-env}/tests/roles/run_tests/templates       clusterctl env file   Cluster parameters and details   ${Manifests}/clusterctl_env_centos. rc       generate templates   Renders cluster, control plane and worker definitions in the Manifest directory   ${metal3-dev-env}/tests/roles/run_tests/tasks/generate_templates. yml       main vars file   Variable file that assigns all the defaults used during deployment   ${metal3-dev-env}/tests/roles/run_tests/vars/main. yml   Here are some of the resources that are created as part of provisioning :       Name   Description         Cluster   a Cluster API resource for managing a cluster       Metal3Cluster   Corresponding Metal3 resource generated as part of bare metal cluster deployment, and managed by Cluster       KubeadmControlPlane   Cluster API resource for managing the control plane, it also manages the Machine object, and has the KubeadmConfig       MachineDeployment   Cluster API resource for managing workers via MachineSet object, it can be used to add/remove workers by scaling Up/Down       MachineSet   Cluster API resource for managing Machine objects for worker nodes       Machine   Cluster API resource for managing nodes - control plane or workers. In case of Controlplane, its directly managed by KubeadmControlPlane, whereas for Workers it’s managed by a MachineSet       Metal3Machine   Corresponding Metal3 resource for managing bare metal nodes, it’s managed by a Machine resource       Metal3MachineTemplate   Metal3 resource which acts as a template when creating a control plane or a worker node       KubeadmConfigTemplate   A template of KubeadmConfig, for Workers, used to generate KubeadmConfig when a new worker node is provisioned   Note The corresponding KubeadmConfig is copied to the controlplane/worker at the time of provisioning. Bare Metal Cluster Deployment: The deployment scripts primarily use ansible and the existing Kubernetesmanagement cluster (based on minikube ) for deploying the bare-metalcluster. Make sure that some of the environment variables used forMetal³ deployment are set, if you didn’t use config_${user}. sh forsetting the environment variables.       Parameter   Description   Default         CAPM3_VERSION   Version of Metal3 API   v1alpha3       POD_CIDR   Pod Network CIDR   192. 168. 0. 0/18       CLUSTER_NAME   Name of bare metal cluster   test1   === Steps Involved: All the scripts for cluster provisioning or de-provisioning are locatedat -${metal3-dev-env}/scripts/. The scripts call a common playbook which handles all the tasks that areavailable. The steps involved in the process are:  The script calls an ansible playbook with necessary parameters ( fromenv variables and defaults ) The playbook executes the role -,${metal3-dev-env}/tests/roles/run_tests,which runs the maintask_filefor provisioning/deprovisioning the cluster, control plane or a worker There aretemplatesin the role, which are used to render configurations in the Manifestdirectory. These configurations use kubeadm and are supplied to theKubernetes module of ansible to create the cluster.  During provisioning, first the clusterctl env file is generated,then the cluster, control plane and worker definition templates forclusterctl are generated at${HOME}/. cluster-api/overrides/infrastructure-metal3/${CAPM3RELEASE}.  Using the templates generated in the previous step, the definitionsfor resources related to cluster, control plane and worker arerendered using clusterctl.  Centos or Ubuntu image isdownloadedin the next step.  Finally using the above definitions, which are passed to the K8smodule in ansible, the corresponding resource( cluster/controlplane/worker ) is provisioned.  These same definitions are reused at the time of de-provisioning thecorresponding resource, again using the K8s module in ansible     note “Note” The manifest directory is created when provisioning istriggered for the first time and is subsequently used to store theconfig files that are rendered for deploying the bare metal cluster.     Provision Cluster: This script, located at the path -${metal3-dev-env}/scripts/provision/cluster. sh, provisions the clusterby creating a Metal3Cluster and a Cluster resource. To see if you have a successful Cluster resource creation( the clusterstill doesn’t have a control plane or workers ), just do: kubectl get Metal3Cluster ${CLUSTER_NAME} -n metal3 This will return the cluster deployed, and you can check the clusterdetails by describing the returned resource. Here is what a Cluster resource looks like: kubectl describe Cluster ${CLUSTER_NAME} -n metal3apiVersion: cluster. x-k8s. io/v1alpha3kind: Clustermetadata: [. . . . . . ]spec: clusterNetwork:  pods:   cidrBlocks:   - 192. 168. 0. 0/18  services:   cidrBlocks:   - 10. 96. 0. 0/12 controlPlaneEndpoint:  host: 192. 168. 111. 249  port: 6443 controlPlaneRef:  apiVersion: controlplane. cluster. x-k8s. io/v1alpha3  kind: KubeadmControlPlane  name: bmetalcluster  namespace: metal3 infrastructureRef:  apiVersion: infrastructure. cluster. x-k8s. io/v1alpha3  kind: Metal3Cluster  name: bmetalcluster  namespace: metal3status: infrastructureReady: true phase: ProvisionedProvision Controlplane: This script, located at the path -${metal3-dev-env}/scripts/provision/controlplane. sh, provisions thecontrol plane member of the cluster using the rendered definition of thecontrol plane explained in the Steps Involved section. TheKubeadmControlPlane creates a Machine which picks up a BareMetalHostsatisfying its requirements as the control plane node, and it is thenprovisioned by the Bare Metal Operator. A Metal3MachineTemplateresource is also created as part of the provisioning process. Note It takes some time for the provisioning of the control plane, you canwatch the process using some steps shared a bit later kubectl get KubeadmControlPlane ${CLUSTER_NAME} -n metal3kubectl describe KubeadmControlPlane ${CLUSTER_NAME} -n metal3apiVersion: controlplane. cluster. x-k8s. io/v1alpha3kind: KubeadmControlPlanemetadata: [. . . . ] ownerReferences: - apiVersion: cluster. x-k8s. io/v1alpha3  blockOwnerDeletion: true  controller: true  kind: Cluster  name: bmetalcluster  uid: aec0f73b-a068-4992-840d-6330bf943d22 resourceVersion:  44555  selfLink: /apis/controlplane. cluster. x-k8s. io/v1alpha3/namespaces/metal3/kubeadmcontrolplanes/bmetalcluster uid: 99487c75-30f1-4765-b895-0b83b0e5402bspec: infrastructureTemplate:  apiVersion: infrastructure. cluster. x-k8s. io/v1alpha3  kind: Metal3MachineTemplate  name: bmetalcluster-controlplane  namespace: metal3 kubeadmConfigSpec:  files:  - content: |    [. . . . ] replicas: 1 version: v1. 18. 0status: replicas: 1 selector: cluster. x-k8s. io/cluster-name=bmetalcluster,cluster. x-k8s. io/control-plane= unavailableReplicas: 1 updatedReplicas: 1kubectl get Metal3MachineTemplate ${CLUSTER_NAME}-controlplane -n metal3To track the progress of provisioning, you can try the following: kubectl get BareMetalHosts -n metal3 -w The BareMetalHosts resource is created when Metal³/metal3-dev-envwas deployed. It is a kubernetes resource that represents a bare metalMachine, with all its details and configuration, and is managed by theBare Metal Operator. You can also use the short representationinstead, i. e. bmh ( short for BareMetalHosts) in the commandabove. You should see all the nodes that were created at the time of metal3deployment, along with their current status as the provisioningprogresses Note All the bare metal hosts listed above were created when Metal³ wasdeployed in the detailed metal3-dev-env walkthrough blogpost. kubectl get Machine -n metal3 -w This shows the status of the Machine associated with the control planeand we can watch the status of provisioning under PHASE Once the provisioning is finished, let’s get the host-ip: sudo virsh net-dhcp-leases baremetalInformation baremetal is one of the 2 networks that were created at the time ofMetal3 deployment, the other being “provisioning” which is used - asyou have guessed - for provisioning the bare metal cluster. Moredetails about networking setup in the metal3-dev-env environment aredescribed in the - detailed metal3-dev-env walkthroughblogpost. You can log in to the control plane node if you want, and can check thedeployment status using two methods. ssh metal3@{control-plane-node-ip}ssh metal3@192. 168. 111. 249Provision Workers: The script is located at${metal3-dev-env-path}/scripts/provision/worker. sh and it provisions anode to be added as a worker to the bare metal cluster. It selects oneof the remaining nodes and provisions it and adds it to the bare metalcluster ( which only has a control plane node at this point ). Theresources created for workers are - MachineDeployment which can bescaled up to add more workers to the cluster and MachineSet which thencreates a Machine managing the node. Information Similar to control plane provisioning, worker provisioning also takessome time, and you can watch the process using steps shared a bitlater. This will also apply when you scale Up/Down workers at a laterpoint in time. This is what a MachineDeployment looks like kubectl describe MachineDeployment ${CLUSTER_NAME} -n metal3apiVersion: cluster. x-k8s. io/v1alpha3kind: MachineDeploymentmetadata: [. . . . ] ownerReferences: - apiVersion: cluster. x-k8s. io/v1alpha3  kind: Cluster  name: bmetalcluster  uid: aec0f73b-a068-4992-840d-6330bf943d22 resourceVersion:  66257  selfLink: /apis/cluster. x-k8s. io/v1alpha3/namespaces/metal3/machinedeployments/bmetalcluster uid: f598da43-0afe-44e4-b793-cd5244c13f4espec: clusterName: bmetalcluster minReadySeconds: 0 progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 1 selector:  matchLabels:   cluster. x-k8s. io/cluster-name: bmetalcluster   nodepool: nodepool-0 strategy:  rollingUpdate:   maxSurge: 1   maxUnavailable: 0  type: RollingUpdate template:  metadata:   labels:    cluster. x-k8s. io/cluster-name: bmetalcluster    nodepool: nodepool-0  spec:   bootstrap:    configRef:     apiVersion: bootstrap. cluster. x-k8s. io/v1alpha3     kind: KubeadmConfigTemplate     name: bmetalcluster-workers   clusterName: bmetalcluster   infrastructureRef:    apiVersion: infrastructure. cluster. x-k8s. io/v1alpha3    kind: Metal3MachineTemplate    name: bmetalcluster-workers   version: v1. 18. 0status: observedGeneration: 1 phase: ScalingUp replicas: 1 selector: cluster. x-k8s. io/cluster-name=bmetalcluster,nodepool=nodepool-0 unavailableReplicas: 1 updatedReplicas: 1To check the status we can follow steps similar to Controlplane case: kubectl get bmh -n metal3 -w We can see the live status of the node being provisioned. As mentionedbefore bmh is the short representation of BareMetalHosts. kubectl get Machine -n metal3 -w This shows the status of Machines associated with workers, apart fromthe one for Controlplane, and we can watch the status of provisioningunder PHASE sudo virsh net-dhcp-leases baremetal To get the node’s IP ssh metal3@{control-plane-node-ip}kubectl get nodes To check if it’s added to the cluster ssh metal3@{node-ip} If you want to log in to the node kubectl scale --replicas=3 MachineDeployment ${CLUSTER_NAME} -n metal3 We can add or remove workers to the cluster, and we can scale up theMachineDeployment up or down, in this example we are adding 2 moreworker nodes, making the total nodes = 3 Deprovisioning: All of the previous components have corresponding de-provisioningscripts which use config files, in the previously mentioned manifestdirectory, and use them to clean up the worker, control plane andcluster. This step will use the already generated cluster/control plane/workerdefinition file, and supply it to Kubernetes ansible module toremove/de-provision the resource. You can find it, under the Manifestdirectory, in the Snapshot shared at the beginning of this blogpostwhere we show the file structure. For example, if you wish to de-provision the cluster, you would do: sh ${metal3-dev-env-path}/scripts/deprovision/worker. shsh ${metal3-dev-env-path}/scripts/deprovision/controlplane. shsh ${metal3-dev-env-path}/scripts/deprovision/cluster. shNote The reason for running the deprovision/worker. sh anddeprovision/controlplane. sh scripts is that not all objects arecleared when we just run the deprovision/cluster. sh script. Following this, if you want to de-provision the control plane it isrecommended to de-provision the cluster itself since we can’tprovision a new control plane with the same cluster. For workerde-provisioning, we only need to run the worker script. The following video demonstrates all the steps to provision andde-provision a Kubernetes cluster explained above. Summary: In this blogpost we saw how to deploy a bare metal cluster once we havea Metal³(metal3-dev-env repo) deployed and by that point we will alreadyhave the nodes ready to be used for a bare metal cluster deployment. In the first section, we show the various configuration files,templates, resource types and their meanings. Then we see the commonsteps involved in the provisioning process. After that, we see a generaloverview of how all resources are related and at what point are theycreated - provision cluster/control plane/worker. In each of the provisioning sections, we see the steps to monitor theprovisioning and how to confirm if it’s successful or not, with briefexplanations wherever required. Finally, we see the de-provisioningsection which uses the resource definitions generated at the time ofprovisioning to de-provision cluster, control plane or worker. Here are a few resources which you might find useful if you want toexplore further, some of them have already been shared earlier.  Metal3-Documentation     Metal3-Try-it    Metal³/metal3-dev-env Detailed metal3-dev-env walkthrough blogpost Kubernetes Metal3 Talk Metal3-Docs-github"
    }, {
    "id": 13,
    "url": "/blog/2020/03/05/CAPI_provider_renaming.html",
    "title": "Cluster API provider renaming",
    "author" : "Maël Kimmerlin",
    "tags" : "metal3, baremetal, cluster API, provider",
    "body": "Renaming of Cluster API provider: Backwards compatibility for v1alpha3 There is no backwards compatibility between v1alpha3 and v1alpha2 releases ofthe Cluster API provider for Metal3. For the v1alpha3 release of Cluster API, the Metal3 provider was renamed fromcluster-api-provider-baremetal to cluster-api-provider-metal3. The CustomResource Definitions were also modified. This post dives into the changes. Repository renaming: From v1alpha3 onwards, the Cluster API provider will be developed incluster-api-provider-metal3. The v1alpha1 and v1alpha2 content will remain incluster-api-provider-baremetal. This repository will be archived but kept for the integration in metal3-dev-env. Custom Resource Definition modifications: The kind of Custom Resource Definition (CRD) has been modified for thefollowing objects:  BareMetalCluster -&gt; Metal3Cluster baremetalcluster -&gt; metal3cluster BareMetalMachine -&gt; Metal3Machine baremetalmachine -&gt; metal3machine BareMetalMachineTemplate -&gt; Metal3MachineTemplate baremetalmachinetemplate -&gt; metal3machinetemplateThe custom resources deployed need to be modified accordingly. Deployment modifications: The prefix of all deployed components for the Metal3 provider was modifiedfrom capbm- to capm3-. The namespace in which the components are deployed bydefault was modified from capbm-system to capm3-system. "
    }, {
    "id": 14,
    "url": "/blog/2020/02/27/talk-kubernetes-finland-metal3.html",
    "title": "Metal³: Kubernetes Native Bare Metal Cluster Management - Maël Kimmerlin - Kubernetes and CNCF Finland Meetup",
    "author" : "Alberto Losada",
    "tags" : "metal3, baremetal, talk, conference, kubernetes, meetup",
    "body": "Conference talk: Metal³: Kubernetes Native Bare Metal Cluster Management - Maël Kimmerlin: On the 20th of January at the Kubernetes and CNCF Finland Meetup, Maël Kimmerlin gave a brilliant presentation about the status of the Metal³ project. In this presentation, Maël starts giving a short introduction of the Cluster API project which provides a solid foundation to develop the Metal³ Bare Metal Operator (BMO). The talk basically focuses on the v1alpha2 infrastructure provider features from the Cluster API. Information The video recording from the “Kubernetes and CNCF Finland Meetup” is composed of three talks. The video embedded starts with Maël’s talk. warning “Warning”Playback of the video has been disabled by the author. Click on the play button and then on the “Watch this video on Youtube” link once it appears. During the first part of the presentation, a detailed explanation of the different Kubernetes Custom Resource Definitions (CRDs) inside Metal³ is shown as also how they are linked with the Cluster API project. As an example, the image below shows the interaction between objects and controllers from both projects: Once finished the introductory part, Maël focuses on the main components of the Metal³ BMO and the provisioning process. This process starts with introspection, where the bare metal server is registered by the operator. Then, the Ironic Python Agent (IPA) image is executed to collect all hardware information from the server.  The second part of the process is the provisioning. In this step, Maël explains how the Bare Metal Operator (BMO) is in charge along with Ironic to present the Operating System image to the physical server and complete its installation.  Next, Maël deeply explains each Custom Resource (CR) used during the provisioning of target Kubernetes clusters in bare metal servers. He refers to objects such as Cluster, BareMetalCluster, Machine, BareMetalMachine, BareMetalHost and so on. Each one is clarified with a YAML file definition of a real case and a workflow diagram that shows the reconciliation procedure. The last part of the talk is dedicated to executing a demo where Maël creates a target Kubernetes cluster from a running minikube VM (also called bootstrap cluster) where Metal³ is deployed. As it is pointed out in the video, the demo is running in emulated hardware. Actually, something similar to the metal3-dev-env project can be used to reproduce the demo. More information on the Metal³ development environment (metal3-dev-env) can be found in the Metal³ try-it section. In case you want to go deeper, take a look at the blog post A detailed walkthrough of the Metal³ development environment. In the end, the result is a new Kubernetes cluster up and running. The cluster is deployed on two emulated physical servers: one runs as the control-plane node and the other as a worker node. Information The slides of the talk can be downloaded from here Speakers: Maël Kimmerlin Maël Kimmerlin is a Senior Software Engineer at Ericsson. In his own words: I am an open-source enthusiast, focusing in Ericsson on Life Cycle Management of Kubernetes clusters on Bare Metal. I am very interested in the Cluster API project from the Kubernetes Lifecycle SIG, and active in its Bare Metal provider, that is Metal³, developing and encouraging the adoption of this project. References:  Video: Metal³: Kubernetes Native Bare Metal Cluster Management Slides"
    }, {
    "id": 15,
    "url": "/blog/2020/02/18/metal3-dev-env-install-deep-dive.html",
    "title": "A detailed walkthrough of the Metal³ development environment",
    "author" : "Alberto Losada",
    "tags" : "metal3, baremetal, metal3-dev-env, documentation, development",
    "body": "Introduction to metal3-dev-env: The metal3-dev-env is acollection of scripts in a GitHub repository inside theMetal³ project that aims toallow contributors and other interested users to run a fully functionalMetal³ environment for testing and have a first contact with theproject. Actually, metal3-dev-env sets up an emulated environmentwhich creates a set of virtual machines (VMs) to manage as if they werebare metal hosts. Warning This is not an installation that is supposed to be run in production. Instead, it is focused on providing a development environment to testand validate new features. The metal3-dev-env repository includes a set of scripts, libraries andresources used to set up a Metal³ development environment. On theMetal³ websitethere is already a documented process on how to use the metal3-dev-envscripts to set up a fully functional cluster to test the functionalityof the Metal³ components. This procedure at a 10,000-foot view is composed of 3 bash scripts plusa verification one:  01_prepare_host. sh - Mainly installs all needed packages.  02_configure_host. sh - Basically create a set of VMs that will bemanaged as if they were bare metal hosts. It also downloads someimages needed for Ironic.  03_launch_mgmt_cluster. sh - Launches a management cluster usingminikube and runs the baremetal-operator on that cluster.  04_verify. sh - Finally runs a set of tests that verify that thedeployment was completed successfullyIn this blog post, we are going to expand the information and providesome hints and recommendations. Warning Metal³ project is changing rapidly, so probably this information isvaluable in the short term. In any case, it is encouraged todouble-check that the information provided is still valid. Before getting down to it, it is worth defining the nomenclature used in the blog post:  Host. It is the server where the virtual environment is running. In this case, it is a physical PowerEdge M520 with 2 x Intel(R)Xeon(R) CPU E5-2450 v2 @ 2. 50GHz, 96GB RAM and a 140GB drive runningCentOS 7 latest. Do not panic, lab environment should work with lowerresources as well.  Virtual bare metal hosts. These are the virtual machines (KVMbased) that are running on the host which are emulating physical hostsin our lab. They are also called bare metal hosts even if they are notphysical servers.  Management or bootstrap cluster. It is a fully functionalKubernetes cluster in charge of running all the necessary Metal³operators and controllers to manage the infrastructure. In this caseit is the minikube virtual machine.  Target cluster. It is the Kubernetes cluster created from themanagement one. It is provisioned and configured using a nativeKubernetes API for that purpose. Create the Metal³ laboratory: Information A non-root user must exist in the host with password-less sudo access. This user is in charge of running the metal3-dev-env scripts. The first thing that needs to be done is, obviously, cloning themetal3-dev-env repository: [alosadag@eko1: ~]$ git clone https://github. com/metal3-io/metal3-dev-env. gitCloning into 'metal3-dev-env'. . . remote: Enumerating objects: 22, done. remote: Counting objects: 100% (22/22), done. remote: Compressing objects: 100% (22/22), done. remote: Total 1660 (delta 8), reused 8 (delta 0), pack-reused 1638Receiving objects: 100% (1660/1660), 446. 08 KiB | 678. 00 KiB/s, done. Resolving deltas: 100% (870/870), done. Before starting to deploy the Metal³ environment, it makes sense todetail a series of scripts inside the library folder that will besourced in every step of the installation process. They are calledshared libraries. [alosadag@eko1:~]$ ls -1 metal3-dev-env/lib/common. shimages. shlogging. shnetwork. shShared libraries: Although there are several scripts placed inside the lib folder that aresourced in some of the deployment steps, common. sh and logging. share the only ones used in all of the executions during the installationprocess. common. sh: The first time this library is run, a new configuration file is createdwith several variables along with their default values. They will beused during the installation process. On the other hand, if the filealready exists, then it just sources the values configured. Theconfiguration file is created inside the cloned folder withconfig_$USER as the file name. [alosadag@eko1 metal3-dev-env]$ ls config_*config_alosadag. shThe configuration file contains multiple variables that will be usedduring the set-up. Some of them are detailed in the setup section ofthe Metal³ try-it web page. In case you need to add or change global variables it should be done inthis config file. Note I personally recommend modifying or adding variables in this configfile instead of exporting them in the shell. By doing that, it isassured that they are persisted [alosadag@eko1 metal3-dev-env]$ cat ~/metal3-dev-env/config_alosadag. sh#!/bin/bash## This is the subnet used on the  baremetal  libvirt network, created as the# primary network interface for the virtual bare metalhosts. ## Default of 192. 168. 111. 0/24 set in lib/common. sh##export EXTERNAL_SUBNET= 192. 168. 111. 0/24 ## This SSH key will be automatically injected into the provisioned host# by the provision_host. sh script. ## Default of ~/. ssh/id_rsa. pub is set in lib/common. sh##export SSH_PUB_KEY=~/. ssh/id_rsa. pub. . . This common. sh library also makes sure there is an ssh public keyavailable in the user’s ssh folder. This key will be injected bycloud-init in all the virtual bare metal machines that will beconfigured later. Then, the user that executed the metal3-dev-envscripts is able to access the target cluster through ssh. Also, common. sh library also sets more global variables apart fromthose in the config file. Note that these variables can be added to theconfig file along with the proper values for your environment.       Name of the variable   Default value         SSH_KEY   ${HOME}/. ssh/id_rsa       SSH_PUB_KEY   ${SSH_KEY}. pub       NUM_NODES   2       VM_EXTRADISKS   false       DOCKER_REGISTRY_IMAGE   docker. io/registry:latest       VBMC_IMAGE   quay. io/metal3-io/vbmc       SUSHY_TOOLS_IMAGE   quay. io/metal3-io/sushy-tools       IPA_DOWNLOADER_IMAGE   quay. io/metal3-io/ironic-ipa-downloader       IRONIC_IMAGE   quay. io/metal3-io/ironic       IRONIC_INSPECTOR_IMAGE   quay. io/metal3-io/ironic-inspector       BAREMETAL_OPERATOR_IMAGE   quay. io/metal3-io/baremetal-operator       CAPM3_VERSION   v1alpha3       CAPBM_IMAGE   quay. io/metal3-io/cluster-api-provider-baremetal:v1alpha1       CAPBM_IMAGE   quay. io/metal3-io/cluster-api-provider-baremetal       DEFAULT_HOSTS_MEMORY   8192       CLUSTER_NAME   test1       KUBERNETES_VERSION   v1. 17. 0       KUSTOMIZE_VERSION   v3. 2. 3   Information It is important to mention that there are several basic functionsdefined in this file that will be used by the rest of scripts. logging. sh: This script ensures that there is a log folder where all the informationgathered during the execution of the scripts is stored. If there is anyissue during the deployment, this is one of the first places to look at. [alosadag@eko1 metal3-dev-env]$ ls -1 logs/01_prepare_host-2020-02-03-122452. log01_prepare_host-2020-02-03-122956. loghost_cleanup-2020-02-03-122656. logFirst step: Prepare the host: In this first step (01_prepare_host. sh), the requirements needed tostart the preparation of the host where the virtual bare metal hostswill run are fulfilled. Depending on the host’s operating system (OS),it will trigger a specific script for CentOS/Red Hat or Ubuntu.  note: “Note”Currently CentOS Linux 7, Red Hat Enterprise Linux 8 and Ubuntuhave been tested. There is work in progress to adapt the deploymentfor CentOS Linux8. As stated previously, CentOS 7 is the operating system chosen to runin both, the host and virtual servers. Therefore, specific packages ofthe operating system are applied in the following script:    centos_install_requirements. shThis script enables epel and tripleo (current-tripleo)repositories where several packages are installed: dnf, ansible,wget, python3 and python related packages such aspython-virtualbmc from tripleo repository.  Note Notice that SELinux is set to permissive and an OS update istriggered, which will cause several packages to be upgraded sincethere are newer packages in the tripleo repositories (mostly pythonrelated) than in the rest of enabled repositories. At this point, thecontainer runtime is also installed. Note that by setting the variableCONTAINER_RUNTIME defined in common. sh is possible tochoose between docker and podman, which is the default for CentOS. Remember that this behavior can be overwritten in your config file. Once the specific requirements for the elected operating system areaccomplished, the download of several external artifacts is executed. Actually minikube, kubectl and kustomize are downloaded from theinternet. Notice that the version of Kustomize and Kubernetes is definedby KUSTOMIZE_VERSION and KUBERNETES_VERSION variables insidecommon. sh, but minikube is always downloading the latestversion available. The next step deals with cleaning ironic containers and pods thatcould be running in the host from failed deployments. This will ensurethat there will be no issues when creating ironic-pod and infra-poda little bit later in this first step.    network. sh.   At this point, the network library script is sourced. As expected,this library deals with the network configuration which includes: IPaddresses, network definitions and IPv6 support which is disabled bydefault by setting PROVISIONING_IPV6 variable:           Name of the variable    Default value    Option              PROVISIONING_NETWORK    172. 22. 0. 0/24    This is the subnet used to run the OS provisioning process          EXTERNAL_SUBNET    192. 168. 111. 0/24    This is the subnet used on the “baremetal” libvirt network, created as the primary network interface for the virtual bare metal hosts          LIBVIRT_FIRMWARE    bios               PROVISIONING_IPV6    false            Below it is depicted a network diagram of the different virtualnetworks and virtual servers involved in the Metal³ environment:    images. sh.   The images. sh library file is sourced as well in script01_prepare_host. sh. The images. sh script contains multiplevariables that set the URL (IMAGE_LOCATION), name (IMAGE_NAME) anddefault username (IMAGE_USERNAME) of the cloud image that needs tobe downloaded. The values of each variable will differ depending onthe operating system of the virtual bare metal hosts. Note that theseimages will be served from the host to the virtual servers through theprovisioning network.  In our case, since CentOS 7 is the base operating system, valueswill be defined as:           Name of the variable    Default value              IMAGE_NAME    CentOS-7-x86_64-GenericCloud-1907. qcow2          IMAGE_LOCATION    http://cloud. centos. org/centos/7/images          IMAGE USERNAME    centos      Information In case it is expected to use a custom cloud image, just modify theprevious variables to match the right location. Now that the cloud image is defined, the download process can bestarted. First, a folder defined by IRONIC_IMAGE_DIR should exist sothat the image (CentOS-7-x86_64-GenericCloud-1907. qcow2) and itschecksum can be stored. This folder and its content will be exposedthrough a local ironic container running in the host.       Name of the variable   Default value       IRONIC_IMAGE_DIR   /opt/metal3-dev-env/ironic/html/images   Below it is verified that the cloud image files were downloadedsuccessfully in the defined folder: [alosadag@eko1 metal3-dev-env]$ ll /opt/metal3-dev-env/ironic/html/imagestotal 920324-rw-rw-r--. 1 alosadag alosadag 942407680 Feb 3 12:39 CentOS-7-x86_64-GenericCloud-1907. qcow2-rw-rw-r--. 1 alosadag alosadag    33 Feb 3 12:39 CentOS-7-x86_64-GenericCloud-1907. qcow2. md5sumOnce the shared script images. sh is sourced, the following containerimages are pre-cached locally to the host in order to speed up thingslater. Below is shown the code snippet in charge of that task: + for IMAGE_VAR in IRONIC_IMAGE IPA_DOWNLOADER_IMAGE VBMC_IMAGE SUSHY_TOOLS_IMAGE DOCKER_REGISTRY_IMAGE+ IMAGE=quay. io/metal3-io/ironic+ sudo podman pull quay. io/metal3-io/ironic. . . . . . . The container image location of each one is defined by their respective variables:       Name of the variable   Default value         VBMC_IMAGE   quay. io/metal3-io/vbmc       SUSHY_TOOLS_IMAGE   quay. io/metal3-io/sushy-tools       IPA_DOWNLOADER_IMAGE   quay. io/metal3-io/ironic-ipa-downloader       IRONIC_IMAGE   quay. io/metal3-io/ironic       DOCKER_REGISTRY_IMAGE   docker. io/registry:latest   Information In case it is expected to modify the public container images to testnew features, it is worth mentioning that there is a containerregistry running as a privileged container in the host. Therefore itis recommended to upload your modified images there and just overwritethe previous variables to match the right location. At this point, an Ansible role is run locally in order to complete thelocal configuration. ansible-playbook \ -e  working_dir=$WORKING_DIR  \ -e  virthost=$HOSTNAME  \ -i vm-setup/inventory. ini \ -b -vvv vm-setup/install-package-playbook. ymlThis playbook imports two roles. One is called packages_installation,which is in charge of installing a few more packages. The list ofpackages installed are listed as default Ansible variables in thevm-setup role inside the metal3-dev-envrepository. The other role is based on thefubarhouse. golangAnsible Galaxy role. It is in charge of installing and configuring theexact golang version 1. 12. 12 defined in an Ansible variable in theinstall-package-playbook. ymlplaybook Once the playbook is finished, a pod called ironic-pod is created. Inside that pod, a privileged ironic-ipa-downloader container isstarted and attached to the host network. This container is in charge ofdownloading the Ironic PythonAgent (IPA)files to a shared volume defined by IRONIC_IMAGE_DIR. This folder isexposed by the ironic container through HTTP. Information The Ironic PythonAgent is anagent for controlling and deploying Ironic controlled baremetal nodes. Typically run in a ramdisk, the agent exposes a REST API forprovisioning servers. See below the code snippet that fulfils the task: sudo podman run -d --net host --privileged --name ipa-downloader \ --pod ironic-pod -e IPA_BASEURI= -v /opt/metal3-dev-env/ironic:/shared \ quay. io/metal3-io/ironic-ipa-downloader /usr/local/bin/get-resource. shBelow is shown the status of the pods and containers at this point: [root@eko1 metal3-dev-env]# podman pod list --ctr-namesPOD ID     NAME     STATUS  CREATED   CONTAINER INFO                       INFRA ID5a0d475351aa  ironic-pod  Running  6 days ago  [5a0d475351aa-infra] [ipa-downloader]           18f3a8f61407The process will wait until the ironic-python-agent (IPA) initramfs,kernel and headers files are downloaded successfully. See below thefiles downloaded along with the CentOS 7 cloud image: [alosadag@eko1 metal3-dev-env]$ ll /opt/metal3-dev-env/ironic/html/imagestotal 920324-rw-rw-r--. 1 alosadag alosadag 942407680 Feb 3 12:39 CentOS-7-x86_64-GenericCloud-1907. qcow2-rw-rw-r--. 1 alosadag alosadag    33 Feb 3 12:39 CentOS-7-x86_64-GenericCloud-1907. qcow2. md5sumdrwxr-xr-x. 2 root   root      147 Feb 3 12:41 ironic-python-agent-1862d000-59d9fdc6304b1lrwxrwxrwx. 1 root   root      72 Feb 3 12:41 ironic-python-agent. initramfs -&gt; ironic-python-agent-1862d000-59d9fdc6304b1/ironic-python-agent. initramfslrwxrwxrwx. 1 root   root      69 Feb 3 12:41 ironic-python-agent. kernel -&gt; ironic-python-agent-1862d000-59d9fdc6304b1/ironic-python-agent. kernellrwxrwxrwx. 1 root   root      74 Feb 3 12:41 ironic-python-agent. tar. headers -&gt; ironic-python-agent-1862d000-59d9fdc6304b1/ironic-python-agent. tar. headersAfterwards, the script makes sure that libvirt is running successfullyon the host and that the non-privileged user has permission to interactwith it. Libvirt daemon should be running so that minikube can beinstalled successfully. See the following script snippet starting theminikube VM: + sudo su -l -c 'minikube start --insecure-registry 192. 168. 111. 1:5000'* minikube v1. 6. 2 on Centos 7. 7. 1908* Selecting 'kvm2' driver from user configuration (alternates: [none])In the same way, as with the host, container images are pre-cached butin this case inside minikube local image repository. Notice that in thiscase the Bare Metaloperator (BMO) isalso downloaded since it will run on minikube. The container location isdefined by BAREMETAL_OPERATOR_IMAGE. In case you want to test newfeatures or new fixes to the BMO, just change the value of the variableto match the location of the modified image:       Name of the variable   Default value       BAREMETAL_OPERATOR_IMAGE   quay. io/metal3-io/baremetal-operator   Note Remember that minikube is the management cluster in our environment. So it must run all the operators and controllers needed for Metal³. Below is shown the output of the script once all the container imageshave been pulled to minikube: + sudo su -l -c 'minikube ssh sudo docker image ls' alosadagREPOSITORY                TAG         IMAGE ID      CREATED       SIZEquay. io/metal3-io/ironic         latest       e5d81adf05ee    26 hours ago    693MBquay. io/metal3-io/ironic-ipa-downloader  latest       d55b0dac2144    6 days ago     239MBquay. io/metal3-io/ironic-inspector    latest       8bb5b844ada6    6 days ago     408MBquay. io/metal3-io/baremetal-operator   latest       3c692a32ddd6    9 days ago     1. 77GBk8s. gcr. io/kube-proxy           v1. 17. 0       7d54289267dc    7 weeks ago     116MBk8s. gcr. io/kube-controller-manager    v1. 17. 0       5eb3b7486872    7 weeks ago     161MBk8s. gcr. io/kube-scheduler         v1. 17. 0       78c190f736b1    7 weeks ago     94. 4MBk8s. gcr. io/kube-apiserver         v1. 17. 0       0cae8d5cc64c    7 weeks ago     171MBkubernetesui/dashboard          v2. 0. 0-beta8    eb51a3597525    7 weeks ago     90. 8MBk8s. gcr. io/coredns            1. 6. 5        70f311871ae1    2 months ago    41. 6MBk8s. gcr. io/etcd              3. 4. 3-0       303ce5db0e90    3 months ago    288MBkubernetesui/metrics-scraper       v1. 0. 2       3b08661dc379    3 months ago    40. 1MBk8s. gcr. io/kube-addon-manager       v9. 0. 2       bd12a212f9dc    6 months ago    83. 1MBk8s. gcr. io/pause             3. 1         da86e6ba6ca1    2 years ago     742kBgcr. io/k8s-minikube/storage-provisioner  v1. 8. 1       4689081edb10    2 years ago     80. 8MBOnce the container images are stored, minikube can be stopped. At thatmoment, the virtual networks shown in the previous picture are attachedto the minikube VM as can be verified by the following command: [alosadag@smc-master metal3-dev-env]$ sudo virsh domiflist minikubeInterface Type    Source   Model    MAC--------------------------------------------------------     network  default  virtio   d4:38:25:25:c6:ca-     network  minikube-net virtio  a4:c2:8a:9d:2a:d8-     network  provisioning virtio  52:54:00:c8:50:97-     network  baremetal virtio   52:54:00:17:b4:ecInformation At this point the host is ready to create the virtual infrastructure. The video below exhibits all the configurations explained and executedduring this first step. Step 2: Configure the host: In this step, the script 02_configure_host. sh basically configures thelibvirt/KVM virtual infrastructure and starts services in the host thatwill be consumed by the virtual bare metal machines:  Web server to expose the ironic-python-agent (IPA) initramfs,kernel, headers and operating system cloud images.  Virtual BMC to emulate a real baseboard management controller (BMC).  Container registry where the virtual servers will pull the imagesneeded to run a K8s installation. Information A baseboard management controller (BMC) is a specialized serviceprocessor that monitors the physical state of a computer, networkserver or other hardware device using sensors and communicating withthe system administrator through an independent connection. The BMC ispart of the Intelligent Platform Management Interface (IPMI) and isusually contained in the motherboard or main circuit board of thedevice to be monitored. First, an ssh-key in charge of communicating to libvirt is created if itdoes not exist previously. This key is called id_rsa_virt_power. It isadded to the root authorized_keys and is used by vbmc and sushy toolsto contact libvirt. Information sushy-tools is a set of simple simulation tools aiming at supportingthe development and testing of the Redfish protocol implementations. Next, another Ansible playbook calledsetup-playbook. ymlis run against the host. It is focused on setting up the virtualinfrastructure around metal3-dev-env. Below it is shown the Ansiblevariables that are passed to the playbook, which actually are obtainingthe values from the global variables defined in thecommon. sh or the configuration file. ANSIBLE_FORCE_COLOR=true ansible-playbook \  -e  working_dir=$WORKING_DIR  \  -e  num_nodes=$NUM_NODES  \  -e  extradisks=$VM_EXTRADISKS  \  -e  virthost=$HOSTNAME  \  -e  platform=$NODES_PLATFORM  \  -e  libvirt_firmware=$LIBVIRT_FIRMWARE  \  -e  default_memory=$DEFAULT_HOSTS_MEMORY  \  -e  manage_baremetal=$MANAGE_BR_BRIDGE  \  -e  provisioning_url_host=$PROVISIONING_URL_HOST  \  -i vm-setup/inventory. ini \  -b -vvv vm-setup/setup-playbook. yml      Name of the variable   Default value         WORKING_DIR   /opt/metal3-dev-env       NUM_NODES   2       VM_EXTRADISKS   false       HOSTNAME   hostname       NODES_PLATFORM   libvirt       LIBVIRT_FIRMWARE   bios       DEFAULT_HOSTS_MEMORY   8192       MANAGE_BR_BRIDGE   y       PROVISIONING_URL_HOST   172. 22. 0. 1   Information There are variables that are only defined as Ansible variables, e. g. number of CPUs of the virtual bare metal server, size of disks, etc. In case you would like to change properties not defined globally inthe metal3-dev-env take a look at the default variables specified inrole:commonandlibvirt The setup-playbook. yml is composed by 3 roles, which are detailed below:    Common.   This role sets up the virtual hardware and network configuration ofthe VMs. Actually it is adependencyof the libvirt and virtbmc Ansible roles. This means that thecommon role must always be executed before the roles that depend onthem. Also, they are only executed once. If two roles state the sameone as their dependency, it is only executed the first time.    Libvirt.   It actually is the role that configures the virtual bare metalservers. They are all identically defined with the same hardware andnetwork configuration. Note that they are not started since they willbe booted later by ironic during the provisioning process. Note It is possible to change the number of VMs to provision by replacingthe value of NUMBER_NODES Finally, once the VMs are defined and we have their MAC address, theironic inventory file ironic_nodes_json is created. The action ofcreating a node is part of the enrollment process and the first stepto prepare a node to reach the available status. {  nodes : [  {    name :  node-0 ,    driver :  ipmi ,    resource_class :  baremetal ,    driver_info : {     username :  admin ,     password :  password ,     port :  6230 ,     address :  ipmi://192. 168. 111. 1:6230 ,     deploy_kernel :  http://172. 22. 0. 1/images/ironic-python-agent. kernel ,     deploy_ramdisk :  http://172. 22. 0. 1/images/ironic-python-agent. initramfs    },    ports : [    {      address :  00:00:e0:4b:24:8b ,      pxe_enabled : true    }   ],    properties : {     local_gb :  50 ,     cpu_arch :  x86_64    }  },  {    name :  node-1 ,    driver :  ipmi ,    resource_class :  baremetal ,    driver_info : {     username :  admin ,     password :  password ,     port :  6231 ,     address :  ipmi://192. 168. 111. 1:6231 ,     deploy_kernel :  http://172. 22. 0. 1/images/ironic-python-agent. kernel ,     deploy_ramdisk :  http://172. 22. 0. 1/images/ironic-python-agent. initramfs    },    ports : [    {      address :  00:00:e0:4b:24:8f ,      pxe_enabled : true    }   ],    properties : {     local_gb :  50 ,     cpu_arch :  x86_64    }  },Information This role is also used to tear down the virtual infrastructuredepending on the variablelibvirt_actioninside the Ansible role: setup or teardown.  VirtBMCThis role is only executed if the bare metal virtual machines arecreated in libvirt, because vbmc needs libvirt to emulate a realBMC. info “Information”VirtualBMC (vmbc) tool simulates a Baseboard Management Controller(BMC) by exposing IPMI responder to the network and talking to libvirtat the host vBMC is running at. Basically, manipulate virtual machineswhich pretend to be bare metal servers. The virtbmc Ansible role creates the vbmc and sushy-toolsconfiguration in the host for each virtual bare metal nodes. Note thateach virtual bare metal host will have a different vbmc socketexposed in the host. The communication to each vbmc is needed by theBMO to start, stop, configure the boot order, etc during theprovisioning stage. Finally, this folders containing the configurationwill be mounted by the vbmc and sushy-tools containers. [alosadag@eko1 metal3-dev-env]$ sudo ls -l --color /opt/metal3-dev-env/virtualbmctotal 0drwxr-x---. 2 root root 21 Feb 5 11:07 sushy-toolsdrwxr-x---. 4 root root 70 Feb 5 11:08 vbmcNext, both host provisioning and baremetal interfaces are configured. The provisioning interface, as the name suggests, will be used toprovision the virtual bare metal hosts by means of the Bare MetalOperator. This interface is configured with an static IP (172. 22. 0. 1): [alosadag@smc-master metal3-dev-env]$ ifconfig provisioningprovisioning: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500  inet 172. 22. 0. 1 netmask 255. 255. 255. 0 broadcast 172. 22. 0. 255  inet6 fe80::1091:c1ff:fea1:6a0f prefixlen 64 scopeid 0x20&lt;link&gt;  ether 12:91:c1:a1:6a:0f txqueuelen 1000 (Ethernet)On the other hand, the baremetal virtual interface behaves as anexternal network. This interface is able to reach the internet and it isthe network where the different Kubernetes nodes will exchangeinformation. This interface is configured as auto, so the IP isretrieved by DHCP. [alosadag@smc-master metal3-dev-env]$ ifconfig baremetalbaremetal: flags=4099&lt;UP,BROADCAST,MULTICAST&gt; mtu 1500  inet 192. 168. 111. 1 netmask 255. 255. 255. 0 broadcast 192. 168. 111. 255  ether 52:54:00:db:85:29 txqueuelen 1000 (Ethernet)Next, an Ansible role calledfirewallwill be executed targeting the host to be sure that the proper portsare opened. In case your host is running Red Hat Enterprise Linux orCentOS 8, firewall module will be used. In any other case, iptablesmodule is the choice. Below is the code snippet where firewalld or iptables is assigned: # Use firewalld on CentOS/RHEL, iptables everywhere elseexport USE_FIREWALLD=Falseif [[ ($OS ==  rhel  || $OS =  centos ) &amp;&amp; ${OS_VERSION} == 8 ]]then export USE_FIREWALLD=TruefiNote This behavior can be changed by replacing the value of theUSE_FIREWALLD variable The ports managed by this role are all associated with the services thattake part in the provisioning process: ironic, vbmc, httpd, pxe,container registry. . Note Services like ironic, pxe, keepalived, httpd and the containerregistry are running in the host as containers attached to the hostnetwork on the host’s provisioning interface. On the other hand, thevbmc service is also running as a privileged container and it islistening in the host’s baremetal interface. Once the network is configured, a local container registry is started. It will be needed in the case of using locally built images. In thatcase, the container images can be modified locally and pushed to thelocal registry. At that point, the specific image location variable mustbe changed so it must point out the local registry. This process makesit easy to verify and test changes to the code locally. At this point, the following containers are running inside two pods onthe host: infra-pod and ironic-pod. [root@eko1 metal3-dev-env]# podman pod list --ctr-namesPOD ID     NAME     STATUS  CREATED   CONTAINER INFO                       INFRA ID67cc53713145  infra-pod  Running  6 days ago  [vbmc] [sushy-tools] [httpd-infra] [67cc53713145-infra]  f1da23fcd77f5a0d475351aa  ironic-pod  Running  6 days ago  [5a0d475351aa-infra] [ipa-downloader]           18f3a8f61407Below are detailed the containers inside the infra-pod pod which arerunning as privileged using the host network:    The httpd container. &gt; &gt;A folder called shared where the cloud OS image and IPA files areavailable is mounted and exposed to the virtual bare metal hosts.   sudo podman run -d --net host --privileged --name httpd-infra \ --pod infra-pod -v /opt/metal3-dev-env/ironic:/shared --entrypoint \  /bin/runhttpd quay. io/metal3-io/ironic  This folder also contains the inspector. ipxe file which contains theinformation needed to be able to run the ironic-python-agent kerneland initramfs. Below, httpd-infra container is accessed and it hasbeen verified that host’s /opt/metal3-dev-env/ironic/(IRONIC_DATA_DIR) is mounted inside the shared folder of thecontainer: [alosadag@eko1 metal3-dev-env]$ sudo podman exec -it httpd-infra bash[root@infra-pod shared]# cat html/inspector. ipxe#!ipxe:retry_bootecho In inspector. ipxeimgfree# NOTE(dtantsur): keep inspection kernel params in [mdns]params in ironic-inspector-imagekernel --timeout 60000 http://172. 22. 0. 1:80/images/ironic-python-agent. kernel ipa-inspection-callback-url=http://172. 22. 0. 1:5050/v1/continue ipa-inspection-collectors=default,extra-hardware,logs systemd. journald. forward_to_console=yes BOOTIF=${mac} ipa-debug=1 ipa-inspection-dhcp-all-interfaces=1 ipa-collect-lldp=1 initrd=ironic-python-agent. initramfs || goto retry_bootinitrd --timeout 60000 http://172. 22. 0. 1:80/images/ironic-python-agent. initramfs || goto retry_bootboot   The vbmc container.   This container mounts two host folders: one is/opt/metal3-dev-env/virtualbmc/vbmc where vbmc configuration foreach node is stored, the other folder is /root/. ssh where root keysare located, specifically id_rsa_virt_power which is used to managethe communication with libvirt.  + sudo podman run -d --net host --privileged --name vbmc --pod infra-pod -v /opt/metal3-dev-env/virtualbmc/vbmc:/root/&gt; . vbmc -v /root/. ssh:/root/ssh quay. io/metal3-io/vbmc    The sushy-tools container.   This container mounts the /opt/metal3-dev-env/virtualbmc/sushy-toolsconfig folder and the /root/. ssh local folder as well. Thefunctionality is similar as the vbmc, however this use redfishinstead of ipmi to connect to the BMC.  + sudo podman run -d --net host --privileged --name sushy-tools --pod infra-pod -v /opt/metal3-dev-env/virtualbmc/&gt; sushy-tools:/root/sushy -v /root/. ssh:/root/ssh quay. io/metal3-io/sushy-tools Information At this point the virtual infrastructure must be ready to apply theKubernetes specific configuration. Note that all the VMs specified byNUMBER_NODES and minikube must be shut down and the defined virtualnetwork must be active: [alosadag@smc-master metal3-dev-env]$ sudo virsh list --all Id  Name              State---------------------------------------------------- -   minikube            shut off -   node_0             shut off -   node_1             shut off -   node_2             shut off[alosadag@smc-master metal3-dev-env]$ sudo virsh net-list --all Name         State   Autostart   Persistent---------------------------------------------------------- baremetal      active   yes      yes default       active   yes      yes minikube-net     active   yes      yes provisioning     active   yes      yesIn the video below it is exhibited all the configuration explained andexecuted during this second step. Step 3: Launch the management cluster (minikube): The third script called 03_launch_mgmt_cluster. sh basically configuresminikube to become a Metal³ management cluster. On top of minikube thebaremetal-operator, capi-controller-manager,capbm-controller-manager and cabpk-controller-manager are installedin the metal3 namespace. In a more detailed way, the script clones the Bare Metal Operator(BMO) and ClusterAPI Provider for Managed Bare Metal Hardware operator(CAPBM)git repositories, creates the cloud. yaml file and starts the minikubevirtual machine. Once minikube is up and running, the BMO is built andexecuted in minikube’s Kubernetes cluster. In the case of the Bare Metal Operator, the branch by default to cloneis master, however, this and other variables shown in the followingtable can be replaced in the config file: + BMOREPO=https://github. com/metal3-io/baremetal-operator. git+ BMOBRANCH=master      Name of the variable   Default value   Options         BMOREPO   https://github. com/metal3-io/baremetal-operator. git           BMOBRANCH   master           CAPBMREPO   https://github. com/metal3-io/cluster-api-provider-baremetal. git           CAPM3_VERSION   v1alpha3   v1alpha4 or v1alpha3       FORCE_REPO_UPDATE   false           BMO_RUN_LOCAL   false           CAPBM_RUN_LOCAL   false       Once the BMO variables are configured, it is time for the operator tobe deployed using kustomize and kubectl as it can seen from thelogs:  Information: Kustomize is a Kubernetes tool that lets you customize raw, template-free YAML files for multiple purposes, leaving the original YAML untouched and usable as is. + kustomize build bmo-dirPrHIrcl+ kubectl apply -f-namespace/metal3 createdcustomresourcedefinition. apiextensions. k8s. io/baremetalhosts. metal3. io createdserviceaccount/metal3-baremetal-operator createdclusterrole. rbac. authorization. k8s. io/metal3-baremetal-operator createdclusterrolebinding. rbac. authorization. k8s. io/metal3-baremetal-operator createdconfigmap/ironic-bmo-configmap-75tkt49k5c createdsecret/mariadb-password-d88m524c46 createddeployment. apps/metal3-baremetal-operator createdOnce the BMO objects are applied, it’s time to transform the virtualbare metal hosts information into a yaml file of kind BareMetalHostCustom Resource (CR). This is done by a golang script passing them theIPMI address, BMC username and password, which are stored as aKubernetes secret, MAC address and name: + go run /home/alosadag/go/src/github. com/metal3-io/baremetal-operator/cmd/make-bm-worker/main. go -address ipmi://192. 168. 111. 1:6230 -password password -user admin -boot-mac 00:be:bc:fd:17:f3 node-0+ read -r name address user password mac+ go run /home/alosadag/go/src/github. com/metal3-io/baremetal-operator/cmd/make-bm-worker/main. go -address ipmi://192. 168. 111. 1:6231 -password password -user admin -boot-mac 00:be:bc:fd:17:f7 node-1+ read -r name address user password mac+ go run /home/alosadag/go/src/github. com/metal3-io/baremetal-operator/cmd/make-bm-worker/main. go -address ipmi://192. 168. 111. 1:6232 -password password -user admin -boot-mac 00:be:bc:fd:17:fb node-2+ read -r name address user password macBelow is shown the bare metal host definition of node-1. Note that theIPMI address is the IP of the host’s provisioning interface. Behind thescenes, IPMI is handled by the vbmc container running in the host. ---apiVersion: v1kind: Secretmetadata: name: node-1-bmc-secrettype: Opaquedata: username: YWRtaW4= password: cGFzc3dvcmQ=---apiVersion: metal3. io/v1alpha1kind: BareMetalHostmetadata: name: node-1spec: online: true bootMACAddress: 00:00:e0:4b:24:8f bmc:  address: ipmi://192. 168. 111. 1:6231  credentialsName: node-1-bmc-secretSee that the MAC address configured in the BareMetalHost specdefinition matches node-1 provisioning interface: [root@eko1 metal3-dev-env]# virsh domiflist node_1Interface Type    Source   Model    MAC-------------------------------------------------------vnet4   bridge   provisioning virtio   00:00:e0:4b:24:8fvnet5   bridge   baremetal virtio   00:00:e0:4b:24:91Finally, the script apply in namespace metal3 each of theBareMetalHost yaml files that match each virtual bare metal host: + kubectl apply -f bmhosts_crs. yaml -n metal3secret/node-0-bmc-secret createdbaremetalhost. metal3. io/node-0 createdsecret/node-1-bmc-secret createdbaremetalhost. metal3. io/node-1 createdsecret/node-2-bmc-secret createdbaremetalhost. metal3. io/node-2 createdLastly, it is the turn of the CAPBM. Similar to BMO, kustomize isused to create the different Kubernetes components and kubectl appliedthe files into the management cluster. Warning Note that installing CAPBM includes installing the components of theCluster API and thecomponents of the Cluster API bootstrap providerkubeadm(CABPK) Below the objects are created through the generate. sh script: ++ mktemp -d capbm-XXXXXXXXXX+ kustomize_overlay_path=capbm-eJPOjCPASD+ . /examples/generate. sh -fGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/_out/cluster. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/_out/controlplane. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/_out/metal3crds. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/_out/metal3plane. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/_out/machinedeployment. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/provider-components/provider-components-cluster-api. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/provider-components/provider-components-kubeadm. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/provider-components/provider-components-baremetal. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/_out/provider-components. yamlThen, kustomize configures the files accordingly to the values definedand kubectl applies them: + kustomize build capbm-eJPOjCPASD+ kubectl apply -f-namespace/cabpk-system creatednamespace/capbm-system creatednamespace/capi-system createdcustomresourcedefinition. apiextensions. k8s. io/baremetalclusters. infrastructure. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/baremetalmachines. infrastructure. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/baremetalmachinetemplates. infrastructure. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/clusters. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/kubeadmconfigs. bootstrap. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/kubeadmconfigtemplates. bootstrap. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/machinedeployments. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/machines. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/machinesets. cluster. x-k8s. io createdrole. rbac. authorization. k8s. io/cabpk-leader-election-role createdrole. rbac. authorization. k8s. io/capbm-leader-election-role createdrole. rbac. authorization. k8s. io/capi-leader-election-role createdclusterrole. rbac. authorization. k8s. io/cabpk-manager-role createdclusterrole. rbac. authorization. k8s. io/cabpk-proxy-role createdclusterrole. rbac. authorization. k8s. io/capbm-manager-role createdclusterrole. rbac. authorization. k8s. io/capbm-proxy-role createdclusterrole. rbac. authorization. k8s. io/capi-manager-role createdrolebinding. rbac. authorization. k8s. io/cabpk-leader-election-rolebinding createdrolebinding. rbac. authorization. k8s. io/capbm-leader-election-rolebinding createdrolebinding. rbac. authorization. k8s. io/capi-leader-election-rolebinding createdclusterrolebinding. rbac. authorization. k8s. io/cabpk-manager-rolebinding createdclusterrolebinding. rbac. authorization. k8s. io/cabpk-proxy-rolebinding createdclusterrolebinding. rbac. authorization. k8s. io/capbm-manager-rolebinding createdclusterrolebinding. rbac. authorization. k8s. io/capbm-proxy-rolebinding createdclusterrolebinding. rbac. authorization. k8s. io/capi-manager-rolebinding createdsecret/capbm-webhook-server-secret createdservice/cabpk-controller-manager-metrics-service createdservice/capbm-controller-manager-service createdservice/capbm-controller-metrics-service createddeployment. apps/cabpk-controller-manager createddeployment. apps/capbm-controller-manager createddeployment. apps/capi-controller-manager createdInformation At this point all controllers and operators must be running in thenamespace metal3 of the management cluster (minikube). All virtualbare metal hosts configured must be shown as BareMetalHostsresources in the metal3 namespace as well. They should be in readystatus and stopped (online is false) In the video below it is exhibited all the configuration explained and executed during this third step. Step 4: Verification: The last script 04_verify. sh is in charge of verifying that thedeployment has been successful by checking several things:  Custom resources (CR) and custom resource definition (CRD) wereapplied and exist in the cluster.  Verify that the virtual bare metal hosts matches the informationdetailed in theBareMetalHost object.  All containers are in running status.  Verify virtual network configuration and status.  Verify operators and controllers are running. However, this verification can be easily achieved manually. Forinstance, checking that controllers and operators running in themanagement cluster (minikube) and all the virtual bare metal hosts arein ready status: [alosadag@eko1 ~]$ kubectl get pods -n metal3 -o wideNAME                     READY  STATUS  RESTARTS  AGE   IP        NODE    NOMINATED NODE  READINESS GATEScabpk-controller-manager-5c67dd56c4-wfwbh  2/2   Running  9     6d23h  172. 17. 0. 5    minikube  &lt;none&gt;      &lt;none&gt;capbm-controller-manager-7f9b8f96b7-grl4r  2/2   Running  12     6d23h  172. 17. 0. 4    minikube  &lt;none&gt;      &lt;none&gt;capi-controller-manager-798c76675f-dxh2n   1/1   Running  10     6d23h  172. 17. 0. 6    minikube  &lt;none&gt;      &lt;none&gt;metal3-baremetal-operator-5b4c59755d-h4zkp  6/6   Running  8     6d23h  192. 168. 39. 101  minikube  &lt;none&gt;      &lt;none&gt;Verify that the BareMetalHosts provisioning status is ready and theBMC configuration is correct. Check that all virtual bare metal hostsare shut down (online is false): [alosadag@eko1 ~]$ kubectl get baremetalhosts -n metal3NAME   STATUS  PROVISIONING STATUS  CONSUMER       BMC             HARDWARE PROFILE  ONLINE  ERRORnode-0  OK    ready                   ipmi://192. 168. 111. 1:6230  unknown      falsenode-1  OK    ready                   ipmi://192. 168. 111. 1:6231  unknown      falsenode-2  OK    ready                   ipmi://192. 168. 111. 1:6232  unknown      falseGet the list of CRDs created in the cluster. Check that, at least, thefollowing ones exist: [alosadag@eko1 ~]$ kubectl get crdsNAME                            CREATED ATbaremetalclusters. infrastructure. cluster. x-k8s. io      2020-01-22T13:19:42Zbaremetalhosts. metal3. io                  2020-01-22T13:19:35Zbaremetalmachines. infrastructure. cluster. x-k8s. io      2020-01-22T13:19:42Zbaremetalmachinetemplates. infrastructure. cluster. x-k8s. io  2020-01-22T13:19:42Zclusters. cluster. x-k8s. io                  2020-01-22T13:19:42Zkubeadmconfigs. bootstrap. cluster. x-k8s. io          2020-01-22T13:19:42Zkubeadmconfigtemplates. bootstrap. cluster. x-k8s. io      2020-01-22T13:19:42Zmachinedeployments. cluster. x-k8s. io             2020-01-22T13:19:43Zmachines. cluster. x-k8s. io                  2020-01-22T13:19:43Zmachinesets. cluster. x-k8s. io                2020-01-22T13:19:43ZInformation KUBECONFIG file is stored in the user’s home directory(~/. kube/config) that executed the scripts. Check the status of all the applications running in minikube or bettersaid, in the management cluster. [alosadag@smc-master logs]$ kubectl get pods -ANAMESPACE   NAME                    READY  STATUS  RESTARTS  AGEkube-system  coredns-6955765f44-fkdzp          1/1   Running  1     164mkube-system  coredns-6955765f44-fxzvz          1/1   Running  1     164mkube-system  etcd-minikube                1/1   Running  1     164mkube-system  kube-addon-manager-minikube         1/1   Running  1     164mkube-system  kube-apiserver-minikube           1/1   Running  1     164mkube-system  kube-controller-manager-minikube      1/1   Running  1     164mkube-system  kube-proxy-87g98              1/1   Running  1     164mkube-system  kube-scheduler-minikube           1/1   Running  1     164mkube-system  storage-provisioner             1/1   Running  2     164mmetal3    cabpk-controller-manager-5c67dd56c4-rldk4  2/2   Running  0     156mmetal3    capbm-controller-manager-7f9b8f96b7-mdfcw  2/2   Running  0     156mmetal3    capi-controller-manager-84947c7497-k6twl  1/1   Running  0     156mmetal3    metal3-baremetal-operator-78bffc8d-z5hqs  6/6   Running  0     156mIn the video below it is exhibited all the configuration explained andexecuted during the verification steps. Summary: In this post a deep dive into the metal3-dev-env scripts was shown. Ithas been deeply detailed the process of creating a Metal³ emulatedenvironment from a set of virtual machines (VMs) to manage as if theywere bare metal hosts. After this post, the reader should have acquired a basic understandingof all the pieces involved in the Metal³ project. Also, and moreimportant, how these scripts can be adapted to your specific needs. Remember that this can be achieved in multiple ways: replacing values inthe global variables, replacing Ansible default variables or evenmodifying playbooks or the scripts themselves. Notice that the Metal³ development environment also focuses ondeveloping new features of the BMO or CAPBM and being able to test themlocally. References:  Video playlist: A detailed walkthrough the installation of the metal3-dev-env on Youtube Getting started with Metal3. io Metal³ code repositories"
    }, {
    "id": 16,
    "url": "/blog/2020/01/20/metal3_deploy_kubernetes_on_bare_metal.html",
    "title": "Metal³: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019",
    "author" : "Pedro Ibáñez Requena",
    "tags" : "hybrid, cloud, metal3, baremetal, shiftdev, edge",
    "body": "Conference talk: Metal³: Deploy Kubernetes on Bare Metal - Yolanda Robla, Red Hat: Some of the most influential minds in the developer industry were landing in the gorgeous ancient city of Split, Croatia, to talk at the Shift Dev 2019 - Developer Conference about the most cutting-edge technologies, techniques and biggest trends in the developer space. In this video, Yolanda Robla speaks about the deployment of Kubernetes on Bare Metal with the help of Metal³, a new tool that enables the management of bare metal hosts via custom resources managed through the Kubernetes API. Speakers: Yolanda Robla Yolanda Robla is a Principal Software Engineer at Red Hat. In her own words:  In my current position in Red Hat as an NFV Partner Engineer, I investigate new technologies and create proofs of concept for partners to embrace new technologies. Being the current PTL of Akraino, I am involved in designing and implementing systems based on Kubernetes for the Edge use cases, ensuring high scalability and reproducibility using a GitOps approach. References:  Video: Metal³: Deploy Kubernetes on Bare Metal video"
    }, {
    "id": 17,
    "url": "/blog/2019/12/04/Introducing_metal3_kubernetes_native_bare_metal_host_management.html",
    "title": "Introducing Metal³: Kubernetes Native Bare Metal Host Management - Russell Bryant & Doug Hellmann, Red Hat - KubeCon NA, November 2019",
    "author" : "Pedro Ibáñez Requena",
    "tags" : "hybrid, cloud, metal3, baremetal, kubecon, edge",
    "body": "Conference talk: Introducing Metal³: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat: Metal³ (metal cubed/Kube) is a new open-source bare metal host provisioning tool created to enable Kubernetes-native infrastructure management. Metal³ enables the management of bare metal hosts via custom resources managed through the Kubernetes API as well as the monitoring of bare metal host metrics to Prometheus. This presentation will explain the motivations behind creating the project and what has been accomplished so far. This will be followed by an architectural overview and description of the Custom Resource Definitions (CRDs) for describing bare metal hosts, leading to a demonstration of using Metal³ in a Kubernetes cluster. In this video, Russell Bryant and Doug Hellmann speak about the what’s and how’s of Metal³, a new tool that enables the management of bare metal hosts via custom resources managed through the Kubernetes API. Speakers: Russell Bryant Russell Bryant is a Distinguished Engineer at Red Hat, where he works on infrastructure management to support Kubernetes clusters. Prior to working on the Metal³ project, Russell worked on other open infrastructure projects. Russell worked in Software Defined Networking with Open vSwitch (OVS) and Open Virtual Network (OVN) and worked on various parts of OpenStack. Russell also worked in open source telephony via the Asterisk project. Doug Hellmann Doug Hellmann is a Senior Principal Software Engineer at Red Hat. He has been a professional developer since the mid-1990s and has worked on a variety of projects in fields such as mapping, medical news publishing, banking, data centre automation, and hardware provisioning. He has been contributing to open-source projects for most of his career and for the past 7 years he has been focusing on open-source cloud computing technologies, including OpenStack and Kubernetes. References:  Presentation: Introducing Metal³ KubeCon NA 2019 PDF Video: Introducing Metal³: Kubernetes Native Bare Metal Host Management videoDemos:  First demo (Inspection)  Second demo (Provisioning)  Third demo (Scale up)  Fourth demo (v1alpha2) "
    }, {
    "id": 18,
    "url": "/blog/2019/11/13/Extend_Your_Data_Center_to_the_Hybrid_Edge-Red_Hat_Summit.html",
    "title": "Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019",
    "author" : "Pedro Ibáñez Requena",
    "tags" : "hybrid, cloud, metal3, baremetal, summit, edge",
    "body": "Conference talk: Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019, Paul Cormier, Burr Stutter and Garima Sharma: A critical part of being successful in the hybrid cloud is being successful in your data centre with your own infrastructure. In this video, Paul Cormier, Burr Sutter and Garima Sharma show how you can bring the Open Hybrid cloud to the edge. Cluster management from multiple cloud providers to on-premise. In the demo you’ll see a multi-cluster inventory for the open hybrid cloud at cloud. redhat. com, OpenShift Container Storage providing storage for Virtual Machines and containers (Cloud, Virtualization and bare metal), and everything Kubernetes native. Speakers: Paul Cormier Executive vice president and president, Products and Technologies. Leads Red Hat’s technology and products organizations, including engineering, product management, and product marketing for Red Hat’s technologies. He joined Red Hat in May 2001 as executive vice president, Engineering. Burr Sutter A lifelong developer advocate, community organizer, and technology evangelist, Burr Sutter is a featured speaker at technology events around the globe —from Bangalore to Brussels and Berlin to Beijing (and most parts in between)— he is currently Director of Developer Experience at Red Hat. A Java Champion since 2005 and former president of the Atlanta Java User Group, Burr founded the DevNexus conference —now the second largest Java event in the U. S. — with the aim of making access to the world’s leading developers affordable to the developer community. Garima Sharma Senior Engineering leader at the world’s largest Open Source company. As a seasoned Tech professional, she runs a global team of Solutions Engineers focused on a large portfolio of Cloud Computing products and technology. She has helped shape science and technology for mission-critical software, reliability in operations and re-design of architecture all geared towards advancements in medicine, security, cloud technologies and bottom-line savings for the client businesses. Whether leading the architecture, development and delivery of customer-centric cutting-edge systems or spearheading diversity and inclusion initiatives via keynotes, blogs and conference presentations, Garima champions the idea of STEM. Garima ardently believes in Maya Angelou’s message that diversity makes for a rich tapestry, and we must understand that all the threads of the tapestry are equal in value no matter what their color. Video:  Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019"
    }, {
    "id": 19,
    "url": "/blog/2019/11/07/Kubernetes-native_Infrastructure-Managed_Baremetal_with_Kubernetes_Operators_and_OpenStack_Ironic.html",
    "title": "Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat",
    "author" : "Pedro Ibáñez Requena",
    "tags" : "kubernetes, metal3, operator, baremetal, openstack, ironic",
    "body": "Conference talk: Open Infrastructure Days UK 2019; Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat: In this session, you can hear about a new effort to enable baremetal Kubernetes deployments using native interfaces, and in particular, the Kubernetes Operator framework, combined with OpenStack Ironic. This approach aims to seamlessly integrate your infrastructure with your workloads, including baremetal servers, storage and container/VM workloads. All this can be achieved using kubernetes native applications, combined with existing, proven deployment and storage tooling. In this talk, we cover the options around Kubernetes deployments today, the specific approach taken by the new Kubernetes-native “MetalKube” project, and the status/roadmap of this new community effort. Speakers: Steve Hardy is a Senior Principal Software Engineer at Red Hat, currently involved in kubernetes/OpenShift deployment and architecture. He is also an active member of the OpenStack community and has been a project team leader of both the Heat (orchestration) and TripleO (deployment) projects. References:  Open Infrastructure Days UK 2019, Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat"
    }, {
    "id": 20,
    "url": "/blog/2019/10/31/OpenStack-Ironic-and-Bare-Metal-Infrastructure_All-Abstractions-Start-Somewhere.html",
    "title": "OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere - Chris Hoge, OpenStack Foundation; Julia Kreger, Red Hat",
    "author" : "Pedro Ibáñez Requena",
    "tags" : "kubernetes, metal3, operator, baremetal, openstack",
    "body": "Conference talk: OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere: The history of cloud computing has rapidly layered abstractions on abstractions to deliver applications faster, more reliably, and easier. Serverless functions on top of containers on top of virtualization. However, at the bottom of every stack is physical hardware that has an entire lifecycle that needs to be managed. In this video, Chris and Julia show how OpenStack Ironic is a solution to the problem of managing bare-metal infrastructure. Speakers: Chris Hoge is a Senior Strategic Program Manager for the OpenStack foundation. He’s been an active contributor to the Interop Working Group (formerly DefCore) and helps run the trademark program for the OpenStack Foundation. He also works on collaborations between the OpenStack and Kubernetes communities. Previously he worked as an OpenStack community manager and developer at Puppet Labs and operated a research cloud for the College of Arts and Sciences at The University of Oregon. When not cloud computing, he enjoys long-distance running, dancing, and throwing a ball for his Border Collie. Julia Kreger is Principal Software Engineer at Red Hat. She started her career in networking and eventually shifted to systems engineering. The DevOps movement leads her into software development and the operationalization of software due to the need to automate large-scale systems deployments. She is experienced in conveying an operational perspective while bridging that with requirements and doesn’t mind getting deep down into code to solve a problem. She is an active core contributor and leader in the OpenStack Ironic project, which is a project she feels passionate about due to many misspent hours in data centres deploying hardware. Prior to OpenStack, Julia contributed to the Shared Learning Infrastructure and worked with large-scale litigation database systems. References:  Open Infrastructure Summit, Denver, CO, April 29 - May 1, 2019 OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere"
    }, {
    "id": 21,
    "url": "/blog/2019/09/11/Baremetal-operator.html",
    "title": "Baremetal Operator",
    "author" : "Pablo Iranzo Gómez",
    "tags" : "openshift, kubernetes, metal3, operator",
    "body": "Introduction: The baremetal operator, documented at https://github. com/metal3-io/baremetal-operator/blob/master/docs/api. md, it’s the Operator in charge of definitions of physical hosts, containing information about how to reach the Out of Band management controller, URL with the desired image to provision, plus other properties related with hosts being used for provisioning instances. Quoting from the project:  The Bare Metal Operator implements a Kubernetes API for managing bare metal hosts. It maintains an inventory of available hosts as instances of the BareMetalHost Custom Resource Definition. The Bare Metal Operator knows how to:Inspect the host’s hardware details and report them on the corresponding BareMetalHost. This includes information about CPUs, RAM, disks, NICs, and more. Provision hosts with a desired imageClean a host’s disk contents before or after provisioning. A bit more in deep approach: The Baremetal Operator (BMO) keeps a mapping of each host and its management interfaces (vendor-based like iLO, iDrac, iRMC, etc) and is controlled via IPMI. All of this is defined in a CRD, for example: apiVersion: v1kind: Secretmetadata: name: metal3-node01-credentials namespace: metal3type: Opaquedata: username: YWRtaW4= password: YWRtaW4=---apiVersion: metal3. io/v1alpha1kind: BareMetalHostmetadata: name: metal3-node01 namespace: metal3spec: bmc:  address: ipmi://172. 22. 0. 2:6230  credentialsName: metal3-node01-credentials bootMACAddress: 00:c2:fc:3b:e1:01 description:    hardwareProfile:  libvirt  online: falseWith above values (described in API), we’re telling the operator:  MAC: Defines the mac address of the NIC connected to the network that will be used for the provisioning of the host bmc: defines the management controller address and the secret used credentialsName: Defines the name of the secret containing username/password for accessing the IPMI serviceOnce the server is ‘defined’ via the CRD, the underlying service (provided by ironic1 as of this writing) is inspected: [root@metal3-kubernetes ~]# kubectl get baremetalhost -n metal3NAME      STATUS  PROVISIONING STATUS  CONSUMER  BMC           HARDWARE PROFILE  ONLINE  ERRORmetal3-node01  OK    inspecting            ipmi://172. 22. 0. 1:6230           falseOnce the inspection has finished, the status will change to ready and made available for provisioning. When we define a machine, we refer to the images that will be used for the actual provisioning in the CRD (image): apiVersion: v1data: userData: DATAkind: Secretmetadata: name: metal3-node01-user-data namespace: metal3type: Opaque---apiVersion:  cluster. k8s. io/v1alpha1 kind: Machinemetadata: name: metal3-node01 namespace: metal3 generateName: baremetal-machine-spec: providerSpec:  value:   apiVersion:  baremetal. cluster. k8s. io/v1alpha1    kind:  BareMetalMachineProviderSpec    image:    url: http://172. 22. 0. 2/images/CentOS-7-x86_64-GenericCloud-1901. qcow2    checksum: http://172. 22. 0. 2/images/CentOS-7-x86_64-GenericCloud-1901. qcow2. md5sum   userData:    name: metal3-node01-user-data    namespace: metal3[root@metal3-kubernetes ~]# kubectl create -f metal3-node01-machine. ymlsecret/metal3-node01-user-data createdmachine. cluster. k8s. io/metal3-node01 createdLet’s examine the annotation created when provisioning (metal3. io/BareMetalHost): [root@metal3-kubernetes ~]# kubectl get machine -n metal3 metal3-node01 -o yamlapiVersion: cluster. k8s. io/v1alpha1kind: Machinemetadata: annotations:  metal3. io/BareMetalHost: metal3/metal3-node01 creationTimestamp:  2019-07-08T15:30:44Z  finalizers: - machine. cluster. k8s. io generateName: baremetal-machine- generation: 2 name: metal3-node01 namespace: metal3 resourceVersion:  6222  selfLink: /apis/cluster. k8s. io/v1alpha1/namespaces/metal3/machines/metal3-node01 uid: 1bfd384a-5467-43b7-98aa-e80e1ace5ce7spec: metadata:  creationTimestamp: null providerSpec:  value:   apiVersion: baremetal. cluster. k8s. io/v1alpha1   image:    checksum: http://172. 22. 0. 1/images/CentOS-7-x86_64-GenericCloud-1901. qcow2. md5sum    url: http://172. 22. 0. 1/images/CentOS-7-x86_64-GenericCloud-1901. qcow2   kind: BareMetalMachineProviderSpec   userData:    name: metal3-node01-user-data    namespace: metal3 versions:  kubelet:   status: addresses: - address: 192. 168. 122. 79  type: InternalIP - address: 172. 22. 0. 39  type: InternalIP - address: localhost. localdomain  type: Hostname lastUpdated:  2019-07-08T15:30:44Z In the output above, the host assigned was the one we’ve defined earlier as well as the other parameters like IP’s, etc generated. Now, if we check baremetal hosts, we can see how it’s getting provisioned: [root@metal3-kubernetes ~]# kubectl get baremetalhost -n metal3NAME      STATUS  PROVISIONING STATUS  CONSUMER  BMC           HARDWARE PROFILE  ONLINE  ERRORmetal3-node01  OK    provisioned            ipmi://172. 22. 0. 1:6230           trueAnd also, check it via the ironic command: [root@metal3-kubernetes ~]# export OS_TOKEN=fake-token ; export OS_URL=http://localhost:6385 ; openstack baremetal node list+--------------------------------------+---------------+--------------------------------------+-------------+--------------------+-------------+| UUID                 | Name     | Instance UUID            | Power State | Provisioning State | Maintenance |+--------------------------------------+---------------+--------------------------------------+-------------+--------------------+-------------+| 7551cfb4-d758-4ad8-9188-859ee53cf298 | metal3-node01 | 7551cfb4-d758-4ad8-9188-859ee53cf298 | power on  | active       | False    |+--------------------------------------+---------------+--------------------------------------+-------------+--------------------+-------------+Wrap-up: We’ve seen how via a CRD we’ve defined credentials for a baremetal host to make it available to get provisioned and how we’ve also defined a machine that was provisioned on top of that baremetal host.       Ironic was chosen as the initial provider for baremetal provisioning, check Ironic documentation for more details about Ironic usage in Metal³ &#8617;    "
    }, {
    "id": 22,
    "url": "/blog/2019/06/25/Metal3.html",
    "title": "Metal3",
    "author" : "Eduardo Minguez",
    "tags" : "openshift, kubernetes, metal3",
    "body": "Originally posted at https://www. underkube. com/posts/2019-06-25-metal3/ In this blog post, I’m going to try to explain in my own words a high leveloverview of what Metal3 is, the motivation behind it and some concepts relatedto a ‘baremetal operator’. Let’s have some definitions! Custom Resource Definition: The k8s API provides out-of-the-box objects such as pods, services, etc. There are a few methods of extending the k8s API (such as API extensions)but since a few releases back, the k8s API can be extended easily with custom resources definitions (CRDs). Basically, this means you can virtually create any type of object definition in k8s(actually only users with cluster-admin capabilities) with a yaml such as: apiVersion: apiextensions. k8s. io/v1beta1kind: CustomResourceDefinitionmetadata: # name must match the spec fields below, and be in the form: &lt;plural&gt;. &lt;group&gt; name: crontabs. stable. example. comspec: # group name to use for REST API: /apis/&lt;group&gt;/&lt;version&gt; group: stable. example. com # list of versions supported by this CustomResourceDefinition versions:  - name: v1   # Each version can be enabled/disabled by Served flag.    served: true   # One and only one version must be marked as the storage version.    storage: true # either Namespaced or Cluster scope: Namespaced names:  # plural name to be used in the URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt;  plural: crontabs  # singular name to be used as an alias on the CLI and for display  singular: crontab  # kind is normally the CamelCased singular type. Your resource manifests use this.   kind: CronTab  # shortNames allow shorter string to match your resource on the CLI  shortNames:   - ct preserveUnknownFields: false validation:  openAPIV3Schema:   type: object   properties:    spec:     type: object     properties:      cronSpec:       type: string      image:       type: string      replicas:       type: integerAnd after kubectl apply -f you can kubectl get crontabs. There is a ton of information with regards to CRDs, like the k8s official documentation. The CRD by himself is not useful per se as nobody will take care of it (that’s why I said definition). Itrequires a controller to watch for those new objects and react to differentevents affecting the object. Controller: A controller is basically a loop that watches the current status of an objectand if it is different from the desired status, it fixes it (reconciliation). This is why k8s is ‘declarative’, you specify the object desired status instead‘how to do it’ (imperative). Again, there are tons of documentation (and examples) around the controller pattern which isbasically the k8s roots, so I’ll let your google-foo take care of it :) Operator: An Operator (in k8s slang) is an application running in your k8scluster that deploys, manages and maintains (so, operates) a k8s application. This k8s application (the one that the operator manages), can be as simple as a ‘hello world’ applicationcontainerized and deployed in your k8s cluster or it can be a much more complexthing, such as a database cluster. The ‘operator’ is like an ‘expert sysadmin’ containerized that takes care ofyour application. Bear in mind that the ‘expert’ tag (meaning the automation behind the operator)depends on the operator implementation… so there can be basic operators thatonly deploy your application or complex operators that handle day 2 operationssuch as upgrades, failovers, backup/restore, etc. See the CoreOS operator definition for more information. Cloud Controller Manager: k8s code is smart enough to be able to leveragethe underlying infrastructure where the cluster is running, such as being ableof creating ‘LoadBalancer’ services, understanding the cluster topology based on the cloud provider AZs where the nodes are running (for scheduling reasons), etc. This task of ‘talking to the cloud provider’ is performed by the Cloud Controller Manager (CCM) and for moreinformation, you can take a look at the official k8s documentation withregards the architecture and the administration (also, if you are brave enough, you can create your own cloud controller manager ) Cluster API: The Cluster API implementation is a WIP ‘framework’ that allows a k8s cluster to manage itself, including the ability to create new clusters, add more nodes, etc. in a ‘k8s way’ (declarative, controllers, CRDs, etc. ), so there are objects such as Cluster that can be expressed as k8s objects: apiVersion:  cluster. k8s. io/v1alpha1 kind: Clustermetadata: name: myclusterspec: clusterNetwork:  services:   cidrBlocks: [ 10. 96. 0. 0/12 ]  pods:   cidrBlocks: [ 192. 168. 0. 0/16 ]  serviceDomain:  cluster. local  providerSpec: . . . but also:  Machine type objects MachineSet type objects MachineDeployment type objects etcThere are someprovider implementations in the wild such as AWS, Azure, GCP, OpenStack,vSphere, etc. ones and the Cluster API project is driven by the SIG Cluster Lifecycle. Please review the official Cluster API repository for more information. Actuator: The actuator is a Cluster API interface that reacts to changes to Machineobjects reconciliating the Machine status. The actuator code is tightly coupled with the provider (that’s why it is aninterface) such as the AWS one. MachineSet vs Machine: To simplify, let’s say that MachineSets are to Machines what ReplicaSets areto Pods. So you can scale the Machines in your cluster just by changingthe number of replicas of a MachineSet. Cluster API vs Cloud Providers: As we have seen, the Cluster API leverages the provider related to the k8sinfrastructure itself (clusters and nodes) and the CCM and the cloud providerintegration for k8s is to leverage the cloud provider to provide support infrastructure. Let’s say Cluster API is for the k8s administrators and theCCM is for the k8s users :) Machine API: The OpenShift 4 Machine API is a combination of some of the upstream Cluster APIwith custom OpenShift resources and it is designed to work in conjunction withthe Cluster Version Operator. OpenShift’s Machine API Operator: The machine-api-operator isan operator that manages the Machine API objects in an OpenShift 4 cluster. The operator is capable of creating machines in AWS and libvirt (more providerscoming soon) via the Machine Controller and it is included out of thebox with OCP 4 (and can be deployed in a k8s vanilla as well) Baremetal: A baremetal server (or bare-metal) is just a computer server. The last year’s terms such as virtualization, containers, serverless, etc. have beenpopular but at the end of the day, all the code running on top of a SaaS, PaaSor IaaS is actually running in a real physical server stored in a datacenterwired to routers, switches and power. That server is a ‘baremetal’ server. If you are used to cloud providers and instances, you probably don’t know thepains of baremetal management… including things such as connecting to thevirtual console (usually it requires an old Java version) to debug issues,configuring pxe for provisioning baremetal hosts (or attach ISOs via the virtual console… or insert a CD/DVD physically into the CD carry if you are‘lucky’ enough…), configuring VLANs for traffic isolation, etc. That kind of operation is not ‘cloud’ ready and there are tools that providebaremetal management, such as maas or ironic. Ironic: OpenStack bare metal provisioning (or ironic) is an open source project (or even better, a number of open source projects) to manage baremetal hosts. Ironic avoids the administrator dealing with pxe configuration, manual deployments, etc. and provides a defined API and a series of plugins to interact with different baremetal models and vendors. Ironic is used in OpenStack to provide baremetal objects but there are someprojects (such as bifrost) to useIronic ‘standalone’ (so, no OpenStack required) Metal3: Metal3 is a project aimed at providing a baremetal operator thatimplements the Cluster API framework required to be able to manage baremetalin a k8s way (easy peasy!). It uses ironic under the hood to avoid reinventing thewheel, but consider it as an implementation detail that may change. The Metal3 baremetal operator watches for BareMetalHost (CRD) objects defined as: apiVersion: metal3. io/v1alpha1kind: BareMetalHostmetadata: name: my-worker-0spec: online: true bootMACAddress: 00:11:22:33:44:55 bmc:  address: ipmi://my-worker-0. ipmi. example. com  credentialsName: my-worker-0-bmc-secretThere are a few more fields in the BareMetalHost object such as the image, hardware profile, etc. The Metal3 project is actually divided into two different components: baremetal-operator: The Metal3 baremetal-operator is the component that manages baremetal hosts. It exposes a new BareMetalHost custom resource in the k8s API that lets you manage hosts in a declarative way. cluster-api-provider-baremetal: The Metal3 cluster-api-provider-baremetal includes the integration with the Cluster API project. This provider currently includes a Machine actuator that acts as a client of the BareMetalHost custom resources. BareMetalHost vs Machine vs Node:  BareMetalHost is a Metal3 object Machine is a Cluster API object Node is where the pods run :)Those three concepts are linked in a 1:1:1 relationship meaning: A BareMetalHost created with Metal3 maps to a Machine object and once theinstallation procedure finishes, a new kubernetes node will be added to thecluster. $ kubectl get nodesNAME                     STATUS  ROLES  AGE  VERSIONmy-node-0. example. com            Ready  master  25h  v1. 14. 0$ kubectl get machines --all-namespacesNAMESPACE        NAME         INSTANCE  STATE  TYPE  REGION  ZONE  AGEopenshift-machine-api  my-node-0                          25h$ kubectl get baremetalhosts --allnamespacesNAMESPACE       NAME   STATUS PROVISIONING STATUS MACHINE BMC HARDWARE PROFILE ONLINE ERRORopenshift-machine-api my-node-0 OK   provisioned my-node-0. example. com ipmi://1. 2. 3. 4 unknown trueThe 1:1 relationship for the BareMetalHost and the Machine is stored in themachineRef field in the BareMetalHost object: $ kubectl get baremetalhost/my-node-0 -n openshift-machine-api -o jsonpath='{. spec. machineRef}'map[name:my-node-0 namespace:openshift-machine-api]In a Machine annotation: $ kubectl get machines my-node-0 -n openshift-machine-api -o jsonpath='{. metadata. annotations}'map[metal3. io/BareMetalHost:openshift-machine-api/my-node-0]The node reference is stored in the . status. nodeRef. name field in theMachine object: $ kubectl get machine my-node-0 -o jsonpath='{. status. nodeRef. name}'my-node-0. example. comRecap: Being able to ‘just scale a node’ in k8s means a lot of underlying concepts and technologies involved behind the scenes :) Resources/links:  https://dzone. com/articles/introducing-the-kubernetes-cluster-api-project-2 https://tanzu. vmware. com/content/blog/the-what-and-the-why-of-the-cluster-api https://github. com/kubernetes-sigs/cluster-api https://github. com/kubernetes-sigs/cluster-api-provider-aws https://itnext. io/deep-dive-to-cluster-api-a0b4e792d57d https://www. linux. com/blog/event/kubecon/2018/4/extending-kubernetes-cluster-api"
    }, {
    "id": 23,
    "url": "/blog/2019/05/13/The_new_stack_Metal3_Uses_OpenStack_Ironic_for_Declarative_Bare_Metal_Kubernetes.html",
    "title": "The new stack Metal³ Uses OpenStack's Ironic for Declarative Bare Metal Kubernetes",
    "author" : "Pedro Ibáñez Requena",
    "tags" : "hybrid, cloud, metal3, baremetal, stack, edge, OpenStack, ironic",
    "body": "The new stack Metal³ Uses OpenStack’s Ironic for Declarative Bare Metal Kubernetes: Mike Melanson talks in this article about the Open Infrastructure Summit in Denver, Colorado. Where bare metal was one of the main leads of the event. During this event, the OpenStack Foundation unveil a new project called Metal³ (pronounced “metal cubed”) that uses Ironic “as a foundation for declarative management of bare metal infrastructure for Kubernetes”. He also comments on how James Penick, Chris Hoge, senior strategic program manager at OpenStack Foundation,and Julia Kreger, OpenStack Ironic Project Team Leader, took to the stage to offer a demonstration of Metal3,the new project that provides “bare metal host provisioning integration for Kubernetes. ” Some words from Kreger in an interview with The New Stack:  “I think the bigger trend that we’re starting to see is a recognition that common tooling and substrate helps everyone succeed faster with more efficiency. ”  “This is combined with a shift in the way operators are choosing to solve their problems at scale, specifically in regards to isolation, cost, or performance. ” For further detail, check out the video of the keynote, which includes a demonstration of Metal3 being used to quickly provision three bare metal servers with Kubernetesor check the full article included below. References:  The new stack: Metal³ Uses OpenStack’s Ironic for Declarative Bare Metal Kubernetes Video of the keynote: OpenStack Ironic and Baremetal Infrastructure. All Abstractions start somewhere"
    }, {
    "id": 24,
    "url": "/blog/2019/04/30/Metal-Kubed-Baremetal-Provisioning-for-Kubernetes.html",
    "title": "Metal³: Baremetal Provisioning for Kubernetes",
    "author" : "Russell Bryant",
    "tags" : "openshift, kubernetes, metal3",
    "body": "Originally posted at https://blog. russellbryant. net/post/2019/04/2019-04-30-metal-metal-kubed-bare-metal-provisioning-for-kubernetes/ Project Introduction: There are a number of great open-source tools for bare metal host provisioning, including Ironic. Metal³ aims to build on these technologies to provide a Kubernetes native API for managing bare metal hosts via a provisioning stack that is also running on Kubernetes. We believe that Kubernetes Native Infrastructure, or managing your infrastructure just like your applications, is a powerful next step in the evolution of infrastructure management. The Metal³ project is also building integration with the Kubernetes cluster-api project, allowing Metal³ to be used as an infrastructure backend for Machine objects from the Cluster API. Metal3 Repository Overview: There is a Metal³ overview and some more detailed design documents in the metal3-docs repository. The baremetal-operator is the component that manages bare metal hosts. It exposes a new BareMetalHost custom resource in the Kubernetes API that lets you manage hosts in a declarative way. Finally, the cluster-api-provider-baremetal repository includes integration with the cluster-api project. This provider currently includes a Machine actuator that acts as a client of the BareMetalHost custom resources. Demo: The project has been going on for a few months now, and there’s enough now to show some working code. For this demonstration, I’ve started with a 3-node Kubernetes cluster installed using OpenShift. $ kubectl get nodesNAME    STATUS  ROLES  AGE  VERSIONmaster-0  Ready  master  24h  v1. 13. 4+d4ce02c1dmaster-1  Ready  master  24h  v1. 13. 4+d4ce02c1dmaster-2  Ready  master  24h  v1. 13. 4+d4ce02c1dMachine objects were created to reflect these 3 masters, as well. $ kubectl get machinesNAME       INSTANCE  STATE  TYPE  REGION  ZONE  AGEostest-master-0                       24hostest-master-1                       24hostest-master-2                       24hFor this cluster-api provider, a Machine has a corresponding BareMetalHost object, which corresponds to the piece of hardware we are managing. There is a design document that covers the relationship between Nodes, Machines, and BareMetalHosts. Since these hosts were provisioned earlier, they are in a special externally provisioned state, indicating that we enrolled them in management while they were already running in a desired state. If changes are needed going forward, the baremetal-operator will be able to automate them. $ kubectl get baremetalhostsNAME         STATUS  PROVISIONING STATUS   MACHINE      BMC             HARDWARE PROFILE  ONLINE  ERRORopenshift-master-0  OK    externally provisioned  ostest-master-0  ipmi://192. 168. 111. 1:6230           trueopenshift-master-1  OK    externally provisioned  ostest-master-1  ipmi://192. 168. 111. 1:6231           trueopenshift-master-2  OK    externally provisioned  ostest-master-2  ipmi://192. 168. 111. 1:6232           trueNow suppose we’d like to expand this cluster by adding another bare metal host to serve as a worker node. First, we need to create a new BareMetalHost object that adds this new host to the inventory of hosts managed by the baremetal-operator. Here’s the YAML for the new BareMetalHost: ---apiVersion: v1kind: Secretmetadata: name: openshift-worker-0-bmc-secrettype: Opaquedata: username: YWRtaW4= password: cGFzc3dvcmQ=---apiVersion: metalkube. org/v1alpha1kind: BareMetalHostmetadata: name: openshift-worker-0spec: online: true bmc:  address: ipmi://192. 168. 111. 1:6233  credentialsName: openshift-worker-0-bmc-secret bootMACAddress: 00:ab:4f:d8:9e:faNow to add the BareMetalHost and its IPMI credentials Secret to the cluster: $ kubectl create -f worker_crs. yamlsecret/openshift-worker-0-bmc-secret createdbaremetalhost. metalkube. org/openshift-worker-0 createdThe list of BareMetalHosts now reflects a new host in the inventory that is ready to be provisioned. It will remain in this ready state until it is claimed by a new Machine object. $ kubectl get baremetalhostsNAME         STATUS  PROVISIONING STATUS   MACHINE      BMC             HARDWARE PROFILE  ONLINE  ERRORopenshift-master-0  OK    externally provisioned  ostest-master-0  ipmi://192. 168. 111. 1:6230           trueopenshift-master-1  OK    externally provisioned  ostest-master-1  ipmi://192. 168. 111. 1:6231           trueopenshift-master-2  OK    externally provisioned  ostest-master-2  ipmi://192. 168. 111. 1:6232           trueopenshift-worker-0  OK    ready                   ipmi://192. 168. 111. 1:6233  unknown      trueWe have a MachineSet already created for workers, but it scaled down to 0. $ kubectl get machinesetsNAME       DESIRED  CURRENT  READY  AVAILABLE  AGEostest-worker-0  0     0               24hWe can scale this MachineSet to 1 to indicate that we’d like a worker provisioned. The baremetal cluster-api provider will then look for an available BareMetalHost, claim it, and trigger provisioning of that host. $ kubectl scale machineset ostest-worker-0 --replicas=1 After the new Machine was created, our cluster-api provider claimed the available host and triggered it to be provisioned. $ kubectl get baremetalhostsNAME         STATUS  PROVISIONING STATUS   MACHINE         BMC             HARDWARE PROFILE  ONLINE  ERRORopenshift-master-0  OK    externally provisioned  ostest-master-0     ipmi://192. 168. 111. 1:6230           trueopenshift-master-1  OK    externally provisioned  ostest-master-1     ipmi://192. 168. 111. 1:6231           trueopenshift-master-2  OK    externally provisioned  ostest-master-2     ipmi://192. 168. 111. 1:6232           trueopenshift-worker-0  OK    provisioning       ostest-worker-0-jmhtc  ipmi://192. 168. 111. 1:6233  unknown      trueThis process takes some time. Under the hood, the baremetal-operator is driving Ironic through a provisioning process. This begins with wiping disks to ensure the host comes up in a clean state. It will eventually write the desired OS image to disk and then reboot into that OS. When complete, a new Kubernetes Node will register with the cluster. $ kubectl get baremetalhostsNAME         STATUS  PROVISIONING STATUS   MACHINE         BMC             HARDWARE PROFILE  ONLINE  ERRORopenshift-master-0  OK    externally provisioned  ostest-master-0     ipmi://192. 168. 111. 1:6230           trueopenshift-master-1  OK    externally provisioned  ostest-master-1     ipmi://192. 168. 111. 1:6231           trueopenshift-master-2  OK    externally provisioned  ostest-master-2     ipmi://192. 168. 111. 1:6232           trueopenshift-worker-0  OK    provisioned       ostest-worker-0-jmhtc  ipmi://192. 168. 111. 1:6233  unknown      true$ kubectl get nodesNAME    STATUS  ROLES  AGE  VERSIONmaster-0  Ready  master  24h  v1. 13. 4+d4ce02c1dmaster-1  Ready  master  24h  v1. 13. 4+d4ce02c1dmaster-2  Ready  master  24h  v1. 13. 4+d4ce02c1dworker-0  Ready  worker  68s  v1. 13. 4+d4ce02c1dThe following screen cast demonstrates this process, as well: Removing a bare metal host from the cluster is very similar. We just have to scale this MachineSet back down to 0. $ kubectl scale machineset ostest-worker-0 --replicas=0 Once the Machine has been deleted, the baremetal-operator will deprovision the bare metal host. $ kubectl get baremetalhostsNAME         STATUS  PROVISIONING STATUS   MACHINE      BMC             HARDWARE PROFILE  ONLINE  ERRORopenshift-master-0  OK    externally provisioned  ostest-master-0  ipmi://192. 168. 111. 1:6230           trueopenshift-master-1  OK    externally provisioned  ostest-master-1  ipmi://192. 168. 111. 1:6231           trueopenshift-master-2  OK    externally provisioned  ostest-master-2  ipmi://192. 168. 111. 1:6232           trueopenshift-worker-0  OK    deprovisioning               ipmi://192. 168. 111. 1:6233  unknown      falseOnce the deprovisioning process is complete, the bare metal host will be back to its ready state, available in the host inventory to be claimed by a future Machine object. $ kubectl get baremetalhostsNAME         STATUS  PROVISIONING STATUS   MACHINE      BMC             HARDWARE PROFILE  ONLINE  ERRORopenshift-master-0  OK    externally provisioned  ostest-master-0  ipmi://192. 168. 111. 1:6230           trueopenshift-master-1  OK    externally provisioned  ostest-master-1  ipmi://192. 168. 111. 1:6231           trueopenshift-master-2  OK    externally provisioned  ostest-master-2  ipmi://192. 168. 111. 1:6232           trueopenshift-worker-0  OK    ready                   ipmi://192. 168. 111. 1:6233  unknown      falseGetting Involved: All development is happening on github. We have a metal3-dev mailing list and use #cluster-api-baremetal on Kubernetes Slack to chat. Occasional project updates are posted to @metal3_io on Twitter. "
    }, {
    "id": 25,
    "url": "/blog/2019/04/12/Raise_some_horns_Red_Hat_s_MetalKube_aims_to_make_Kubernetes_on_bare_machines_simple.html",
    "title": "Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple",
    "author" : "Pedro Ibáñez Requena",
    "tags" : "hybrid, cloud, metal3, baremetal, stack, edge, openstack, ironic",
    "body": "The Register; Raise some horns: Red Hat’s Metal³ aims to make Kubernetes on bare machines simple: Max Smolaks talks inthis article about the OpenInfra Days in the UK, 2019: where Metal³ wasrevealed earlier last week by Steve Hardy, Red Hat’s senior principalsoftware engineer. The Open Infrastructure Days in the UK is an eventorganized by the local Open Infrastructure community and supported bythe OpenStack Foundation. The Open-source software developers at Red Hatare working on a tool that would simplify the deployment and managementof Kubernetes clusters on bare-metal servers. Steve told The Register:  “In some situations, you won’t want to run a full OpenStackinfrastructure-as-a-service layer to provide, potentially, formultiple Kubernetes clusters”. Hardy is a notable contributor to OpenStack, having previously worked onHeat and TripleO projects. He said one of the reasons for choosingIronic was its active development – and when new features get added toIronic, the Metal³ team gets them “for free”.  “OpenStack has always been a modular set of projects, and people havealways had the opportunity to reuse components for differentapplications. This is just an example of where we are leveraging oneparticular component for infrastructure management, just as analternative to using a full infrastructure API,” Hardy said. Thierry Carrez, veep of engineering at the OpenStack Foundation also toldThe Register:  “I like the fact that the projects end up being reusable on their own,for the functions they bring to the table – this helps us integratewith adjacent communities”. Hardy also commented:  It’s still early days for Metal³ - the project has just sixcontributors, and there’s no telling when it might reach release. “It’s a very, very young project but we are keen to get more communityparticipation and feedback,”. For further detail, check out the full article atThe Register: Raise some horns: Red Hat’s MetalKube aims to make Kubernetes onbare machines simple. References:  Steve Hardy: Red Hat’s seniorprincipal software engineer.  Thierry Carrez: veep of engineering at theOpenStack Foundation.  The Register: Raise some horns: Red Hat’s MetalKube aims to make Kubernetes on bare machines simple"
    }, , {
    "id": 26,
    "url": "/blog/categories.html",
    "title": "Categories",
    "author" : "",
    "tags" : "",
    "body": " -   Blog  Read about the newest updates in the community. &lt;/section&gt;             Categories:        hybrid:               One cluster - multiple providers July 08, 2022                 Metal³: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019 January 20, 2020                 Introducing Metal³: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat - KubeCon NA, November 2019 December 04, 2019                 Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019 November 13, 2019                 The new stack Metal³ Uses OpenStack's Ironic for Declarative Bare Metal Kubernetes May 13, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             cloud:               Metal³: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019 January 20, 2020                 Introducing Metal³: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat - KubeCon NA, November 2019 December 04, 2019                 Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019 November 13, 2019                 The new stack Metal³ Uses OpenStack's Ironic for Declarative Bare Metal Kubernetes May 13, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             metal3:               Metal3. io Becomes a CNCF Incubating Project August 27, 2025                 Introducing Baremetal Operator end-to-end test suite December 13, 2024                 Scaling Kubernetes with Metal3: Simulating 1000 Clusters with Fake Ironic Agents October 24, 2024                 Scaling to 1000 clusters - Part 3 May 30, 2024                 Metal3 at KubeCon EU 2024 April 10, 2024                 How to run Metal3 website locally with Jekyll January 18, 2024                 Scaling to 1000 clusters - Part 2 May 17, 2023                 Scaling to 1000 clusters - Part 1 May 05, 2023                 One cluster - multiple providers July 08, 2022                 Metal3 Introduces Pivoting May 05, 2021                 Introducing the Metal3 IP Address Manager July 06, 2020                 Raw image streaming available in Metal3 July 05, 2020                 Metal³ development environment walkthrough part 2: Deploying a new bare metal cluster June 18, 2020                 Cluster API provider renaming March 05, 2020                 Metal³: Kubernetes Native Bare Metal Cluster Management - Maël Kimmerlin - Kubernetes and CNCF Finland Meetup February 27, 2020                 A detailed walkthrough of the Metal³ development environment February 18, 2020                 Metal³: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019 January 20, 2020                 Introducing Metal³: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat - KubeCon NA, November 2019 December 04, 2019                 Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019 November 13, 2019                 Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat November 07, 2019                 OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere - Chris Hoge, OpenStack Foundation; Julia Kreger, Red Hat October 31, 2019                 Baremetal Operator September 11, 2019                 Metal3 June 25, 2019                 The new stack Metal³ Uses OpenStack's Ironic for Declarative Bare Metal Kubernetes May 13, 2019                 Metal³: Baremetal Provisioning for Kubernetes April 30, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             baremetal:               Scaling Kubernetes with Metal3: Simulating 1000 Clusters with Fake Ironic Agents October 24, 2024                 How to run Metal3 website locally with Jekyll January 18, 2024                 Metal3 Introduces Pivoting May 05, 2021                 Introducing the Metal3 IP Address Manager July 06, 2020                 Raw image streaming available in Metal3 July 05, 2020                 Cluster API provider renaming March 05, 2020                 Metal³: Kubernetes Native Bare Metal Cluster Management - Maël Kimmerlin - Kubernetes and CNCF Finland Meetup February 27, 2020                 A detailed walkthrough of the Metal³ development environment February 18, 2020                 Metal³: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019 January 20, 2020                 Introducing Metal³: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat - KubeCon NA, November 2019 December 04, 2019                 Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019 November 13, 2019                 Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat November 07, 2019                 OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere - Chris Hoge, OpenStack Foundation; Julia Kreger, Red Hat October 31, 2019                 The new stack Metal³ Uses OpenStack's Ironic for Declarative Bare Metal Kubernetes May 13, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             stack:               The new stack Metal³ Uses OpenStack's Ironic for Declarative Bare Metal Kubernetes May 13, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             edge:               Introducing Baremetal Operator end-to-end test suite December 13, 2024                 Scaling to 1000 clusters - Part 3 May 30, 2024                 Scaling to 1000 clusters - Part 2 May 17, 2023                 Scaling to 1000 clusters - Part 1 May 05, 2023                 One cluster - multiple providers July 08, 2022                 Metal³: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019 January 20, 2020                 Introducing Metal³: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat - KubeCon NA, November 2019 December 04, 2019                 Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019 November 13, 2019                 The new stack Metal³ Uses OpenStack's Ironic for Declarative Bare Metal Kubernetes May 13, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             openstack:               Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat November 07, 2019                 OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere - Chris Hoge, OpenStack Foundation; Julia Kreger, Red Hat October 31, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             ironic:               Scaling Kubernetes with Metal3: Simulating 1000 Clusters with Fake Ironic Agents October 24, 2024                 Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat November 07, 2019                 The new stack Metal³ Uses OpenStack's Ironic for Declarative Bare Metal Kubernetes May 13, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             openshift:               Baremetal Operator September 11, 2019                 Metal3 June 25, 2019                 Metal³: Baremetal Provisioning for Kubernetes April 30, 2019             kubernetes:               Metal³ development environment walkthrough part 2: Deploying a new bare metal cluster June 18, 2020                 Metal³: Kubernetes Native Bare Metal Cluster Management - Maël Kimmerlin - Kubernetes and CNCF Finland Meetup February 27, 2020                 Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat November 07, 2019                 OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere - Chris Hoge, OpenStack Foundation; Julia Kreger, Red Hat October 31, 2019                 Baremetal Operator September 11, 2019                 Metal3 June 25, 2019                 Metal³: Baremetal Provisioning for Kubernetes April 30, 2019             OpenStack:               The new stack Metal³ Uses OpenStack's Ironic for Declarative Bare Metal Kubernetes May 13, 2019             operator:               Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat November 07, 2019                 OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere - Chris Hoge, OpenStack Foundation; Julia Kreger, Red Hat October 31, 2019                 Baremetal Operator September 11, 2019             summit:               Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019 November 13, 2019             kubecon:               Metal3 at KubeCon EU 2024 April 10, 2024                 Introducing Metal³: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat - KubeCon NA, November 2019 December 04, 2019             shiftdev:               Metal³: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019 January 20, 2020             metal3-dev-env:               How to run Metal3 website locally with Jekyll January 18, 2024                 Metal³ development environment walkthrough part 2: Deploying a new bare metal cluster June 18, 2020                 A detailed walkthrough of the Metal³ development environment February 18, 2020             documentation:               How to run Metal3 website locally with Jekyll January 18, 2024                 A detailed walkthrough of the Metal³ development environment February 18, 2020             development:               How to run Metal3 website locally with Jekyll January 18, 2024                 A detailed walkthrough of the Metal³ development environment February 18, 2020             talk:               Metal3 at KubeCon EU 2024 April 10, 2024                 Metal³: Kubernetes Native Bare Metal Cluster Management - Maël Kimmerlin - Kubernetes and CNCF Finland Meetup February 27, 2020             conference:               Metal3 at KubeCon EU 2024 April 10, 2024                 Metal³: Kubernetes Native Bare Metal Cluster Management - Maël Kimmerlin - Kubernetes and CNCF Finland Meetup February 27, 2020             meetup:               Metal³: Kubernetes Native Bare Metal Cluster Management - Maël Kimmerlin - Kubernetes and CNCF Finland Meetup February 27, 2020             cluster API:               Introducing Baremetal Operator end-to-end test suite December 13, 2024                 Scaling Kubernetes with Metal3: Simulating 1000 Clusters with Fake Ironic Agents October 24, 2024                 Scaling to 1000 clusters - Part 3 May 30, 2024                 Scaling to 1000 clusters - Part 2 May 17, 2023                 Scaling to 1000 clusters - Part 1 May 05, 2023                 One cluster - multiple providers July 08, 2022                 Metal³ development environment walkthrough part 2: Deploying a new bare metal cluster June 18, 2020                 Cluster API provider renaming March 05, 2020             provider:               Introducing Baremetal Operator end-to-end test suite December 13, 2024                 Scaling to 1000 clusters - Part 3 May 30, 2024                 Scaling to 1000 clusters - Part 2 May 17, 2023                 Scaling to 1000 clusters - Part 1 May 05, 2023                 One cluster - multiple providers July 08, 2022                 Cluster API provider renaming March 05, 2020             raw image:               Raw image streaming available in Metal3 July 05, 2020             image streaming:               Raw image streaming available in Metal3 July 05, 2020             IPAM:               Introducing the Metal3 IP Address Manager July 06, 2020             ip address manager:               Introducing the Metal3 IP Address Manager July 06, 2020             Pivoting:               Metal3 Introduces Pivoting May 05, 2021             Move:               Metal3 Introduces Pivoting May 05, 2021             scaling:               Scaling Kubernetes with Metal3: Simulating 1000 Clusters with Fake Ironic Agents October 24, 2024             cncf:               Metal3. io Becomes a CNCF Incubating Project August 27, 2025             community:               Metal3. io Becomes a CNCF Incubating Project August 27, 2025             announcement:               Metal3. io Becomes a CNCF Incubating Project August 27, 2025                     Categories:             hybrid        cloud        metal3        baremetal        stack        edge        openstack        ironic        openshift        kubernetes        OpenStack        operator        summit        kubecon        shiftdev        metal3-dev-env        documentation        development        talk        conference        meetup        cluster API        provider        raw image        image streaming        IPAM        ip address manager        Pivoting        Move        scaling        cncf        community        announcement      "
    }, {
    "id": 27,
    "url": "/community-resources.html",
    "title": "Community Resources",
    "author" : "",
    "tags" : "",
    "body": " -     Community Resources    Join conversations with the other people who are involved in the creation, maintenance, and future of Metal3. io. &lt;/section&gt;           Around the Web: Conference Talks:  Metal3: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019 Introducing Metal3 kubernetes native bare metal host management - Kubecon NA 2019 Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019 OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere - Chris Hoge, OpenStack Foundation; Julia Kreger, Red Hat Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red HatIn The News:  The New Stack: Metal3 Uses OpenStack’s Ironic for Declarative Bare Metal Kubernetes The Register: Raise some horns: Red Hat’s MetalKube aims to make Kubernetes on bare machines simpleBlog Posts:  Metal3 Blog postsCommunity Meetups:  Join Metal3 Team Meetups to engage in discussion with members and helpwith a deeper understanding of the project as well as the futurediscussion               Get Connected:                                                                                                                                                                                Mailing List                               Slack                     Twitter                                                                               GitHub                                                                                                       Blog                                                            YouTube                           Partnering Communities:                                                                                                                                         Ironic Website                                                                                                                          Ironic Community                                  Cluster API Community                  Providers of Testing Resources:                     Nordix. Details here.         Netlify Open Source Plan.           "
    }, {
    "id": 28,
    "url": "/contribute.html",
    "title": "Contribute",
    "author" : "",
    "tags" : "",
    "body": " - ContributeWant to contribute to the Metal3 Project? Here's everything you need to know. &lt;/section&gt;  &lt;main&gt;  &lt;section class= mk-main__section &gt;    &lt;header class= mk-main__header &gt;  &lt;h2 class= mk-heading mk-heading--xl mk-m-border &gt;About Metal3 Community&lt;/h2&gt;&lt;/header&gt; The Metal3 community is an open-source community dedicated to the advancement of the Metal3 project, a Kubernetes-based solution for managing bare metal infrastructure as code. Leveraging Kubernetes and tools like Ironic, Metal3 allows users to treat physical machines like virtual machines, simplifying provisioning, management, and scalability of bare metal resources. If you want to learn more about the Metal3 community or get involved in the project, you can visit their official community page at Community Resources.  How to Contribute: Familiarize yourself with the Metal3 repositories on GitHub. Follow the documentation to set up your development environment. This will allow you to test your changes locally before submitting them. Join community meetings and mailing list to engage in discussions with the members of the metal3 community. Join Ironic Community for steps on how to contribute. Check out community resources page to find out more.  Best Practices:   Review existing issues, feature requests, and ongoing discussions to find areas where you can contribute.  Introduce yourself and be patient while waiting for your PR to be reviewed.  If it's your first time contributing, look for issues with good first issue labels.  Ask questions and seek clarifications when needed.    Open Issues:     Review the Metal3 project's existing issues.   Review Ironic Bugs to contribute to the Ironic project. &lt;/section&gt;&lt;/main&gt; "
    }, {
    "id": 29,
    "url": "/documentation.html",
    "title": "Documentation",
    "author" : "",
    "tags" : "",
    "body": " -       Documentation     The Metal³ project (pronounced “metal cubed”) exists to provide components that allow you to do bare metal host management for Kubernetes. Metal³ works as a Kubernetes application, meaning it runs on Kubernetes and is managed through Kubernetes interfaces.      If you are looking for documentation about how to use Metal³, please check the user-guide.            Metal3 Component Overview:                     It is helpful to understand the high level architecture of of the Machine API Integration. Click on each step to learn more about that particular component.                   Machine controller:           Bare metal actuator                          The first component is the Bare Metal Actuator, which is an implementation of the Machine Actuator interface defined by the cluster-api project. This actuator reacts to changes to Machine objects and acts as a client of the BareMetalHost custom resources managed by the Bare Metal                           Bare metal operator:           With CRDs representing bare metal inventory with configuration needed by its bare metal management workers.                             The architecture also includes a new Bare Metal Operator, which includes the following:             A Controller for a new Custom Resource, BareMetalHost. This custom resource represents an inventory of known (configured or automatically discovered) bare metal hosts. When a Machine is created the Bare Metal Actuator will claim one of these hosts to be provisioned as a new Kubernetes node.             In response to BareMetalHost updates, the controller will perform bare metal host provisioning actions as necessary to reach the desired state.             The creation of the BareMetalHost inventory can be done in two ways:                           Manually via creating BareMetalHost objects.               Optionally, automatically created via a bare metal host discovery process.                 For more information about Operators, see the operator-sdk.                                         Bare metal management pods:                           The operator manages a set of tools for controlling the power on the host, monitoring the host status, and provisioning images to the host. These tools run inside the pod with the operator, and do not require any configuration by the user.                                       Around the Web: Conference Talks:  Metal3: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019 Introducing Metal3 kubernetes native bare metal host management - Kubecon NA 2019 Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019 OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere - Chris Hoge, OpenStack Foundation; Julia Kreger, Red Hat Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red HatIn The News:  The New Stack: Metal3 Uses OpenStack’s Ironic for Declarative Bare Metal Kubernetes The Register: Raise some horns: Red Hat’s MetalKube aims to make Kubernetes on bare machines simpleBlog Posts:  Metal3 Blog postsCommunity Meetups:  Join Metal3 Team Meetups to engage in discussion with members and helpwith a deeper understanding of the project as well as the futurediscussion  "
    }, {
    "id": 30,
    "url": "/faqs.html",
    "title": "FAQs",
    "author" : "",
    "tags" : "",
    "body": " -   Frequently Asked Questions  Here are some answers to common questions, discover more about Metal3                   What is the baremetal operator?    :          Baremetal Operator is a Kubernetes controller providing support forseveral custom resources, most importantly - BareMetalHosts.                  What kind of boot processes can be paired with specific BMC protocols?    :          Drivers with “virtual media” in their name can use the virtual mediatechnology to boot an ISO remotely. The other drivers require networkboot, more specifically - iPXE.                  What is Cluster API provider Metal3 (CAPM3)?    :          CAPM3 is aninfrastructure providerfor the Cluster API that uses Metal3 and Ironic to provision machinesfor your cluster.                  How does Metal3 relate to Cluster API (CAPI)?    :          The Metal3 project includes the Cluster API Provider Metal3 (CAPM3) - aninfrastructure provider for Cluster API.                  What CPU architectures are supported?    :          Both x86_64 (Intel) and AARCH64 (Arm) are supported. Mixed architectures(e. g. some hosts x86_64, some - aarch64) are not supported yet.                  What is IPMI?    :          IPMI is the acronym for Intelligent Platform Management Interfacewhich is used to monitor hardware health (fans, voltage, temperature,etc). The specification is available athereand was created by a joint effort by several manufacturers. It allows usto also define the boot order and power status of the hardware.                  What kinds of operating systems can be installed?    :          You can use any operating system that is available in a cloud format(e. g. qcow2). If you need first boot configuration, the image has tocontain cloud-init or a similar first-boot tool.                  Does Metal3 support provisioners other than Ironic?    :          While it’s technically possible to add more provisioners, only Ironic issupported now, and supporting other provisioners is not on the currentroadmap.                  How can one supply network configuration during provisioning?    :          You can put it to the BareMetalHost’s network Data field in theOpenStack network data format.                  Ironic is developed as part of OpenStack, does Metal3 require OpenStack?    :          Ironic can be used as a stand-alone service without any other OpenStackservices. In fact, Baremetal Operator does not support any otherOpenStack services.                  Can I use my own operating system installer with Metal3?    :          You can use the live ISO workflowto attach a bootable ISO to the machine using virtual media. Note thatBaremetal Operator will not track the installation process in this caseand will consider the host active once the ISO is booted.                  What is an out-of-band management controller?    :          Enterprise hardware usually has an integrated or optional controllerthat allows reaching the server even if it’s powered down, either viadedicated or shared nic. This controller allows some checks on theserver hardware and also perform some settings like changing powerstatus, changing Boot Order, etc. The Baremetal Operator uses it topower on, reboot and provision the physical servers to be used forrunning workloads on top. Commercial names include iDrac, iLO,iRMC, etc and most of them should support IPMI.                  Do I need to use the Metal3 with Cluster API or can I use Metal3 independently?    :          It is completely optional to use Cluster API. You can use only theBaremetal Operator and skip CAPM3 completely if all you need isbare-metal provisioning via Kubernetes API.                  What is Ironic and how does Metal3 relate to it?    :          Ironic is a bare metal provisioner, it handles provisioning of physicalmachines. Metal3 exposes a part of the Ironic functionality as aKubernetes native API via the Baremetal Operator. Ironic is not part ofMetal3 but Metal3 relies on Ironic to provision the bare metal hosts.                  What is an operator?    :          An Operator is a method of packaging, deploying and managing aKubernetes application. A Kubernetes application is an application thatis both deployed on Kubernetes and managed using the Kubernetes APIs andkubectl tooling. You can think of Operators as the runtime that managesthis type of application on Kubernetes. If you want to learn more aboutOperators you can check the Operator framework websitehttps://operatorframework. io/what/                  What is cleaning? Can I disable it?    :          Cleaning removes partitioning information from the disks to avoidconflicts with the new operating system. Seeautomated cleaning for details.                  What is inspection? Can I disable it?    :          Inspection is used to populate hardware information in the BareMetalHostobjects. You can disable it,but you may need to populate this information yourself. Do not blindlydisable inspection if it fails - chances are high the subsequentoperations fail the same way.                  What is iPXE?    :          The iPXE project develops firmware for booting machines over thenetwork. It’s a more feature-rich alternative to the well known PXE andcan be used as an add-on on top of PXE.                  What is virtual media?    :          Virtual media is a technology that allows booting an ISO on a remotemachine without resorting to network boot (e. g. PXE).                  Why use Ironic?    :          Ironic is an established service with a long history of production usageand good support for industry standards. By using it, Metal3 canconcentrate on providing the best integration with Kubernetes.            &lt;script&gt;  ! function(t, e, a) {     use strict ;    var r = {};    var i =  mk-faqs ,      l = i +  __question ,      n = i +  __answer ,      o =  [data-aria-faq-heading] ,      d =  [data-aria-faq-panel] ,      c = 0;    r. create = function() {      var t, a, s, u, A, g, h =  none ,        b = e. querySelectorAll( [data-aria-faq] );      for (c += 1, g = 0; g &lt; b. length; g++) {        var f;        if ((t = b[g]). hasAttribute( id ) || (t. id =  acc_  + c +  -  + g), t. classList. add(i), e. querySelectorAll( #  + t. id +  &gt; li ). length ? (a = e. querySelectorAll( #  + t. id +   li &gt;   + d), s = e. querySelectorAll( #  + t. id +   li &gt;   + o)) : (a = e. querySelectorAll( #  + t. id +   &gt;   + d), s = e. querySelectorAll( #  + t. id +   &gt;   + o)), t. hasAttribute( data-default ) &amp;&amp; (h = t. getAttribute( data-default )), A = t. hasAttribute( data-constant ), t. hasAttribute( data-multi ), t. hasAttribute( data-transition )) {          var y = t. querySelectorAll(d);          for (f = 0; f &lt; y. length; f++) y[f]. classList. add(n +  --transition )        }        for (r. setupPanels(t. id, a, h, A), r. setupHeadingButton(s, A), u = e. querySelectorAll( #  + t. id +  &gt; li ). length ? e. querySelectorAll( #  + t. id +   li &gt;   + o +   .   + l) : e. querySelectorAll( #  + t. id +   &gt;   + o +   .   + l), f = 0; f &lt; u. length; f++) u[f]. addEventListener( click , r. actions), u[f]. addEventListener( keydown , r. keytrolls)      }    }, r. setupPanels = function(t, e, a, r) {      var i, l, o, d, c;      for (i = 0; i &lt; e. length; i++) o = t +  _panel_  + (i + 1), d = a, c = r, (l = e[i]). setAttribute( id , o), s(e[0], !0), l. classList. add(n),  none  !== d &amp;&amp; NaN !== parseInt(d) &amp;&amp; (d &lt;= 1 ? s(e[0], !1) : d - 1 &gt;= e. length ? s(e[e. length - 1], !1) : s(e[d - 1], !1)), (c &amp;&amp;  none  === d || NaN === parseInt(d)) &amp;&amp; s(e[0], !1)    }, r. setupHeadingButton = function(t, a) {      var r, i, n, o, d, c;      for (c = 0; c &lt; t. length; c++) i = (r = t[c]). nextElementSibling. id, n = e. getElementById(i). getAttribute( aria-hidden ), o = e. createElement( button ), d = r. textContent, r. innerHTML =   , o. setAttribute( type ,  button ), o. setAttribute( aria-controls , i), o. setAttribute( id , i +  _question ), o. classList. add(l),  false  === n ? (u(o, !0), g(o, !0), a &amp;&amp; o. setAttribute( aria-disabled ,  true )) : (u(o, !1), g(o, !1)), r. appendChild(o), o. appendChild(e. createTextNode(d))    }, r. actions = function(t) {      var a, i = this. id. replace(/_panel. *$/g,   ),        n = e. getElementById(this. getAttribute( aria-controls ));      a = e. querySelectorAll( #  + i +  &gt; li ). length ? e. querySelectorAll( #  + i +   li &gt;   + o +   .   + l) : e. querySelectorAll( #  + i +   &gt;   + o +   .   + l), t. preventDefault(), r. togglePanel(t, i, n, a)    }, r. togglePanel = function(t, a, r, i) {      var l, n, o = t. target;      if ( true  !== o. getAttribute( aria-disabled ) &amp;&amp; (l = o. getAttribute( aria-controls ), g(o,  true ),  true  === o. getAttribute( aria-expanded ) ? (u(o,  false ), s(r,  true )) : (u(o,  true ), s(r,  false ), e. getElementById(a). hasAttribute( data-constant ) &amp;&amp; A(o,  true )), e. getElementById(a). hasAttribute( data-constant ) || !e. getElementById(a). hasAttribute( data-multi )))        for (n = 0; n &lt; i. length; n++) o !== i[n] &amp;&amp; (g(i[n],  false ), l = i[n]. getAttribute( aria-controls ), A(i[n],  false ), u(i[n],  false ), s(e. getElementById(l),  true ))    }, r. keytrolls = function(t) {      if (t. target. classList. contains(l)) {        var a, r = t. keyCode || t. which,          i = this. id. replace(/_panel. *$/g,   );        switch (a = e. querySelectorAll( #  + i +  &gt; li ). length ? e. querySelectorAll( #  + i +   li &gt;   + o +   .   + l) : e. querySelectorAll( #  + i +   &gt;   + o +   .   + l), r) {          case 35:            t. preventDefault(), a[a. length - 1]. focus();            break;          case 36:            t. preventDefault(), a[0]. focus()        }      }    }, r. init = function() {      r. create()    };    var s = function(t, e) {        t. setAttribute( aria-hidden , e)      },      u = function(t, e) {        t. setAttribute( aria-expanded , e)      },      A = function(t, e) {        t. setAttribute( aria-disabled , e)      },      g = function(t, e) {        t. setAttribute( data-current , e)      };    r. init()  }(window, document);  &lt;/script&gt;"
    }, {
    "id": 31,
    "url": "/blog/",
    "title": "Blog",
    "author" : "",
    "tags" : "",
    "body": " -     Blog    Read about the newest updates in the community.     &lt;/section&gt;                   Metal3. io Becomes a CNCF Incubating Project:         Wednesday, 27/08/2025        By Honza Pokorný        We are pleased to share some incredible news with our community! The CNCF Technical Oversight Committee has officially voted to accept Metal3 as an incubating project. This milestone represents years of hard work, collaboration, and innovation, and we couldn’t be more excited about what lies ahead! Our Journey from Sandbox. . .         Read More                    Introducing Baremetal Operator end-to-end test suite:         Friday, 13/12/2024        By Lennart Jern        In the beginning, there was metal3-dev-env. It could set up a virtualized “baremetal” lab and test all the components together. As Metal3 matured, it grew in complexity and capabilities, with release branches, API versions, etc. Metal3-dev-env did everything from cloning the repositories and building the container images, to deploying the. . .         Read More                    Scaling Kubernetes with Metal3: Simulating 1000 Clusters with Fake Ironic Agents:         Thursday, 24/10/2024        By Huy Mai        If you’ve ever tried scaling out Kubernetes clusters in a bare-metal environment, you’ll know that large-scale testing comes with serious challenges. Most of us don’t have access to enough physical servers—or even virtual machines—to simulate the kinds of large-scale environments we need for stress testing, especially when deploying hundreds or. . .         Read More                    Scaling to 1000 clusters - Part 3:         Thursday, 30/05/2024        By Lennart Jern        In part 1, we introduced the Bare Metal Operator test mode and saw how it can be used to play with BareMetalHosts without Ironic and without any actual hosts. We continued in part 2 with how to fake workload clusters enough for convincing Cluster API’s controllers that they are healthy. . . .         Read More                    Metal3 at KubeCon EU 2024:         Wednesday, 10/04/2024        By Lennart Jern        The Metal3 project was present at KubeCon EU 2024 with multiple maintainers, contributors and users! For many of us, this was the first time we met in the physical world, despite working together for years already. This was very valuable and appreciated by many of us, I am sure. We. . .         Read More                                             1                            2                            3                            4                            5                            6                                                                                              Categories:             hybrid        cloud        metal3        baremetal        stack        edge        openstack        ironic        openshift        kubernetes        OpenStack        operator        summit        kubecon        shiftdev        metal3-dev-env        documentation        development        talk        conference        meetup        cluster API        provider        raw image        image streaming        IPAM        ip address manager        Pivoting        Move        scaling        cncf        community        announcement      "
    }, , {
    "id": 32,
    "url": "/privacy-statement.html",
    "title": "Privacy Statement",
    "author" : "",
    "tags" : "",
    "body": " -   Privacy Statement&lt;/section&gt;           Privacy Statement for the Metal3 Project: As Metal3. io and most of the infrastructure of the Metal3 Project arecurrently hosted by Red Hat Inc. , this site falls under theRed Hat Privacy Policy. All terms of that privacy policy apply to this site. Should we changeour hosting in the future, this Privacy Policy will be updated. How to Contact Us: If you have any questions about any of these practices or Metal3’s useof your personal information, please feel free to contactus or file anIssuein our GitHub repo. Metal3 will work with you to resolve any concerns you may have aboutthis Statement. Changes to this Privacy Statement: Metal3 reserves the right to change this policy from time to time. If wedo make changes, the revised Privacy Statement will be posted on thissite. A notice will be posted on our blog and/or mailing lists wheneverthis privacy statement is changed in a material way. This Privacy Statement was last amended on September 25, 2019. "
    }, , , , , , {
    "id": 33,
    "url": "/blog/page2/",
    "title": "Blog",
    "author" : "",
    "tags" : "",
    "body": " -     Blog    Read about the newest updates in the community.     &lt;/section&gt;       {% for post in paginator. posts %}            {{ post. title }}:         {{ post. date | date:  %A, %-d/%m/%Y  }}        By {{ post. author }}        {{ post. content | strip_html | truncatewords: 50 }}         Read More        {% endfor %}     {% if paginator. total_pages &gt; 1 %}           {% if paginator. previous_page %}                                                        {% endif %}      {% for page in (1. . paginator. total_pages) %}       {% if page == paginator. page %}        {{ page }}       {% elsif page == 1 %}        {{ page }}       {% else %}        {{ page }}       {% endif %}      {% endfor %}      {% if paginator. next_page %}                                                     {% endif %}          {% endif %}    {% include blog-aside. html %}"
    }, {
    "id": 34,
    "url": "/blog/page3/",
    "title": "Blog",
    "author" : "",
    "tags" : "",
    "body": " -     Blog    Read about the newest updates in the community.     &lt;/section&gt;       {% for post in paginator. posts %}            {{ post. title }}:         {{ post. date | date:  %A, %-d/%m/%Y  }}        By {{ post. author }}        {{ post. content | strip_html | truncatewords: 50 }}         Read More        {% endfor %}     {% if paginator. total_pages &gt; 1 %}           {% if paginator. previous_page %}                                                        {% endif %}      {% for page in (1. . paginator. total_pages) %}       {% if page == paginator. page %}        {{ page }}       {% elsif page == 1 %}        {{ page }}       {% else %}        {{ page }}       {% endif %}      {% endfor %}      {% if paginator. next_page %}                                                     {% endif %}          {% endif %}    {% include blog-aside. html %}"
    }, {
    "id": 35,
    "url": "/blog/page4/",
    "title": "Blog",
    "author" : "",
    "tags" : "",
    "body": " -     Blog    Read about the newest updates in the community.     &lt;/section&gt;       {% for post in paginator. posts %}            {{ post. title }}:         {{ post. date | date:  %A, %-d/%m/%Y  }}        By {{ post. author }}        {{ post. content | strip_html | truncatewords: 50 }}         Read More        {% endfor %}     {% if paginator. total_pages &gt; 1 %}           {% if paginator. previous_page %}                                                        {% endif %}      {% for page in (1. . paginator. total_pages) %}       {% if page == paginator. page %}        {{ page }}       {% elsif page == 1 %}        {{ page }}       {% else %}        {{ page }}       {% endif %}      {% endfor %}      {% if paginator. next_page %}                                                     {% endif %}          {% endif %}    {% include blog-aside. html %}"
    }, {
    "id": 36,
    "url": "/blog/page5/",
    "title": "Blog",
    "author" : "",
    "tags" : "",
    "body": " -     Blog    Read about the newest updates in the community.     &lt;/section&gt;       {% for post in paginator. posts %}            {{ post. title }}:         {{ post. date | date:  %A, %-d/%m/%Y  }}        By {{ post. author }}        {{ post. content | strip_html | truncatewords: 50 }}         Read More        {% endfor %}     {% if paginator. total_pages &gt; 1 %}           {% if paginator. previous_page %}                                                        {% endif %}      {% for page in (1. . paginator. total_pages) %}       {% if page == paginator. page %}        {{ page }}       {% elsif page == 1 %}        {{ page }}       {% else %}        {{ page }}       {% endif %}      {% endfor %}      {% if paginator. next_page %}                                                     {% endif %}          {% endif %}    {% include blog-aside. html %}"
    }, {
    "id": 37,
    "url": "/blog/page6/",
    "title": "Blog",
    "author" : "",
    "tags" : "",
    "body": " -     Blog    Read about the newest updates in the community.     &lt;/section&gt;       {% for post in paginator. posts %}            {{ post. title }}:         {{ post. date | date:  %A, %-d/%m/%Y  }}        By {{ post. author }}        {{ post. content | strip_html | truncatewords: 50 }}         Read More        {% endfor %}     {% if paginator. total_pages &gt; 1 %}           {% if paginator. previous_page %}                                                        {% endif %}      {% for page in (1. . paginator. total_pages) %}       {% if page == paginator. page %}        {{ page }}       {% elsif page == 1 %}        {{ page }}       {% else %}        {{ page }}       {% endif %}      {% endfor %}      {% if paginator. next_page %}                                                     {% endif %}          {% endif %}    {% include blog-aside. html %}"
    }, , ];

var idx = lunr(function () {
    this.ref('id')
    this.field('title', { boost: 2 })
    this.field('body')
    this.field('author')
    this.field('url')
    this.field('tags', { boost: 2 })
    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results </p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}

function getQueryVariable(variable) {
  var query = window.location.search.substring(1);
  var vars = query.split('&');

  for (var i = 0; i < vars.length; i++) {
    var pair = vars[i].split('=');

    if (pair[0] === variable) {
      return decodeURIComponent(pair[1].replace(/\+/g, '%20'));
    }
  }
}


var searchTerm = getQueryVariable('query');
if (searchTerm) {
  lunr_search(searchTerm)
}

</script>
<style>
    #lunrsearchresults {padding-top: 0.2rem;}
    .lunrsearchresult {padding-bottom: 1rem;}
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>
</main>
<footer class="mk-main-footer">
  <div>
    <div class="mk-cncf-footer">
      <p>We are a <a href="https://cncf.io/">Cloud Native Computing Foundation</a> sandbox project.</p>
      <p><img id= "cncf-image" src="/assets/images/cncf-color.png"/></p>
      <p>Copyright 2025 The Metal³ Contributors - <a href="/privacy-statement.html">Privacy Statement</a></p>
      <p>Copyright 2025 The Linux Foundation. All Rights Reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a> page.</p>
    </div>
      <div class="mk-icons-footer">
        <p>
<!--           <a href="https://twitter.com/metal3_io" aria-label="Visit us on Twitter">
            <i class="fab fa-twitter fa-lg"></i>
          </a> -->
          <a href="https://kubernetes.slack.com/messages/CHD49TLE7" data-placement="top" title="Join our Slack channel">
            <i class="fab fa-slack fa-lg"></i>
          </a>
          <a href="https://github.com/metal3-io" aria-label="View our repo on GitHub">
            <i class="fab fa-github fa-lg"></i>
          </a>
          <a href="https://groups.google.com/g/metal3-dev" aria-label="Send us an email">
            <i class="fas fa-envelope fa-lg"></i>
          </a>
          <a href="https://www.youtube.com/channel/UC_xneeYbo-Dl4g-U78xW15g/videos" aria-label="See our YouTube channel">
            <i class="fab fa-youtube fa-lg"></i>
          </a>
        </p>
      </div>
  </div>
</footer>
</div><!--wrapper-->
<script>
var toggle = document.querySelector('#toggle');
var menu = document.querySelector('#main_nav');
var menuItems = document.querySelectorAll('#main_nav li a');

toggle.addEventListener('click', function(){
if (menu.classList.contains('is-active')) {
  this.setAttribute('aria-expanded', 'false');
  menu.classList.remove('is-active');
} else {
  menu.classList.add('is-active');
  this.setAttribute('aria-expanded', 'true');
  //menuItems[0].focus();
}
});
</script>
    <script src="/assets/js/copy.js"></script>
    <!-- This comes from DTM/DPAL and must be latest entry in body-->
<script>
  let pageLocation = window.location.href
  let footerIcons = document.querySelector(".mk-icons-footer")

  if(pageLocation.includes("community-resources.html")){
    footerIcons.style.display = "none"
  }else {null}

</script>
    <script type="text/javascript">
        if (("undefined" !== typeof _satellite) && ("function" === typeof _satellite.pageBottom)) {
            _satellite.pageBottom();
        }
    </script>
</body>
</html>

