<!doctype html>
<html class="no-js" lang="en">

<head>
    <script id="dpal" src="//www.redhat.com/ma/dpal.js" type="text/javascript"></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="/assets/css/styles.css">
    <link rel="shortcut icon" type="image/png" href="/favicon.png">
    <meta name="theme-color" content="#008585">
    
    <title>MetalÂ³ - Metal Kubed</title>
    <!-- # Opengraph protocol properties: https://ogp.me/ -->
    <meta name="author" content="The MetalÂ³ - Metal Kubed website team, " >
    
    <meta name="twitter:card" content="summary">
    <meta name="description" content="Metal3.io aims to build on baremetal host provisioning technologies to provide a Kubernetes native API for managing bare metal hosts via a provisioning stack that is also running on Kubernetes.">
    <meta name="keywords" content="hybrid, cloud, metal3, baremetal, stack, edge, openstack, ironic, openshift, kubernetes, operator, summit, kubecon, shiftdev, metal3-dev-env, documentation, development, talk, conference, meetup, " >
    <meta property="og:title" content="MetalÂ³ - Metal Kubed">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://metal3.io/search.html" >
    <meta property="og:image" content="https://metal3.io/assets/images/metal3logo.png">
    <meta property="og:description" content="Metal3.io aims to build on baremetal host provisioning technologies to provide a Kubernetes native API for managing bare metal hosts via a provisioning stack that is also running on Kubernetes." >
    <meta property="og:site_name" content="MetalÂ³ - Metal Kubed" >
    <meta property="og:article:author" content="The MetalÂ³ - Metal Kubed website team" >
    <meta property="og:article:published_time" content="2020-03-03 07:42:34 +0000" >
    <meta name="twitter:title" content="MetalÂ³ - Metal Kubed">
    <meta name="twitter:description" content="Metal3.io aims to build on baremetal host provisioning technologies to provide a Kubernetes native API for managing bare metal hosts via a provisioning stack that is also running on Kubernetes.">

    <link type="application/atom+xml" rel="alternate" href="https://metal3.io/feed.xml" title="MetalÂ³ - Metal Kubed" />
    <meta name="google-site-verification" content="HCdbGknTOCTKQVt7m-VxTG4BEYXxSqm-sDb-iklqrB0" />
  <link href="https://fonts.googleapis.com/css?family=Nunito:200,400&display=swap" rel="stylesheet">
  <script defer src="https://use.fontawesome.com/releases/v5.1.0/js/all.js" integrity="sha384-3LK/3kTpDE/Pkp8gTNp2gR/2gOiwQ6QaO7Td0zV76UFJVhqLl4Vl3KL1We6q6wR9" crossorigin="anonymous"></script>
  <!-- Photoswipe.com gallery-->

  <!-- Core CSS file -->
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe.min.css">

  <!-- Skin CSS file (styling of UI - buttons, caption, etc.)
      In the folder of skin CSS file there are also:
      - .png and .svg icons sprite,
      - preloader.gif (for browsers that do not support CSS animations) -->
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/default-skin/default-skin.min.css">
</head>
<body>
    <!--[if IE]>
      <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
    <![endif]-->

<div class="mk-wrapper">
    <section class="mk-masthead mk-masthead--sub">
<header class="mk-main-header">
    <a href="/" class="mk-main-header__brand">
        <svg version="1.1" viewBox="0 0 557 540" xmlns="http://www.w3.org/2000/svg">
          <g fill="none" fill-rule="evenodd">
            <g transform="translate(-1)" fill-rule="nonzero">
            <path d="m181.91 539.68h-0.7c-0.76 0-1.44-0.11-2-0.17h-0.14l-1.62-0.2c-15.204-1.867-29.364-8.7129-40.27-19.47l-1.07-1.06-49.46-61.26-73.34-90.59-0.5-0.72c-2.8927-4.0899-5.2989-8.503-7.17-13.15-1.0257-2.532-1.8875-5.1274-2.58-7.77v-0.11c-0.22-0.85-0.43-1.69-0.62-2.56v-0.14c-0.8042-3.5966-1.2861-7.2578-1.44-10.94v-0.47-0.48c-0.067687-4.136 0.26722-8.2688 1-12.34l0.11-0.61 14.51-63.64 28.72-126c4.0017-17.442 15.802-32.074 32-39.68l178.2-85.93 3.34-0.67c5.6926-1.1418 11.484-1.7201 17.29-1.7201h2.83 0.57c8.4518-0.016309 16.808 1.7879 24.5 5.2901l0.47 0.22 175.82 84.2 0.48 0.25c7.1101 3.7526 13.491 8.7481 18.84 14.75 2.7886 3.1018 5.2639 6.4715 7.39 10.06l0.17 0.29c2.1776 3.7964 3.9314 7.8205 5.23 12l0.3 1 44.23 190.25 0.17 1.42c2.0399 16.443-2.1677 33.052-11.79 46.54l-0.48 0.68-121.46 150.24c-7.2792 9.604-17.475 16.59-29.06 19.91-0.93 0.27-1.87 0.52-2.81 0.75l-0.3 0.07c-5.0328 1.1701-10.183 1.76-15.35 1.76h-194.01z" fill="#fff"/>
            <path d="m492 131.65c-0.75221-2.3458-1.7582-4.6025-3-6.73-1.2507-2.1148-2.7114-4.0982-4.36-5.92-3.3032-3.7145-7.2456-6.8067-11.64-9.13l-179.82-86c-4.3569-1.9816-9.0938-2.9883-13.88-2.95h-0.77c-4.9428-0.19294-9.8909 0.20318-14.74 1.18l-179.72 86.6c-8.9642 4.1124-15.498 12.17-17.67 21.79l-3.69 16.16 216.29 117.67 0.34-0.18 217.22-112.72-4.56-19.77z" fill="#00E0C1"/>
            <path d="m279 264.32l-216.29-117.67-25.77 113.1-14.73 64.63c-0.44744 2.4671-0.64178 4.9734-0.58 7.48v0.29c0.072639 2.1493 0.33702 4.2878 0.79 6.39 0.12 0.56 0.26 1.1 0.4 1.65 0.41228 1.5719 0.92671 3.1151 1.54 4.62 1.1152 2.7748 2.5517 5.4095 4.28 7.85l23.69 29.27 51 63 49.67 61.55c6.7982 6.7311 15.643 11.009 25.14 12.16 0.67 0 1.31 0.18 2 0.21h99.17v-254.34l-0.31-0.19z" fill="#00EEC4"/>
            <path d="m536.75 324.38l-40.19-173-217.23 112.76v254.71h98.82c3.2616 0.017 6.5139-0.34884 9.69-1.0906 0.62-0.15 1.23-0.31 1.84-0.49 6.2438-1.7629 11.72-5.5604 15.56-10.79l66.09-81.75 31.31-38.73 26.94-33.33c5.8432-8.2018 8.4013-18.295 7.17-28.29z" fill="#00D1BD"/>
            <path d="m120.94 369l137 75.89c1.3702 0.76284 3.0421 0.74251 4.3933-0.05344s2.1796-2.2483 2.1767-3.8166v-161.02c0-5.718-3.1489-10.971-8.19-13.67l-1.64-0.87-134.68-71.99c-0.8041-0.43178-1.7757-0.41032-2.56 0.056543-0.78426 0.46687-1.2663 1.3108-1.27 2.2235l-0.94 163.17c0.02 3.63 2.77 8.44 5.71 10.08z" fill="#fff"/>
            <path d="m282.61 103.85c-4.0372-0.033083-8.0333 0.81323-11.71 2.481l-134.2 60.47c-0.91184 0.40637-1.512 1.2973-1.5476 2.295-0.032512 0.99771 0.50554 1.9274 1.3876 2.395l135.72 72.51c0.15 0.09 0.31 0.16 0.47 0.24l0.59 0.29 0.26 0.11 0.8 0.34h0.09c4.9879 1.8704 10.539 1.5061 15.24-1l139.14-73.94c1.1079-0.5822 1.7814-1.7504 1.7328-3.0009-0.054039-1.2505-0.82096-2.3596-1.9728-2.8491l-135.06-58.05c-3.4545-1.4945-7.1761-2.2735-10.94-2.291z" fill="#fff"/>
            <path d="m442.82 192.61c-1.08-0.49333-2.4133-0.29667-4 0.59l-24.52 13.54c-3.6117 1.9922-6.3845 5.2194-7.81 9.09l-37.49 87.55-37.31-46.2c-1.59-2.29-4.2-2.45-7.81-0.45l-24.51 13.54c-1.6358 0.9266-3.0116 2.2508-4 3.85-1.0039 1.4454-1.5667 3.151-1.62 4.91v166.83c0 1.59 0.55 2.59 1.63 3s2.42 0.19 4-0.69l27.34-15.1c1.6143-0.90735 2.9864-2.1902 4-3.74 1.0178-1.3976 1.5863-3.0717 1.63-4.8v-105l23.21 30.12c2.1667 2.1267 4.6967 2.3933 7.59 0.8l11.67-6.45c3.18-1.7467 5.71-4.8067 7.59-9.18l23.43-55.9 0.25 97.1 0.17 8.31c-0.10795 1.217 0.57186 2.3675 1.69 2.86 1.2747 0.44018 2.6836 0.23517 3.78-0.55l27.27-15.83c1.5869-0.9387 2.9245-2.2455 3.9-3.81 0.9881-1.4207 1.5184-3.1095 1.5212-4.84v-166.43c0.028815-1.6-0.51118-2.64-1.6012-3.12z" fill="#fff"/>
            </g>
          </g>
        </svg>
      </a>
      <nav role="navigation" class="mk-main-header__nav-wrapper">
        <button class="mk-main-header__toggle" id="toggle" aria-controls="main_nav" aria-expanded="false" aria-label="navigation toggle" >
          <svg version="1.1" viewBox="0 0 512 448" xmlns="http://www.w3.org/2000/svg">
          <g>
          <path d="m296 0h192c13.255 0 24 10.745 24 24v160c0 13.255-10.745 24-24 24h-192c-13.255 0-24-10.745-24-24v-160c0-13.255 10.745-24 24-24zm-80 0h-192c-13.255 0-24 10.745-24 24v160c0 13.255 10.745 24 24 24h192c13.255 0 24-10.745 24-24v-160c0-13.255-10.745-24-24-24zm-216 264v160c0 13.255 10.745 24 24 24h192c13.255 0 24-10.745 24-24v-160c0-13.255-10.745-24-24-24h-192c-13.255 0-24 10.745-24 24zm296 184h192c13.255 0 24-10.745 24-24v-160c0-13.255-10.745-24-24-24h-192c-13.255 0-24 10.745-24 24v160c0 13.255 10.745 24 24 24z"/>
          </g>
          </svg>
          <span class="mk-main-header__toggle-text">menu</span>
        </button>
        <ul id="main_nav" class="mk-main-nav">
          <li ><a class="mk-main-nav__item" href="/blog/index.html">Blog</a></li>
          <li ><a class="mk-main-nav__item" href="/community-resources.html">Community Resources</a></li>
          <li ><a class="mk-main-nav__item" href="/documentation.html">Documentation</a></li>
          <li ><a class="mk-main-nav__item" href="/try-it.html">Try It!</a></li>
          <li  class="active" >
            <form action="/search.html" method="get" autocomplete="off">
              <div class="autocomplete" style="width:150px;">
                <input type="text" id="search-input" class="docs-search--input" placeholder="search term" name="query">
              </div>
              <input id="search-button" type="submit" value="ğŸ”" disabled='true'>
            </form>
          </li>
      </li>


        </ul>
      </nav>
  </header>
  
<script>
function autocomplete(inp, arr) {
  /*the autocomplete function takes two arguments,
  the text field element and an array of possible autocompleted values:*/
  var currentFocus;
  /*execute a function when someone writes in the text field:*/
  inp.addEventListener("input", function(e) {
      var a, b, i, val = this.value;
      /*close any already open lists of autocompleted values*/
      closeAllLists();
      if (!val) { return false;}
      currentFocus = -1;
      /*create a DIV element that will contain the items (values):*/
      a = document.createElement("DIV");
      a.setAttribute("id", this.id + "autocomplete-list");
      a.setAttribute("class", "autocomplete-items");
      /*append the DIV element as a child of the autocomplete container:*/
      this.parentNode.appendChild(a);
      /*for each item in the array...*/
      for (i = 0; i < arr.length; i++) {
        /*check if the item starts with the same letters as the text field value:*/
        if (arr[i].substr(0, val.length).toUpperCase() == val.toUpperCase()) {
          /*create a DIV element for each matching element:*/
          b = document.createElement("DIV");
          /*make the matching letters bold:*/
          b.innerHTML = "<strong>" + arr[i].substr(0, val.length) + "</strong>";
          b.innerHTML += arr[i].substr(val.length);
          /*insert a input field that will hold the current array item's value:*/
          b.innerHTML += "<input type='hidden' value='" + arr[i] + "'>";
          /*execute a function when someone clicks on the item value (DIV element):*/
              b.addEventListener("click", function(e) {
              /*insert the value for the autocomplete text field:*/
              inp.value = this.getElementsByTagName("input")[0].value;
              /*close the list of autocompleted values,
              (or any other open lists of autocompleted values:*/
              closeAllLists();
          });
          a.appendChild(b);
        }
      }
  });
  /*execute a function presses a key on the keyboard:*/
  inp.addEventListener("keydown", function(e) {
      document.getElementById("search-button").disabled= undefined;
      var x = document.getElementById(this.id + "autocomplete-list");
      if (x) x = x.getElementsByTagName("div");
      if (e.keyCode == 40) {
        /*If the arrow DOWN key is pressed,
        increase the currentFocus variable:*/
        currentFocus++;
        /*and and make the current item more visible:*/
        addActive(x);
      } else if (e.keyCode == 38) { //up
        /*If the arrow UP key is pressed,
        decrease the currentFocus variable:*/
        currentFocus--;
        /*and and make the current item more visible:*/
        addActive(x);
      } else if (e.keyCode == 13) {
        /*If the ENTER key is pressed, prevent the form from being submitted,*/
        if (currentFocus > -1) {
          /*and simulate a click on the "active" item:*/
          if (x) {
            x[currentFocus].click();
            e.preventDefault();
          }
        }
        if (document.getElementById("search-input").value == "") {
          e.preventDefault();
        }
      }
  });
  function addActive(x) {
    /*a function to classify an item as "active":*/
    if (!x) return false;
    /*start by removing the "active" class on all items:*/
    removeActive(x);
    if (currentFocus >= x.length) currentFocus = 0;
    if (currentFocus < 0) currentFocus = (x.length - 1);
    /*add class "autocomplete-active":*/
    x[currentFocus].classList.add("autocomplete-active");
  }
  function removeActive(x) {
    /*a function to remove the "active" class from all autocomplete items:*/
    for (var i = 0; i < x.length; i++) {
      x[i].classList.remove("autocomplete-active");
    }
  }
  function closeAllLists(elmnt) {
    /*close all autocomplete lists in the document,
    except the one passed as an argument:*/
    var x = document.getElementsByClassName("autocomplete-items");
    for (var i = 0; i < x.length; i++) {
      if (elmnt != x[i] && elmnt != inp) {
      x[i].parentNode.removeChild(x[i]);
    }
  }
}
/*execute a function when someone clicks in the document:*/
document.addEventListener("click", function (e) {
    closeAllLists(e.target);
});
}
</script>
<script>
var mykeywords = ["hybrid", "cloud", "metal3", "baremetal", "stack", "edge", "openstack", "ironic", "openshift", "kubernetes", "operator", "summit", "kubecon", "shiftdev", "metal3-dev-env", "documentation", "development", "talk", "conference", "meetup", ]
autocomplete(document.getElementById("search-input"), mykeywords);
</script>
<script src="/assets/js/clipboard.min.js"></script>
<!-- Photoswipe -->
<!-- Core JS file -->
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe.min.js"></script>
<!-- UI JS file -->
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>

<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="/assets/js/lunr.min.js"></script>

<div class="mk-masthead__content--sub">
        <h1 class="mk-masthead__content--sub__title">Search results</h1>
</div>
</section>
<main class="mk-main mk-blog">
            <article class="mk-main__section mk-main__content mk-main__section__content">
    <div class="container post">
      <h1 class="page-title"></h1>
      <article class="post-content">
        <div id="lunrsearchresults">
            <ul></ul>
        </div>
      </article>
    </div>

</article>
<nav class="mk-pagination">
        
        
</nav>

    </main>



<script>

var documents = [{
    "id": 0,
    "url": "/blog/2020/02/27/talk-kubernetes-finland-metal3.html",
    "title": "MetalÂ³: Kubernetes Native Bare Metal Cluster Management - MaÃ«l Kimmerlin - Kubernetes and CNCF Finland Meetup",
    "author" : "Alberto Losada",
    "tags" : "metal3, baremetal, talk, conference, kubernetes, meetup",
    "body": "Conference talk: MetalÂ³: Kubernetes Native Bare Metal Cluster Management - MaÃ«l Kimmerlin: On the 20th of January at the Kubernetes and CNCF Finland Meetup, MaÃ«l Kimmerlin gave a brilliant presentation about the status of the MetalÂ³ project. In this presentation, MaÃ«l starts giving a short introduction of the Cluster API project which provides a solid foundation to develop the MetalÂ³ Bare Metal Operator (BMO). The talk basically focuses on the v1alpha2 infrastructure provider features from the Cluster API. Information The video recording from the â€œKubernetes and CNFC Finland Meetupâ€ is composed by three talks. The video embedded starts with MaÃ«lâ€™s talk. Warning Playback of the video has been disabled by the author. Click on play button and then on â€œWatch this video on Youtubeâ€ link once it appears.  During the first part of the presentation, a detailed explanation of the different Kubernetes Custom Resource Definitions (CRDs) inside MetalÂ³ is shown and also how they are linked with the Cluster API project. As an example, the image below shows the interaction between objects and controllers from both projects: Once finished the introductory part, MaÃ«l focuses on the main components of the MetalÂ³ BMO and the provisioning process. This process starts with introspection, where the bare metal server is registered by the operator. Then, the Ironic Python Agent (IPA) image is executed to collect all hardware information from the server.  The second part of the process is the provisioning. In this step, MaÃ«l explains how the Bare Metal Operator (BMO) is in charge along with Ironic to present the Operating System image to the physical server and complete its installation.  Next, MaÃ«l deeply explains each Custom Resource (CR) used during the provisioning of target Kubernetes clusters in bare metal servers. He refers to objects such as Cluster, BareMetalCluster, Machine, BareMetalMachine, BareMetalHost and so on. Each one is clarified with a YAML file definition of a real case and a workflow diagram that shows the reconciliation procedure. Last part of the talk is dedicated to execute a demo where MaÃ«l creates a target Kubernetes cluster from a running minikube VM (also called bootstrap cluster) where MetalÂ³ is deployed. As it is pointed out in the video, the demo is running in emulated hardware. Actually, something similar to the metal3-dev-env project which can be used to reproduce the demo. More information of the MetalÂ³ development environment (metal3-dev-env) can be found in the MetalÂ³ try-it section. In case you want to go deeper, take a look at the blog post A detailed walkthrough of the MetalÂ³ development environment. At the end, the result is a new Kubernetes cluster up and running. The cluster is deployed on two emulated physical servers: one runs as the control-plane node and the other as a worker node. Information The slides of the talk can be downloaded from here Speakers: MaÃ«l Kimmerlin MaÃ«l Kimmerlin is a Senior Software Engineer at Ericsson. In his own words: I am an open-source enthusiast, focusing in Ericsson on Life Cycle Management of Kubernetes clusters on Bare Metal. I am very interested in the Cluster API project from the Kubernetes Lifecycle SIG, and active in its Bare Metal provider, that is MetalÂ³, developing and encouraging the adoption of this project. References:  Video: MetalÂ³: Kubernetes Native Bare Metal Cluster Management Slides"
    }, {
    "id": 1,
    "url": "/blog/2020/02/18/metal3-dev-env-install-deep-dive.html",
    "title": "A detailed walkthrough of the MetalÂ³ development environment",
    "author" : "Alberto Losada",
    "tags" : "metal3, baremetal, metal3-dev-env, documentation, development",
    "body": "Introduction to metal3-dev-env: metal3-dev-env is a collection of scripts in a Github repository inside the MetalÂ³ project that aims to allow contributors and other interested users to run a fully functional MetalÂ³ environment for testing and have a first contact with the project. Actually, metal3-dev-env sets up an emulated environment which creates a set of virtual machines (VMs) to manage as if they were bare metal hosts. Warning This is not an installation that is supposed to be run in production. Instead, it is focused on providing a development environment to test and validate new features. The metal3-dev-env repository includes a set of scripts, libraries and resources used to set up a MetalÂ³ development environment. On the MetalÂ³ website there is already a documented process on how to use the metal3-dev-env scripts to set up a fully functional cluster to test the functionality of the MetalÂ³ components. This procedure at 10,000 foot view is composed by 3 bash scripts plus a verification one:  01_prepare_host. sh - Mainly installs all needed packages.  02_configure_host. sh - Basically create a set of VMs that will be managed as if they were bare metal hosts. It also downloads some images needed for Ironic.  03_launch_mgmt_cluster. sh - Launches a management cluster using minikube and runs the baremetal-operator on that cluster.  04_verify. sh - Finally runs a set of tests that verify that the deployment completed successfullyIn this blog post we are going to expand the information and provide some hints and recommendations. Warning MetalÂ³ project is changing rapidly, so probably this information is valuable in the short term. In any case, it is encouraged to double check that the information provided is still valid. Before get down to it, it is worth defining the nomenclature used along the blog post:  Host. It is the server where the virtual environment is running. In this case it is a physical PowerEdge M520 with 2 x Intel(R) Xeon(R) CPU E5-2450 v2 @ 2. 50GHz, 96GB RAM and a 140GB drive running CentOS 7 latest. Do not panic, lab environment should work with lower resources as well.  Virtual bare metal hosts. These are the virtual machines (KVM based) that are running on the host which are emulating physical hosts in our lab. They are also called bare metal hosts even if they are not physical servers.  Management or bootstrap cluster. It is a fully functional Kubernetes cluster in charge of running all the necessary MetalÂ³ operators and controllers to manage the infrastructure. In this case it is the minikube virtual machine.  Target cluster. It is the Kubernetes cluster created from the management one. It is provisioned and configured using a native Kubernetes API for that purpose. Create the MetalÂ³ laboratory: Information A non-root user must exist in the host with passwordless sudo access. This user is in charge of running the metal3-dev-env scripts. First thing that needs to be done is, obviously, cloning the metal3-dev-env repository: [alosadag@eko1: ~]$ git clone https://github. com/metal3-io/metal3-dev-env. gitCloning into 'metal3-dev-env'. . . remote: Enumerating objects: 22, done. remote: Counting objects: 100% (22/22), done. remote: Compressing objects: 100% (22/22), done. remote: Total 1660 (delta 8), reused 8 (delta 0), pack-reused 1638Receiving objects: 100% (1660/1660), 446. 08 KiB | 678. 00 KiB/s, done. Resolving deltas: 100% (870/870), done. Before starting to deploy the MetalÂ³ environment, it makes sense to detail a series of scripts inside the library folder that will be sourced in every step of the installation process. They are called shared libraries. [alosadag@eko1:~]$ ls -1 metal3-dev-env/lib/common. shimages. shlogging. shnetwork. shShared libraries: Although there are several scripts placed inside the lib folder that are sourced in some of the deployment steps, common. sh and logging. sh are the only ones used in all of the executions during the installation process. common. sh: The first time this library is run, a new configuration file is created with several variables along with their default values. They will be used during the installation process. On the other hand, if the file already exists, then it just sources theÂ values configured. The configuration file is created inside the cloned folder with config_$USER as file name. [alosadag@eko1 metal3-dev-env]$ Â ls config_*config_alosadag. shThe configuration file contains multiple variables that will be used during the set up. Some of them are detailed in the setup section of the MetalÂ³ try-it web page. In case you need to add or change global variables it should be done in this config file. Note I personally recommend modify or add variables in this config file instead of exporting them in the shell. By doing that, it is assured that they are persisted [alosadag@eko1 metal3-dev-env]$ cat ~/metal3-dev-env/config_alosadag. sh#!/bin/bash## This is the subnet used on the  baremetal  libvirt network, created as the# primary network interface for the virtual bare metalhosts. ## Default of 192. 168. 111. 0/24 set in lib/common. sh##export EXTERNAL_SUBNET= 192. 168. 111. 0/24 ## This SSH key will be automatically injected into the provisioned host# by the provision_host. sh script. ## Default of ~/. ssh/id_rsa. pub is set in lib/common. sh##export SSH_PUB_KEY=~/. ssh/id_rsa. pub. . . The common. sh library also makes sure there is an ssh public key available in the userâ€™s ssh folder. This key will be injected by cloud-init in all the virtual bare metal machines that will be configured later. Then, the user that executed the metal3-dev-env scripts is able to access the target cluster through ssh. This common. sh library also sets more global variables apart from the those in the config file. Note that these variables can be added to the config file along with the proper values for your environment.       Name of the variable   Default value         SSH_KEY   ${HOME}/. ssh/id_rsa       SSH_PUB_KEY   ${SSH_KEY}. pub       NUM_NODES   2       VM_EXTRADISKS   false       DOCKER_REGISTRY_IMAGE   docker. io/registry:latest       VBMC_IMAGE   quay. io/metal3-io/vbmc       SUSHY_TOOLS_IMAGE   quay. io/metal3-io/sushy-tools       IPA_DOWNLOADER_IMAGE   quay. io/metal3-io/ironic-ipa-downloader       IRONIC_IMAGE   quay. io/metal3-io/ironic       IRONIC_INSPECTOR_IMAGE   quay. io/metal3-io/ironic-inspector       BAREMETAL_OPERATOR_IMAGE   quay. io/metal3-io/baremetal-operator       CAPI_VERSION   v1alpha1       CAPBM_IMAGE   quay. io/metal3-io/cluster-api-provider-baremetal:v1alpha1       CAPBM_IMAGE   quay. io/metal3-io/cluster-api-provider-baremetal       DEFAULT_HOSTS_MEMORY   8192       CLUSTER_NAME   test1       KUBERNETES_VERSION   v1. 17. 0       KUSTOMIZE_VERSION   v3. 2. 3   Information It is important to mention that there are several basic functions defined in this file that will be used by the rest of scripts. logging. sh: This script ensures that there is a log folder where all the information gathered during theÂ execution of the scripts is stored. If there is any issue during the deployment, this is one of the first places to look at. [alosadag@eko1 metal3-dev-env]$ ls -1 logs/01_prepare_host-2020-02-03-122452. log01_prepare_host-2020-02-03-122956. loghost_cleanup-2020-02-03-122656. logFirst step: Prepare the host: In this first step (01_prepare_host. sh), the requirements needed to start the preparation of the host where the virtual bare metal hosts will run are fulfilled. Depending on the hostâ€™s operating system (OS), it will trigger a specific script for CentOS/Red Hat or Ubuntu.  note: â€œNoteâ€Currently CentOS Linux 7, Red Hat Enterprise Linux 8 and Ubuntu have been tested. There is work in progress to adapt the deployment for CentOS Linux 8. As stated previously, CentOS 7 is the operating system chosen to run in both, the host and virtual servers. Therefore, specific packages of the operating system are applied in the following script:    centos_install_requirements. sh  This script enables the epel and tripleo (current-tripleo) repositories where several packages are installed: dnf, ansible, wget, python3 and python related packages such as python-virtualbmc from tripleo repository. Note Notice that SELinux is set to permissive and an OS update is triggered, which will cause several packages to be upgraded since there are newer packages in the tripleo repositories (mostly python related) than in the rest of enabled repositories. At this point, the container runtime is also installed. Note that by setting the variable CONTAINER_RUNTIME defined in common. sh is possible to choose between docker and podman, which is the default for CentOS. Remember that this behaviour can be overwriten in your config file. Once the specific requirements for the elected operating system are accomplished, the download of several external artifacts is executed. Actually minikube, kubectl and kustomize are downloaded from the internet. Notice that the version of Kustomize and Kubernetes are defined by KUSTOMIZE_VERSION and KUBERNETES_VERSION variables inside common. sh, but minikube is always downloading the latest version available. Next step deals with cleaning ironic containers and pods that could be running in the host from failed deployments. This will ensure that there will be no issues when creating ironic-pod and infra-pod a little bit later in this first step.    network. sh.   At this point, the network library script is sourced. As expected, this library deals with the network configuration which includes: IP addresses, network definitions and IPv6 support which is disabled by default by setting PROVISIONING_IPV6 variable:             Name of the variable    Default value    Option          PROVISIONING_NETWORK    172. 22. 0. 0/24    This is the subnet used to run the OS provisioning process          EXTERNAL_SUBNET    192. 168. 111. 0/24    This is the subnet used on the â€œbaremetalâ€ libvirt network, created as the primary network interface for the virtual bare metal hosts          LIBVIRT_FIRMWARE    bios    Â           PROVISIONING_IPV6    false    Â        Below it is depicted a network diagram of the different virtual networks and virtual servers involved in the MetalÂ³ environment:    images. sh.   The images. sh library file is sourced as well in the 01_prepare_host. sh script . The images. sh script contains multiple variables that set the URL (IMAGE_LOCATION), name (IMAGE_NAME) and default username (IMAGE_USERNAME) of the cloud image that needs to be downloaded. The values of each variable will differ depending on the operating system of the virtual bare metal hosts. Note that these images will be served from the host to the virtual servers through the provisioning network.  In our case, since CentOS 7 is the base operating system, values will be defined as:             Name of the variable    Default value          IMAGE_NAME    CentOS-7-x86_64-GenericCloud-1907. qcow2          IMAGE_LOCATION    http://cloud. centos. org/centos/7/images          IMAGE USERNAME    centos      Information In case it is expected to use a custom cloud image, just modify the previous variables to match the right location. Now that the cloud image is defined, the download process can be started. First, a folder defined by IRONIC_IMAGE_DIR should exist so that the image (CentOS-7-x86_64-GenericCloud-1907. qcow2) and its checksum can be stored. This folder and its content will be exposed through a local ironic container running in the host.       Name of the variable   Default value       IRONIC_IMAGE_DIR   /opt/metal3-dev-env/ironic/html/images   Below it is verified that the cloud image files were downloaded successfully in the defined folder: [alosadag@eko1 metal3-dev-env]$ ll /opt/metal3-dev-env/ironic/html/imagestotal 920324-rw-rw-r--. 1 alosadag alosadag 942407680 FebÂ  3 12:39 CentOS-7-x86_64-GenericCloud-1907. qcow2-rw-rw-r--. 1 alosadag alosadagÂ  Â  Â  Â  33 Feb 3 12:39 CentOS-7-x86_64-GenericCloud-1907. qcow2. md5sumOnce the images. sh shared script is sourced, the following container images are pre-cached locally to the host in order to speed up things later. Below it is shown the code snippet in charge of that task: + for IMAGE_VAR in IRONIC_IMAGE IPA_DOWNLOADER_IMAGE VBMC_IMAGE SUSHY_TOOLS_IMAGE DOCKER_REGISTRY_IMAGE+ IMAGE=quay. io/metal3-io/ironic+ sudo podman pull quay. io/metal3-io/ironic. . . . . . . The container image location of each one is defined by their respective variables:       Name of the variable   Default value       VBMC_IMAGE   quay. io/metal3-io/vbmc       SUSHY_TOOLS_IMAGE   quay. io/metal3-io/sushy-tools       IPA_DOWNLOADER_IMAGE   quay. io/metal3-io/ironic-ipa-downloader       IRONIC_IMAGE   quay. io/metal3-io/ironic       DOCKER_REGISTRY_IMAGE   docker. io/registry:latest   Information In case it is expected to modify the public container images to test new features, it is worth mentioning that there is a container registry running as a privileged container in the host. Therefore it is recommended to upload your modified images there and just overwrite the previous variables to match the right location. At this point, an Ansible role is run locally in order to complete the local configuration. ansible-playbook \ -e  working_dir=$WORKING_DIR  \ -e  virthost=$HOSTNAME  \ -i vm-setup/inventory. ini \ -b -vvv vm-setup/install-package-playbook. ymlThis playbook imports two roles. One called packages_installation, which is in charge of installing a few more packages. The list of packages installed are listed as default Ansible variables in the vm-setup role inside the metal3-dev-env repository. The other role is based on the fubarhouse. golang Ansible Galaxy role. It is in charge of installing and configuring the exact golang version 1. 12. 12 defined in an Ansible variable in the install-package-playbook. yml playbook Once the playbook is finished, a pod called ironic-pod is created. Inside that pod, a privileged ironic-ipa-downloader container is started and attached to the host network. This container is in charge of downloading the Ironic Python Agent (IPA) files to a shared volume defined by IRONIC_IMAGE_DIR. This folder is exposed by the ironic container through HTTP. Information The Ironic Python Agent is an agent for controlling and deploying Ironic controlled baremetal nodes. Typically run in a ramdisk, the agent exposes a REST API for provisioning servers. See below the code snippet that fullfil the task: sudo podman run -d --net host --privileged --name ipa-downloader --pod ironic-pod -e IPA_BASEURI= -v /opt/metal3-dev-env/ironic:/shared quay. io/metal3-io/ironic-ipa-downloader /usr/local/bin/get-resource. shBelow, it is shown the status of the pods and containers at this point: [root@eko1 metal3-dev-env]# podman pod list --ctr-namesPOD ID Â  Â  Â  Â  NAME     STATUSÂ  Â  CREATED   CONTAINER INFO Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  INFRA ID5a0d475351aa Â  ironic-pod Â  Running Â  6 days ago Â  [5a0d475351aa-infra] [ipa-downloader]Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  18f3a8f61407The process will wait until the ironic-python-agent (IPA) initramfs, kernel and headers files are downloaded successfully. See below the files downloaded along with the CentOS 7 cloud image: [alosadag@eko1 metal3-dev-env]$ ll /opt/metal3-dev-env/ironic/html/imagestotal 920324-rw-rw-r--. 1 alosadag alosadag 942407680 FebÂ  3 12:39 CentOS-7-x86_64-GenericCloud-1907. qcow2-rw-rw-r--. 1 alosadag alosadagÂ  Â  Â  Â  33 Feb 3 12:39 CentOS-7-x86_64-GenericCloud-1907. qcow2. md5sumdrwxr-xr-x. 2 root Â  Â  root      147 Feb 3 12:41 ironic-python-agent-1862d000-59d9fdc6304b1lrwxrwxrwx. 1 root Â  Â  root      72 Feb 3 12:41 ironic-python-agent. initramfs -&gt; ironic-python-agent-1862d000-59d9fdc6304b1/ironic-python-agent. initramfslrwxrwxrwx. 1 root Â  Â  root      69 Feb 3 12:41 ironic-python-agent. kernel -&gt; ironic-python-agent-1862d000-59d9fdc6304b1/ironic-python-agent. kernellrwxrwxrwx. 1 root Â  Â  root      74 Feb 3 12:41 ironic-python-agent. tar. headers -&gt; ironic-python-agent-1862d000-59d9fdc6304b1/ironic-python-agent. tar. headersAfterwards, the script makes sure that libvirt is running successfully on the host and the non-privilege user has permissions to interact with it. Libvirt daemon should be running so that minikube can be installed successfully. See the following script snippet starting the minikube VM: + sudo su -l -c 'minikube start --insecure-registry 192. 168. 111. 1:5000'* minikube v1. 6. 2 on Centos 7. 7. 1908* Selecting 'kvm2' driver from user configuration (alternates: [none])In the same way as with the host, container images are pre-cached but in this case inside minikube local image repository. Â Notice that in this case the Bare Metal operator (BMO) is also downloaded since it will run on minikube. The container location is defined by BAREMETAL_OPERATOR_IMAGE. In case you want to test new features or new fixes to the BMO, just change the value of the variable to match the location of the modified image:       Name of the variable   Default value       BAREMETAL_OPERATOR_IMAGE   quay. io/metal3-io/baremetal-operator   Note Remember that minikube is the management cluster in our environment. So it must run all the operators and controllers needed for MetalÂ³. Below it is shown the output of the script once all the container images have been pulled to minikube: + sudo su -l -c 'minikube ssh sudo docker image ls' alosadagREPOSITORYÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  TAG         IMAGE ID      CREATED Â  Â  Â  Â  Â  Â  SIZEquay. io/metal3-io/ironicÂ  Â  Â  Â  Â  Â  Â  Â  Â  latest       e5d81adf05ee    26 hours agoÂ  Â  Â  Â  693MBquay. io/metal3-io/ironic-ipa-downloader Â  latest       d55b0dac2144    6 days agoÂ  Â  Â  Â  Â  239MBquay. io/metal3-io/ironic-inspectorÂ  Â  Â  Â  latest       8bb5b844ada6    6 days agoÂ  Â  Â  Â  Â  408MBquay. io/metal3-io/baremetal-operatorÂ  Â  Â  latest       3c692a32ddd6    9 days agoÂ  Â  Â  Â  Â  1. 77GBk8s. gcr. io/kube-proxy Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  v1. 17. 0       7d54289267dc    7 weeks ago Â  Â  Â  Â  116MBk8s. gcr. io/kube-controller-managerÂ  Â  Â  Â  v1. 17. 0       5eb3b7486872    7 weeks ago Â  Â  Â  Â  161MBk8s. gcr. io/kube-scheduler Â  Â  Â  Â  Â  Â  Â  Â  v1. 17. 0       78c190f736b1    7 weeks ago Â  Â  Â  Â  94. 4MBk8s. gcr. io/kube-apiserver Â  Â  Â  Â  Â  Â  Â  Â  v1. 17. 0       0cae8d5cc64c    7 weeks ago Â  Â  Â  Â  171MBkubernetesui/dashboardÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  v2. 0. 0-beta8    eb51a3597525    7 weeks ago Â  Â  Â  Â  90. 8MBk8s. gcr. io/corednsÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  1. 6. 5        70f311871ae1    2 months agoÂ  Â  Â  Â  41. 6MBk8s. gcr. io/etcd Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  3. 4. 3-0       303ce5db0e90    3 months agoÂ  Â  Â  Â  288MBkubernetesui/metrics-scraperÂ  Â  Â  Â  Â  Â  Â  v1. 0. 2       3b08661dc379    3 months agoÂ  Â  Â  Â  40. 1MBk8s. gcr. io/kube-addon-manager Â  Â  Â  Â  Â  Â  v9. 0. 2       bd12a212f9dc    6 months agoÂ  Â  Â  Â  83. 1MBk8s. gcr. io/pauseÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  3. 1         da86e6ba6ca1    2 years ago Â  Â  Â  Â  742kBgcr. io/k8s-minikube/storage-provisioner Â  v1. 8. 1       4689081edb10    2 years ago Â  Â  Â  Â  80. 8MBOnce the container images are stored, minikube can be stopped. In that moment, the virtual networks shown in the previous picture are attached to the minikube VM as it can be verified by the following command: [alosadag@smc-master metal3-dev-env]$ sudo virsh domiflist minikubeInterfaceÂ  Type    Source Â  Â  Model    MAC--------------------------------------------------------Â  Â  Â  Â  Â  network  defaultÂ  Â  virtio   d4:38:25:25:c6:ca-Â  Â  Â  Â  Â  network  minikube-net virtioÂ  Â  Â  a4:c2:8a:9d:2a:d8-Â  Â  Â  Â  Â  network  provisioning virtioÂ  Â  Â  52:54:00:c8:50:97-Â  Â  Â  Â  Â  network  baremetalÂ  virtio   52:54:00:17:b4:ecInformation At this point the host is ready to create the virtual infrastucture. In the video below it is exhibited all the configuration explained and executed during this first step. Step 2: Configure the host: In this step, the script 02_configure_host. sh basically configures the libvirt/KVM virtual infrastructure and starts services in the host that will be consumed by the virtual bare metal machines:  Web server to expose the ironic-python-agent (IPA) initramfs, kernel, headers and operating system cloud images.  Virtual BMC to emulate a real baseboard management controller (BMC).  Container registry where the virtual servers will pull the images needed to run a K8s installation. Information A baseboard management controller (BMC) is a specialized service processor that monitors the physical state of a computer, network server or other hardware device using sensors and communicating with the system administrator through an independent connection. The BMC is part of the Intelligent Platform Management Interface (IPMI) and is usually contained in the motherboard or main circuit board of the device to be monitored. First, an ssh-key in charge of communicating to libvirt is created if it does not exist previously. This key is called id_rsa_virt_power. It is added to the root authorized_keys and is used by the vbmc and sushy tools to contact libvirt. Information sushy-tools is a set of simple simulation tools aiming at supporting the development and testing of the Redfish protocol implementations. Next, another Ansible playbook called setup-playbook. yml is run against the host. It is focused on set up the virtual infrastructure around metal3-dev-env. Below it is shown the Ansible variables that are passed to the playbook, which actually are obtaining the values from the global variables defined in the common. sh or the configuration file. ANSIBLE_FORCE_COLOR=true ansible-playbook \  -e  working_dir=$WORKING_DIR  \  -e  num_nodes=$NUM_NODES  \  -e  extradisks=$VM_EXTRADISKS  \  -e  virthost=$HOSTNAME  \  -e  platform=$NODES_PLATFORM  \  -e  libvirt_firmware=$LIBVIRT_FIRMWARE  \  -e  default_memory=$DEFAULT_HOSTS_MEMORY  \  -e  manage_baremetal=$MANAGE_BR_BRIDGE  \  -e  provisioning_url_host=$PROVISIONING_URL_HOST  \  -i vm-setup/inventory. ini \  -b -vvv vm-setup/setup-playbook. yml      Name of the variable   Default value         WORKING_DIR   /opt/metal3-dev-env       NUM_NODES   2       VM_EXTRADISKS   false       HOSTNAME   hostname       NODES_PLATFORM   libvirt       LIBVIRT_FIRMWARE   bios       DEFAULT_HOSTS_MEMORY   8192       MANAGE_BR_BRIDGE   y       PROVISIONING_URL_HOST   172. 22. 0. 1   Information There are variables that are only defined as Ansible variables, e. g. number of CPUs of the virtual bare metal server, size of disks, etc. In case you would like to change properties not defined globally in the metal3-dev-env take a look a the default variables specified in role: common and libvirt Thesetup-playbook. yml is composed by 3 roles, which are detailed below:    Common.   This role sets up the virtual hardware and network configuration of the VMs. Actually it is a dependency of the libvirt and virtbmc Ansible roles. This means that the common role must always be executed before the roles that depend on them. Also, they are only executed once. If two roles state the same one as their dependency, it is only executed the first time.      Libvirt.   It actually is the role that configures the virtual bare metal servers. They are all identically defined with the same hardware and network configuration. Note that they are not started since they will be booted later by ironic during the provisioning process. Note It is possible to change the number of VMs to provision by replacing the value of NUMBER_NODES  Finally, once the VMs are defined and we have their MAC address, the ironic inventory file ironic_nodes_json is created. The action of creating a node is part of the enrollment process and the first step to prepare a node to reach the â€œavailableâ€ status. {  nodes : [  {    name :  node-0 ,    driver :  ipmi ,    resource_class :  baremetal ,    driver_info : {     username :  admin ,     password :  password ,     port :  6230 ,     address :  ipmi://192. 168. 111. 1:6230 ,     deploy_kernel :  http://172. 22. 0. 1/images/ironic-python-agent. kernel ,     deploy_ramdisk :  http://172. 22. 0. 1/images/ironic-python-agent. initramfs    },    ports : [    {      address :  00:00:e0:4b:24:8b ,      pxe_enabled : true    }   ],    properties : {     local_gb :  50 ,     cpu_arch :  x86_64    }  },  {    name :  node-1 ,    driver :  ipmi ,    resource_class :  baremetal ,    driver_info : {     username :  admin ,     password :  password ,     port :  6231 ,     address :  ipmi://192. 168. 111. 1:6231 ,     deploy_kernel :  http://172. 22. 0. 1/images/ironic-python-agent. kernel ,     deploy_ramdisk :  http://172. 22. 0. 1/images/ironic-python-agent. initramfs    },    ports : [    {      address :  00:00:e0:4b:24:8f ,      pxe_enabled : true    }   ],    properties : {     local_gb :  50 ,     cpu_arch :  x86_64    }  },Information This role is also used to tear down the virtual infrastructure depending on the variable libvirt_actionÂ inside the Ansible role: setup or teardown.    Virtbmc  This role is only executed if the bare metal virtual machines are created in libvirt, because vbmc needs libvirt to emulate a real BMC. Information VirtualBMC (vmbc) tool simulates a Baseboard Management Controller (BMC) by exposing IPMI responder to the network and talking to libvirt at the host vBMC is running at. Basically, manipulate virtual machines which pretend to be bare metal servers.  The virtbmc Ansible role creates the vbmc and sushy-tools configuration in the host for each virtual bare metal nodes. Note that each virtual bare metal host will have a different vbmc socket exposed in the host. The communication to each vbmc is needed by the BMO to start, stop, configure the boot order, etc during the provisioning stage. Finally, this folders containing the configuration will be mounted by the vbmc and sushy-tools containers.    [alosadag@eko1 metal3-dev-env]$ sudo ls -l --color /opt/metal3-dev-env/virtualbmctotal 0drwxr-x---. 2 root root 21 Feb 5 11:07 sushy-toolsdrwxr-x---. 4 root root 70 Feb 5 11:08 vbmc Next, both host provisioning and baremetal interfaces are configured. The provisioning interface, as the name suggests, will be used to provision the virtual bare metal hosts by means of the `Bare Metal Operator`. This interface is configured with an static IP (172. 22. 0. 1):```sh[alosadag@smc-master metal3-dev-env]$ ifconfig provisioningprovisioning: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;Â  mtu 1500Â Â Â Â Â Â Â Â inet 172. 22. 0. 1Â  netmask 255. 255. 255. 0Â  broadcast 172. 22. 0. 255Â Â Â Â Â Â Â Â inet6 fe80::1091:c1ff:fea1:6a0fÂ  prefixlen 64 scopeid 0x20&lt;link&gt;Â Â Â Â Â Â Â Â ether 12:91:c1:a1:6a:0fÂ  txqueuelen 1000 (Ethernet)On the other hand, the baremetal virtual interface behaves as an external network. This interface is able to reach the internet and it is the network where the different Kubernetes nodes will exchange information. This interface is configured as auto, so the IP is retrieved by DHCP. [alosadag@smc-master metal3-dev-env]$ ifconfig baremetalbaremetal: flags=4099&lt;UP,BROADCAST,MULTICAST&gt;Â  mtu 1500Â Â Â Â Â Â Â Â inet 192. 168. 111. 1Â  netmask 255. 255. 255. 0Â  broadcast 192. 168. 111. 255Â Â Â Â Â Â Â Â ether 52:54:00:db:85:29Â  txqueuelen 1000 (Ethernet)Next, an Ansible role called firewall will be executed targetting the host to be sure that the proper ports are opened. In case your host is running Red Hat Enterprise Linux or CentOS 8, firewalld module will be used. In any other case, iptables module is the choice. Below, the code snippet where firewalld or iptables is assigned: # Use firewalld on CentOS/RHEL, iptables everywhere elseexport USE_FIREWALLD=Falseif [[ ($OS ==  rhel  || $OS =  centos ) &amp;&amp; ${OS_VERSION} == 8 ]]thenÂ Â export USE_FIREWALLD=TruefiNote This behaviour can be changed by replacing the value of the USE_FIREWALLD variable The ports managed by this role are all associated to the services that take part of the provisioning process: ironic, vbmc, httpd, pxe, container registry. . Note Services like ironic, pxe, keepalived, httpd and the container registry are running in the host as containers attached to the host network on the hostâ€™s provisioning interface. On the other hand, the vbmc service is also running as a privileged container and it is listening in the hostâ€™s baremetal interface. Once the network is configured, a local container registry is started. It will be needed in the case of using local built images. In that case, the container images can be modified locally and pushed to the local registry. At that point, the specific image location variable must be changed so it must point out the local registry. This process makes easy to verify and test changes to the code locally. At this point the following containers are running inside two pods on the host: infra-pod and ironic-pod. [root@eko1 metal3-dev-env]# podman pod list --ctr-namesPOD ID Â  Â  Â  Â  NAME     STATUSÂ  Â  CREATED   CONTAINER INFO Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  INFRA ID67cc53713145 Â  infra-pod  Running Â  6 days ago  [vbmc] [sushy-tools] [httpd-infra] [67cc53713145-infra]Â  Â  f1da23fcd77f5a0d475351aa Â  ironic-pod  Running Â  6 days ago  [5a0d475351aa-infra] [ipa-downloader]Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  18f3a8f61407Below are detailed the containers inside the infra-pod pod which are running as privileged using the host network:    The httpd container. &gt; &gt;A folder called shared where the cloud OS image and IPA files are available is mounted and exposed to the virtual bare metal hosts.     sudo podman run -d â€“net host â€“privileged â€“name httpd-infra â€“pod infra-pod -v /opt/metal3-dev-env/ironic:/shared â€“entrypoint /bin/runhttpd quay. io/metal3-io/ironic&gt; This folder also contains the `inspector. ipxe` file which contains the information needed to be able to run the `ironic-python-agent` kernel and initramfs. Below, httpd-infra container is accessed and it has been verified that host's `/opt/metal3-dev-env/ironic/` (`IRONIC_DATA_DIR`) is mounted inside the *shared* folder of the container:```sh[alosadag@eko1 metal3-dev-env]$ sudo podman exec -it httpd-infra bash[root@infra-pod shared]# cat html/inspector. ipxeÂ #!ipxe:retry_bootecho In inspector. ipxeimgfree# NOTE(dtantsur): keep inspection kernel params in [mdns]params in ironic-inspector-imagekernel --timeout 60000 http://172. 22. 0. 1:80/images/ironic-python-agent. kernel ipa-inspection-callback-url=http://172. 22. 0. 1:5050/v1/continue ipa-inspection-collectors=default,extra-hardware,logs systemd. journald. forward_to_console=yes BOOTIF=${mac} ipa-debug=1 ipa-inspection-dhcp-all-interfaces=1 ipa-collect-lldp=1 initrd=ironic-python-agent. initramfs || goto retry_bootinitrd --timeout 60000 http://172. 22. 0. 1:80/images/ironic-python-agent. initramfs || goto retry_bootboot   The vbmc container.   This container mounts two host folders. One is the /opt/metal3-dev-env/virtualbmc/vbmc where the vbmc configuration for each node is stored. The other folder is the /root/. ssh where root keys are located, specifically id_rsa_virt_power which is used to manage the communication with libvirt.  + sudo podman run -d --net host --privileged --name vbmc --pod infra-pod -v /opt/metal3-dev-env/virtualbmc/vbmc:/root/&gt; . vbmc -v /root/. ssh:/root/ssh quay. io/metal3-io/vbmc    The sushy-tools container.   This container mounts the /opt/metal3-dev-env/virtualbmc/sushy-tools config folder and the/root/. sshlocal folder as well. The functionality is similar as thevbmc`, however this use redfish instead of ipmi to connect to the BMC.  + sudo podman run -d --net host --privileged --name sushy-tools --pod infra-pod -v /opt/metal3-dev-env/virtualbmc/&gt; sushy-tools:/root/sushy -v /root/. ssh:/root/ssh quay. io/metal3-io/sushy-tools Information At this point the virtual infrastructure must be ready to apply the Kubernetes specific configuration. Note that all the VMs specified by NUMBER_NODES and minikube must be shut down and the defined virtual network must be active: [alosadag@smc-master metal3-dev-env]$ sudo virsh list --allÂ IdÂ  Â  Name  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  State----------------------------------------------------Â - Â  Â  minikube Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  shut offÂ - Â  Â  node_0 Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  shut offÂ - Â  Â  node_1 Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  shut offÂ - Â  Â  node_2 Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  shut off[alosadag@smc-master metal3-dev-env]$ sudo virsh net-list --allÂ Name Â  Â  Â  Â  Â  Â  Â  Â  State   Autostart Â  Â  Persistent----------------------------------------------------------Â baremetalÂ  Â  Â  Â  Â  Â  active   yes    Â  Â  yesÂ defaultÂ  Â  Â  Â  Â  Â  Â  active   yes    Â  Â  yesÂ minikube-net Â  Â  Â  Â  active   yes    Â  Â  yesÂ provisioning Â  Â  Â  Â  active   yes    Â  Â  yesIn the video below it is exhibited all the configuration explained and executed during this second step. Step 3: Launch the management cluster (minikube): The third script called 03_launch_mgmt_cluster. sh basically configures minikube to become a MetalÂ³ management cluster. On top of minikube the baremetal-operator, capi-controller-manager, capbm-controller-manager and cabpk-controller-manager are installed in the metal3 namespace. In a more detailed way, the script clones the Bare Metal Operator (BMO) and Cluster API Provider for Managed Bare Metal Hardware operator (CAPBM) git repositories, creates the cloud. yaml file and starts the minikube virtual machine. Once minikube is up and running, the BMO is built and executed in minikubeâ€™s Kubernetes cluster. In case of the Bare Metal Operator the branch by default to clone is master, however this and other variables shown in the following table can be replaced in the config file: + BMOREPO=https://github. com/metal3-io/baremetal-operator. git+ BMOBRANCH=master      Name of the variable   Default value   Options         BMOREPO   https://github. com/metal3-io/baremetal-operator. git   Â        BMOBRANCH   master   Â        CAPBMREPO   https://github. com/metal3-io/cluster-api-provider-baremetal. git   Â        CAPI_VERSION   v1alpha2   v1alpha1 or v1alpha3       FORCE_REPO_UPDATE   false   Â        BMO_RUN_LOCAL   false   Â        CAPBM_RUN_LOCAL   false   Â    Once the BMO variables are configured, it is time for the operator to be deployed using kustomize and kubectl as it can seen from the logs:  Information: Kustomize is a Kubernetes tool that lets you customize raw, template-free YAML files for multiple purposes, leaving the original YAML untouched and usable as is. + kustomize build bmo-dirPrHIrcl+ kubectl apply -f-namespace/metal3 createdcustomresourcedefinition. apiextensions. k8s. io/baremetalhosts. metal3. io createdserviceaccount/metal3-baremetal-operator createdclusterrole. rbac. authorization. k8s. io/metal3-baremetal-operator createdclusterrolebinding. rbac. authorization. k8s. io/metal3-baremetal-operator createdconfigmap/ironic-bmo-configmap-75tkt49k5c createdsecret/mariadb-password-d88m524c46 createddeployment. apps/metal3-baremetal-operator createdOnce the BMO objects are applied, itâ€™s time to transform the virtual bare metal hosts information into a yaml file of kind BareMetalHost Custom Resource (CR). This is done by a golang script passing them the IPMI address, BMC username and password, which are stored as a Kubernetes secret, MAC address and name: + go run /home/alosadag/go/src/github. com/metal3-io/baremetal-operator/cmd/make-bm-worker/main. go -address ipmi://192. 168. 111. 1:6230 -password password -user admin -boot-mac 00:be:bc:fd:17:f3 node-0+ read -r name address user password mac+ go run /home/alosadag/go/src/github. com/metal3-io/baremetal-operator/cmd/make-bm-worker/main. go -address ipmi://192. 168. 111. 1:6231 -password password -user admin -boot-mac 00:be:bc:fd:17:f7 node-1+ read -r name address user password mac+ go run /home/alosadag/go/src/github. com/metal3-io/baremetal-operator/cmd/make-bm-worker/main. go -address ipmi://192. 168. 111. 1:6232 -password password -user admin -boot-mac 00:be:bc:fd:17:fb node-2+ read -r name address user password macBelow is shown the bare metal host definition of node-1. Note that the IPMI address is the IP of the hostâ€™s provisioning interface. Behind the scenes, IPMI is handled by the vbmc container running in the host. ---apiVersion: v1kind: Secretmetadata:Â Â name: node-1-bmc-secrettype: Opaquedata:Â Â username: YWRtaW4=Â Â password: cGFzc3dvcmQ=---apiVersion: metal3. io/v1alpha1kind: BareMetalHostmetadata:Â Â name: node-1spec:Â Â online: trueÂ Â bootMACAddress: 00:00:e0:4b:24:8fÂ Â bmc:Â Â Â Â address: ipmi://192. 168. 111. 1:6231Â Â Â Â credentialsName: node-1-bmc-secretSee that the MAC address configured in the BareMetalHost spec definition matches node-1 provisioning interface: [root@eko1 metal3-dev-env]# virsh domiflist node_1InterfaceÂ  Type    Source Â  Â  Model    MAC-------------------------------------------------------vnet4Â  Â  Â  bridge   provisioning virtioÂ  Â  Â  00:00:e0:4b:24:8fvnet5Â  Â  Â  bridge   baremetalÂ  virtio   00:00:e0:4b:24:91Finally, the script apply in namespace metal3 each of the BareMetalHost yaml files that match each virtual bare metal host: + kubectl apply -f bmhosts_crs. yaml -n metal3secret/node-0-bmc-secret createdbaremetalhost. metal3. io/node-0 createdsecret/node-1-bmc-secret createdbaremetalhost. metal3. io/node-1 createdsecret/node-2-bmc-secret createdbaremetalhost. metal3. io/node-2 createdLastly, it is the turn of the CAPBM. Similar to BMO, kustomize is used to create the different Kubernetes components and kubectl applied the files into the management cluster. Warning Note that installing CAPBM includes installing the components of the Cluster API and the components of the Cluster API bootstrap provider kubeadm (CABPK) Below the objects are created through the generate. sh script: ++ mktemp -d capbm-XXXXXXXXXX+ kustomize_overlay_path=capbm-eJPOjCPASD+ . /examples/generate. sh -fGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/_out/cluster. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/_out/controlplane. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/_out/metal3crds. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/_out/metal3plane. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/_out/machinedeployment. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/provider-components/provider-components-cluster-api. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/provider-components/provider-components-kubeadm. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/provider-components/provider-components-baremetal. yamlGenerated /home/alosadag/go/src/github. com/metal3-io/cluster-api-provider-baremetal/examples/_out/provider-components. yamlThen, kustomize configures the files accordingly to the values defined and kubectl applies them: + kustomize build capbm-eJPOjCPASD+ kubectl apply -f-namespace/cabpk-system creatednamespace/capbm-system creatednamespace/capi-system createdcustomresourcedefinition. apiextensions. k8s. io/baremetalclusters. infrastructure. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/baremetalmachines. infrastructure. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/baremetalmachinetemplates. infrastructure. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/clusters. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/kubeadmconfigs. bootstrap. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/kubeadmconfigtemplates. bootstrap. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/machinedeployments. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/machines. cluster. x-k8s. io createdcustomresourcedefinition. apiextensions. k8s. io/machinesets. cluster. x-k8s. io createdrole. rbac. authorization. k8s. io/cabpk-leader-election-role createdrole. rbac. authorization. k8s. io/capbm-leader-election-role createdrole. rbac. authorization. k8s. io/capi-leader-election-role createdclusterrole. rbac. authorization. k8s. io/cabpk-manager-role createdclusterrole. rbac. authorization. k8s. io/cabpk-proxy-role createdclusterrole. rbac. authorization. k8s. io/capbm-manager-role createdclusterrole. rbac. authorization. k8s. io/capbm-proxy-role createdclusterrole. rbac. authorization. k8s. io/capi-manager-role createdrolebinding. rbac. authorization. k8s. io/cabpk-leader-election-rolebinding createdrolebinding. rbac. authorization. k8s. io/capbm-leader-election-rolebinding createdrolebinding. rbac. authorization. k8s. io/capi-leader-election-rolebinding createdclusterrolebinding. rbac. authorization. k8s. io/cabpk-manager-rolebinding createdclusterrolebinding. rbac. authorization. k8s. io/cabpk-proxy-rolebinding createdclusterrolebinding. rbac. authorization. k8s. io/capbm-manager-rolebinding createdclusterrolebinding. rbac. authorization. k8s. io/capbm-proxy-rolebinding createdclusterrolebinding. rbac. authorization. k8s. io/capi-manager-rolebinding createdsecret/capbm-webhook-server-secret createdservice/cabpk-controller-manager-metrics-service createdservice/capbm-controller-manager-service createdservice/capbm-controller-metrics-service createddeployment. apps/cabpk-controller-manager createddeployment. apps/capbm-controller-manager createddeployment. apps/capi-controller-manager createdInformation At this point all controllers and operators must be running in the namespace metal3 of the management cluster (minikube). All virtual bare metal hosts configured must be shown as BareMetalHosts resources in the metal3 namespace as well. They should be in ready status and stopped (online is false) In the video below it is exhibited all the configuration explained and executed during this third step. Step 4: Verification: The last script 04_verify. sh is in charge of verifying that the deployment has been successful by checking several things:  Custom resources (CR) and custom resource definition (CRD) were applied and exist in the cluster.  Verify that the virtual bare metal hosts matches the information detailed in theBareMetalHost object.  All containers are in running status.  Verify virtual network configuration and status.  Verify operators and controllers are running. However, this verification can be easily achieved manually. For instance, checking that controllers and operators running in the management cluster (minikube) and all the virtual bare metal hosts are in ready status: [alosadag@eko1 ~]$ kubectl get pods -n metal3 -o wideNAME Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  READY  STATUS  RESTARTS  AGE   IP    Â  Â  Â  Â  NODE    NOMINATED NODE  READINESS GATEScabpk-controller-manager-5c67dd56c4-wfwbhÂ  Â  2/2 Â  Â  Running Â  9     6d23h Â  172. 17. 0. 5    minikube Â  &lt;none&gt;      &lt;none&gt;capbm-controller-manager-7f9b8f96b7-grl4rÂ  Â  2/2 Â  Â  Running Â  12     6d23h Â  172. 17. 0. 4    minikube Â  &lt;none&gt;      &lt;none&gt;capi-controller-manager-798c76675f-dxh2n Â  Â  1/1  Â  Running Â  10     6d23h Â  172. 17. 0. 6    minikube Â  &lt;none&gt;      &lt;none&gt;metal3-baremetal-operator-5b4c59755d-h4zkp Â  6/6 Â  Â  Running Â  8     6d23h Â  192. 168. 39. 101  minikube Â  &lt;none&gt;      &lt;none&gt;Verify that the BareMetalHosts provisioning status is ready and the BMC configuration is correct. Check that all virtual bare metal hosts are shut down (online is false): [alosadag@eko1 ~]$ kubectl get baremetalhosts -n metal3NAME Â  Â  STATUS  PROVISIONING STATUS Â  CONSUMER       BMC    Â  Â  Â  Â  Â  Â  Â  Â  Â  HARDWARE PROFILE  ONLINE  ERRORnode-0 Â  OK    readyÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ipmi://192. 168. 111. 1:6230  unknown      falseÂ Â Â Â node-1 Â  OK    readyÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ipmi://192. 168. 111. 1:6231  unknown      falseÂ Â Â Â Â node-2 Â  OK    readyÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ipmi://192. 168. 111. 1:6232  unknown      falseGet the list of CRDs created in the cluster. Check that, at least, the following ones exist: [alosadag@eko1 ~]$ kubectl get crdsNAMEÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  CREATED ATbaremetalclusters. infrastructure. cluster. x-k8s. io Â  Â  Â  Â  Â  2020-01-22T13:19:42Zbaremetalhosts. metal3. ioÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  2020-01-22T13:19:35Zbaremetalmachines. infrastructure. cluster. x-k8s. io Â  Â  Â  Â  Â  2020-01-22T13:19:42Zbaremetalmachinetemplates. infrastructure. cluster. x-k8s. io Â  2020-01-22T13:19:42Zclusters. cluster. x-k8s. io Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  2020-01-22T13:19:42Zkubeadmconfigs. bootstrap. cluster. x-k8s. io Â  Â  Â  Â  Â  Â  Â  Â  Â  2020-01-22T13:19:42Zkubeadmconfigtemplates. bootstrap. cluster. x-k8s. io Â  Â  Â  Â  Â  2020-01-22T13:19:42Zmachinedeployments. cluster. x-k8s. io Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  2020-01-22T13:19:43Zmachines. cluster. x-k8s. io Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  2020-01-22T13:19:43Zmachinesets. cluster. x-k8s. ioÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  2020-01-22T13:19:43ZInformation KUBECONFIG file is stored in the userâ€™s home directory (~/. kube/config) that executed the scripts. Check the status of all the applications running in minikube or better said, in the management cluster. [alosadag@smc-master logs]$ kubectl get pods -ANAMESPACE Â  Â  NAME     Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  READY  STATUS  RESTARTS  AGEkube-system Â  coredns-6955765f44-fkdzpÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  1/1   Running  1     164mkube-system Â  coredns-6955765f44-fxzvzÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  1/1   Running  1     164mkube-system Â  etcd-minikube Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  1/1   Running  1     164mkube-system Â  kube-addon-manager-minikube Â  Â  Â  Â  Â  Â  Â  Â  1/1   Running  1     164mkube-system Â  kube-apiserver-minikube Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  1/1   Running  1     164mkube-system Â  kube-controller-manager-minikubeÂ  Â  Â  Â  Â  Â  1/1   Running  1     164mkube-system Â  kube-proxy-87g98Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  1/1   Running  1     164mkube-system Â  kube-scheduler-minikube Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  1/1   Running  1     164mkube-system Â  storage-provisioner Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  1/1   Running  2     164mmetal3Â  Â  Â  Â  cabpk-controller-manager-5c67dd56c4-rldk4 Â  2/2   Running  0     156mmetal3Â  Â  Â  Â  capbm-controller-manager-7f9b8f96b7-mdfcw Â  2/2   Running  0     156mmetal3Â  Â  Â  Â  capi-controller-manager-84947c7497-k6twlÂ  Â  1/1   Running  0     156mmetal3Â  Â  Â  Â  metal3-baremetal-operator-78bffc8d-z5hqsÂ  Â  6/6   Running  0     156mIn the video below it is exhibited all the configuration explained and executed during the verification steps. Summary: In this post a deep dive into the metal3-dev-env scripts was shown. It has been deeply detailed the process of creating a MetalÂ³ emulated environment from a set of virtual machines (VMs) to manage as if they were bare metal hosts. After this post, the reader should have acquired a basic understanding of all the pieces involved in the MetalÂ³ project. Also, and more important, how these scripts can be adapted to your specific needs. Remember that this can be achieved in multiple ways: replacing values in the global variables, replacing Ansible default variables or even modifying playbooks or the scripts themselves. Notice that the MetalÂ³ development environment also focuses on developing new features of the BMO or CAPBM and being able to test them locally. References:  Video playlist: A detailed walkthrough the installation of the metal3-dev-env on Youtube Getting started with Metal3. io MetalÂ³ code repositories"
    }, {
    "id": 2,
    "url": "/blog/2020/01/20/metal3_deploy_kubernetes_on_bare_metal.html",
    "title": "MetalÂ³: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019",
    "author" : "Pedro IbÃ¡Ã±ez Requena",
    "tags" : "hybrid, cloud, metal3, baremetal, shiftdev, edge",
    "body": "Conference talk: MetalÂ³: Deploy Kubernetes on Bare Metal - Yolanda Robla, Red Hat: Some of the most influential minds in the developer industry were landing in the gorgeous ancient city of Split, Croatia, to talk in the Shift Dev 2019 - Developer Conference about the most cutting edge technologies, techniques and biggest trends in the developer space. In this video, Yolanda Robla speaks about the deployment of Kubernetes on Bare Metal with the help of MetalÂ³, a new tool that enables the management of bare metal hosts via custom resources managed through the Kubernetes API. Speakers: Yolanda Robla Yolanda Robla is a Principal Software Engineer at Red Hat. In her own words:  In my current position in Red Hat as an NFV Partner Engineer, I investigate new technologies and create proofs of concept for partners to embrace new technologies. Being the current PTL of Akraino, I am involved in designing and implementing systems based on Kubernetes for the Edge use cases, ensuring high scalability and reproducibility using a GitOps approach. References:  Video: MetalÂ³: Deploy Kubernetes on Bare Metal video"
    }, {
    "id": 3,
    "url": "/blog/2019/12/04/Introducing_metal3_kubernetes_native_bare_metal_host_management.html",
    "title": "Introducing MetalÂ³: Kubernetes Native Bare Metal Host Management - Russell Bryant & Doug Hellmann, Red Hat - KubeCon NA, November 2019",
    "author" : "Pedro IbÃ¡Ã±ez Requena",
    "tags" : "hybrid, cloud, metal3, baremetal, kubecon, edge",
    "body": "Conference talk: Introducing MetalÂ³: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat: MetalÂ³ (â€œmetal kubedâ€) is a new open source bare metal host provisioning tool created to enable Kubernetes-native infrastructure management. MetalÂ³ enables the management of bare metal hosts via custom resources managed through the Kubernetes API as well as the monitoring of bare metal host metrics to Prometheus. This presentation will explain the motivations behind creating the project and what has been accomplished so far. This will be followed by an architectural overview and description of the Custom Resource Definitions (CRDs) for describing bare metal hosts, leading to a demonstration of using MetalÂ³ in a Kubernetes cluster. In this video, Russell Bryant and Doug Hellmann speak about the whats and hows of MetalÂ³, a new tool that enables the management of bare metal hosts via custom resources managed through the Kubernetes API. Speakers: Russell Bryant Russell Bryant is a Distinguished Engineer at Red Hat, where he works on infrastructure management to support Kubernetes clusters. Prior to working on the MetalÂ³ project, Russell has worked on other open infrastructure projects. Russell worked in Software Defined Networking with Open vSwitch (OVS) and Open Virtual Network (OVN) and worked on various parts of OpenStack. Russell also worked in open source telephony via the Asterisk project. Doug Hellmann Doug Hellmann is a Senior Principal Software Engineer at Red Hat. He has been a professional developer since the mid 1990s and has worked on a variety of projects in fields such as mapping, medical news publishing, banking, data center automation, and hardware provisioning. He has been contributing to open source projects for most of his career and for the past 7 years he has been focusing on open source cloud computing technologies, including OpenStack and Kubernetes. References:  Presentation: Introducing MetalÂ³ KubeCon NA 2019 PDF Video: Introducing MetalÂ³: Kubernetes Native Bare Metal Host Management videoDemos:  First demo (Inspection)  Second demo (Provisioning)  Third demo (Scale up)  Fourth demo (v1alpha2) "
    }, {
    "id": 4,
    "url": "/blog/2019/11/13/Extend_Your_Data_Center_to_the_Hybrid_Edge-Red_Hat_Summit.html",
    "title": "Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019",
    "author" : "Pedro IbÃ¡Ã±ez Requena",
    "tags" : "hybrid, cloud, metal3, baremetal, summit, edge",
    "body": "Conference talk: Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019, Paul Cormier, Burr Stutter and Garima Sharma: A critical part of being successful in the hybrid cloud is being successful in your data center with your own infrastructure. In this video, Paul Cormier, Burr Sutter and Garima Sharma show how you can bring the Open Hybrid cloud to the edge. Cluster management from multiple cloud providers to on premise. In the demo youâ€™ll see a multi-cluster inventory for the open hybrid cloud at cloud. redhat. com, OpenShift Container Storage providing storage for Virtual Machines and containers (Cloud, Virtualization and bare metal), and everything Kubernetes native. Speakers: Paul Cormier Executive vice president and president, Products and Technologies. Leads Red Hatâ€™s technology and products organizations, including engineering, product management, and product marketing for Red Hatâ€™s technologies. He joined Red Hat in May 2001 as executive vice president, Engineering. Burr Sutter A lifelong developer advocate, community organizer, and technology evangelist, Burr Sutter is a featured speaker at technology events around the globe â€”from Bangalore to Brussels and Berlin to Beijing (and most parts in between)â€” he is currently Director of Developer Experience at Red Hat. A Java Champion since 2005 and former president of the Atlanta Java User Group, Burr founded the DevNexus conference â€”now the second largest Java event in the U. S. â€” with the aim of making access to the worldâ€™s leading developers affordable to the developer community. Garima Sharma Senior Engineering leader at worldâ€™s largest Open Source company. As a seasoned Tech professional, she runs a global team of Solutions Engineers focused on a large portfolio of Cloud Computing products and technology. She has helped shape science and technology for mission critical software, reliability in operations and re-design of architecture all geared towards advancements in medicine, security, cloud technologies and bottom line savings for the client businesses. Whether leading the architecture, development and delivery of customer centric cutting edge systems, or spearheading diversity and inclusion initiatives via keynotes, blogs and conference presentations, Garima champions the idea of STEM. Garima ardently believes in Maya Angelouâ€™s message that diversity makes for a rich tapestry, and we must understand that all the threads of the tapestry are equal in value no matter what their color. Video:  Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019"
    }, {
    "id": 5,
    "url": "/blog/2019/11/07/Kubernetes-native_Infrastructure-Managed_Baremetal_with_Kubernetes_Operators_and_OpenStack_Ironic.html",
    "title": "Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat",
    "author" : "Pedro IbÃ¡Ã±ez Requena",
    "tags" : "kubernetes, metal3, operator, baremetal, openstack, ironic",
    "body": "Conference talk: Open Infrastructure Days UK 2019; Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat: In this session you can hear about a new effort to enable baremetal Kubernetes deployments using native interfaces, and in particular the Kubernetes Operator framework, combined with OpenStack Ironic. This approach aims to seamlessly integrate your infrastructure with your workloads, including baremetal servers, storage and container/VM workloads. All this can be achieved using kubernetes native applications, combined with existing, proven deployment and storage tooling. In this talk we cover the options around Kubernetes deployments today, the specific approach taken by the new Kubernetes-native â€œMetalkubeâ€ project, and the status/roadmap of this new community effort. Speakers: Steve Hardy is Senior Principal Software Engineer at Red Hat, currently involved in kubernetes/OpenShift deployment and architecture. He is also an active member of the OpenStack community, and has been a project team lead of both the Heat (orchestration) and TripleO (deployment) projects. References:  Open Infrastructure Days UK 2019, Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat"
    }, {
    "id": 6,
    "url": "/blog/2019/10/31/OpenStack-Ironic-and-Bare-Metal-Infrastructure_All-Abstractions-Start-Somewhere.html",
    "title": "OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere - Chris Hoge, OpenStack Foundation; Julia Kreger, Red Hat",
    "author" : "Pedro IbÃ¡Ã±ez Requena",
    "tags" : "kubernetes, metal3, operator, baremetal, openstack",
    "body": "Conference talk: OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere: The history of cloud computing has rapidly layered abstractions on abstractions to deliver applications faster, more reliably, and easier. Serverless functions on top of containers on top of virtualization. However, at the bottom of every stack is physical hardware that has an entire lifecycle that needs to be managed. In this video, Chris and Julia show how OpenStack Ironic is a solution to the problem of managing bare-metal infrastructure. Speakers: Chris Hoge is a Senior Strategic Program Manager for the OpenSatck foundation. Heâ€™s been an active contributor to the Interop Working Group (formerly DefCore), and helps run the trademark program for the OpenStack Foundation. He also works on collaborations between the OpenStack and Kubernetes communities. Previously he worked as an OpenStack community manager and developer at Puppet Labs, and operated a research cloud for the College of Arts and Sciences at The University of Oregon. When not cloud computing, he enjoys long-distance running, dancing, and throwing a ball for his Border Collie. Julia Kreger is Principal Software Engineer at Red Hat. She started her career in networking and eventually shifted to systems engineering. The DevOps movement lead her into software development and the operationalization of software due to the need to automate large scale systems deployments. She is experienced in conveying an operational perspective while bridging that with requirements and doesnâ€™t mind getting deep down into code to solve a problem. She is an active core contributor and leader in OpenStack Ironic project, which is a project she feels passionate about due to many misspent hours in data centers deploying hardware. Prior to OpenStack, Julia contributed to the Shared Learning Infrastructure and worked with large scale litigation database systems. References:  Open Infraestructure Summit, Denver, CO, April 29 - May 1, 2019 OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere"
    }, {
    "id": 7,
    "url": "/blog/2019/09/11/Baremetal-operator.html",
    "title": "Baremetal Operator",
    "author" : "Pablo Iranzo GÃ³mez",
    "tags" : "openshift, kubernetes, metal3, operator",
    "body": "Introduction: The baremetal operator, documented at https://github. com/metal3-io/baremetal-operator/blob/master/docs/api. md, itâ€™s the Operator in charge of definitions of physical hosts, containing information about how to reach the Out of Band management controller, URL with the desired image to provision, plus other properties related with hosts being used for provisioning instances. Quoting from the project:  The Bare Metal Operator implements a Kubernetes API for managing bare metal hosts. It maintains an inventory of available hosts as instances of the BareMetalHost Custom Resource Definition. The Bare Metal Operator knows how to:Inspect the hostâ€™s hardware details and report them on the corresponding BareMetalHost. This includes information about CPUs, RAM, disks, NICs, and more. Provision hosts with a desired imageClean a hostâ€™s disk contents before or after provisioning. A bit more in deep approach: The Baremetal Operator (BMO) keeps a mapping of each host and its management interfaces (vendor based like iLO, iDrac, iRMC, etc) and controlled via IPMI. All of this is defined in a CRD, for example: apiVersion: v1kind: Secretmetadata: name: metal3-node01-credentials namespace: metal3type: Opaquedata: username: YWRtaW4= password: YWRtaW4=---apiVersion: metal3. io/v1alpha1kind: BareMetalHostmetadata: name: metal3-node01 namespace: metal3spec: bmc:  address: ipmi://172. 22. 0. 2:6230  credentialsName: metal3-node01-credentials bootMACAddress: 00:c2:fc:3b:e1:01 description:    hardwareProfile:  libvirt  online: falseWith above values (described in API), weâ€™re telling the operator:  MAC: Defines the mac address of the NIC connected to the network that will be used for provision the host bmc: defines the management controller address and the secret used credentialsName: Defines the name of the secret containing username/password for accessing the IPMI serviceOnce the server is â€˜definedâ€™ via the CRD, the underlying service (provided by ironic1 as of this writing) is inspected: [root@metal3-kubernetes ~]# kubectl get baremetalhost -n metal3NAME      STATUS  PROVISIONING STATUS  CONSUMER  BMC           HARDWARE PROFILE  ONLINE  ERRORmetal3-node01  OK    inspecting            ipmi://172. 22. 0. 1:6230           falseOnce the inspection has finished, the status will change to ready and made available for provisioning. When we define a machine, we refer the images that will be used for the actual provisioning in the CRD (image): apiVersion: v1data: userData: DATAkind: Secretmetadata: name: metal3-node01-user-data namespace: metal3type: Opaque---apiVersion:  cluster. k8s. io/v1alpha1 kind: Machinemetadata: name: metal3-node01 namespace: metal3 generateName: baremetal-machine-spec: providerSpec:  value:   apiVersion:  baremetal. cluster. k8s. io/v1alpha1    kind:  BareMetalMachineProviderSpec    image:    url: http://172. 22. 0. 2/images/CentOS-7-x86_64-GenericCloud-1901. qcow2    checksum: http://172. 22. 0. 2/images/CentOS-7-x86_64-GenericCloud-1901. qcow2. md5sum   userData:    name: metal3-node01-user-data    namespace: metal3[root@metal3-kubernetes ~]# kubectl create -f metal3-node01-machine. ymlsecret/metal3-node01-user-data createdmachine. cluster. k8s. io/metal3-node01 createdLetâ€™s examine the annotation created when provisioning (metal3. io/BareMetalHost): [root@metal3-kubernetes ~]# kubectl get machine -n metal3 metal3-node01 -o yamlapiVersion: cluster. k8s. io/v1alpha1kind: Machinemetadata: annotations:  metal3. io/BareMetalHost: metal3/metal3-node01 creationTimestamp:  2019-07-08T15:30:44Z  finalizers: - machine. cluster. k8s. io generateName: baremetal-machine- generation: 2 name: metal3-node01 namespace: metal3 resourceVersion:  6222  selfLink: /apis/cluster. k8s. io/v1alpha1/namespaces/metal3/machines/metal3-node01 uid: 1bfd384a-5467-43b7-98aa-e80e1ace5ce7spec: metadata:  creationTimestamp: null providerSpec:  value:   apiVersion: baremetal. cluster. k8s. io/v1alpha1   image:    checksum: http://172. 22. 0. 1/images/CentOS-7-x86_64-GenericCloud-1901. qcow2. md5sum    url: http://172. 22. 0. 1/images/CentOS-7-x86_64-GenericCloud-1901. qcow2   kind: BareMetalMachineProviderSpec   userData:    name: metal3-node01-user-data    namespace: metal3 versions:  kubelet:   status: addresses: - address: 192. 168. 122. 79  type: InternalIP - address: 172. 22. 0. 39  type: InternalIP - address: localhost. localdomain  type: Hostname lastUpdated:  2019-07-08T15:30:44Z In the output above, the host assigned was the one weâ€™ve defined earlier as well as the other parameters like IPâ€™s, etc generated. Now, if we check baremetal hosts, we can see how itâ€™s getting provisioned: [root@metal3-kubernetes ~]# kubectl get baremetalhost -n metal3NAME      STATUS  PROVISIONING STATUS  CONSUMER  BMC           HARDWARE PROFILE  ONLINE  ERRORmetal3-node01  OK    provisioned            ipmi://172. 22. 0. 1:6230           trueAnd also, check it via the ironic command: [root@metal3-kubernetes ~]# export OS_TOKEN=fake-token ; export OS_URL=http://localhost:6385 ; openstack baremetal node list+--------------------------------------+---------------+--------------------------------------+-------------+--------------------+-------------+| UUID                 | Name     | Instance UUID            | Power State | Provisioning State | Maintenance |+--------------------------------------+---------------+--------------------------------------+-------------+--------------------+-------------+| 7551cfb4-d758-4ad8-9188-859ee53cf298 | metal3-node01 | 7551cfb4-d758-4ad8-9188-859ee53cf298 | power on  | active       | False    |+--------------------------------------+---------------+--------------------------------------+-------------+--------------------+-------------+Wrap-up: Weâ€™ve seen how via a CRD weâ€™ve defined credentials for a baremetal host to make it available to get provisioned and how weâ€™ve also defined a machine that was provisioned on top of that baremetal host.       Ironic was chosen as the initial provider for baremetal provisioning, check Ironic documentation for more details about Ironic usage in MetalÂ³Â &#8617;    "
    }, {
    "id": 8,
    "url": "/blog/2019/06/25/Metal3.html",
    "title": "Metal3",
    "author" : "Eduardo Minguez",
    "tags" : "openshift, kubernetes, metal3",
    "body": "Originally posted at https://www. underkube. com/posts/metal3/ In this blog post, Iâ€™m going to try to explain in my own words a high leveloverview of what Metal3 is, the motivation behind it and some concepts relatedto a â€˜baremetal operatorâ€™. Letâ€™s have some definitions! Custom Resource Definition: The k8s API provides some out-of-the-box objects such as pods, services, etc. There are a few methods of extending the k8s API (such as API extensions)but since a few releases back, the k8s API can be extended easily with custom resources definitions (CRDs). Basically this means you can virtually create any type of object definition in k8s(actually only user with cluster-admin capabilities) with a yaml such as: apiVersion: apiextensions. k8s. io/v1beta1kind: CustomResourceDefinitionmetadata: # name must match the spec fields below, and be in the form: &lt;plural&gt;. &lt;group&gt; name: crontabs. stable. example. comspec: # group name to use for REST API: /apis/&lt;group&gt;/&lt;version&gt; group: stable. example. com # list of versions supported by this CustomResourceDefinition versions:  - name: v1   # Each version can be enabled/disabled by Served flag.    served: true   # One and only one version must be marked as the storage version.    storage: true # either Namespaced or Cluster scope: Namespaced names:  # plural name to be used in the URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt;  plural: crontabs  # singular name to be used as an alias on the CLI and for display  singular: crontab  # kind is normally the CamelCased singular type. Your resource manifests use this.   kind: CronTab  # shortNames allow shorter string to match your resource on the CLI  shortNames:   - ct preserveUnknownFields: false validation:  openAPIV3Schema:   type: object   properties:    spec:     type: object     properties:      cronSpec:       type: string      image:       type: string      replicas:       type: integerAnd after kubectl apply -f you can kubectl get crontabs. There are tons of information with regards to CRDs, like the k8s official documentation. The CRD by himself is not useful â€˜per seâ€™ as nobody will take care of it (thatâ€™s why I said definition). Itrequires a controller to watch for those new objects and react to differentevents affecting the object. Controller: A controller is basically a loop that watches the current status of an objectand if it is different from the desired status, it fix it (reconciliation). This is why k8s is â€˜declarativeâ€™, you specify the object desired status insteadâ€˜how to do itâ€™ (imperative). Again, there are tons of documentation (and examples) around the controller pattern which isbasically the k8s roots, so Iâ€™ll let your google-foo take care of it :) Operator: An Operator (in k8s slang) is an application running in your k8scluster that deploys, manages and maintain (so, operates) a k8s application. This k8s application (the one that the operator manages), can be as simple as a â€˜hello worldâ€™ applicationcontainerized and deployed in your k8s cluster or it can be a much more complexthing, such a database cluster. The â€˜operatorâ€™ is like an â€˜expert sysadminâ€™ containerized that takes care ofyour application. Bear in mind that the â€˜expertâ€™ tag (meaning the automation behind the operator)depends on the operator implementationâ€¦ so there can be basic operators thatonly deploy your application or complex operators that handle day 2 operationssuch as upgrades, fail overs, backup/restore, etc. See the CoreOS operator definition for more information. Cloud Controller Manager: k8s code is smart enough to be able to leveragethe underlying infrastructure where the cluster is running, such as being ableof creating â€˜LoadBalancerâ€™ services, understanding the cluster topology based on the cloud provider AZs where the nodes are running (for scheduling reasons), etc. This task of â€˜talking to the cloud providerâ€™ is performed by the Cloud Controller Manager (CCM) and for moreinformation you can take a look at the official k8s documentation withregards the architecture and the administration (also, if you are brave enough, you can create your own cloud controller manager ) Cluster API: The Cluster API implementation is a WIP â€˜frameworkâ€™ that allows a k8s cluster to manage itself, including the ability of creating new clusters, adding more nodes, etc. in a â€˜k8s wayâ€™ (declarative, controllers, CRDs, etc. ), so there are objects such as Cluster that can be expressed as k8s objects: apiVersion:  cluster. k8s. io/v1alpha1 kind: Clustermetadata: name: myclusterspec: clusterNetwork:  services:   cidrBlocks: [ 10. 96. 0. 0/12 ]  pods:   cidrBlocks: [ 192. 168. 0. 0/16 ]  serviceDomain:  cluster. local  providerSpec: . . . but also:  Machine type objects [MachineSet type objects](https://github. com/kubernetes-sigs/cluster-api/blob/master/pkg/apis/cluster/v1alpha1/machineset_types. go) MachineDeployment type objects etcThere are someprovider implementations in the wild such as the AWS, Azure, GCP, OpenStack,vSphere, etc. ones and the Cluster API project is driven by the SIG Cluster Lifecycle. Please review the official Cluster API repository for more information. Actuator: The actuator is a Cluster API interface that reacts to changes to Machineobjects reconciliating the Machine status. The actuator code is tightly coupled with the provider (thatâ€™s why it is aninterface) such as the AWS one. MachineSet vs Machine: To simplify, letâ€™s say that MachineSets are to Machines what ReplicaSets areto Pods. So you can scale the Machines in your cluster just by changingthe number of replicas of a MachineSet. Cluster API vs Cloud Providers: As we have seen, the Cluster API leverages the provider related to the k8sinfrastructure itself (clusters and nodes) and the CCM and the cloud providerintegration for k8s is to leverage the cloud provider to provide support infrastructure. Letâ€™s say Cluster API is for the k8s administrators and theCCM is for the k8s users :) Machine API: The OpenShift 4 Machine API is a combination of some of the upstream Cluster APIwith custom OpenShift resources and it is designed to work in conjunction withthe Cluster Version Operator. OpenShiftâ€™s Machine API Operator: The machine-api-operator isan operator that manages the Machine API objects in an OpenShift 4 cluster. The operator is capable of creating machines in AWS and libvirt (more providerscoming soon) via the Machine Controller and it is included out of thebox with OCP 4 (and can be deployed in a k8s vanilla as well) Baremetal: A baremetal server (or bare-metal) is just a computer server. The last years terms such as virtualization, containers, serverless, etc. have beenpopular but at the end of the day, all the code running on top of a SaaS, PaaSor IaaS is actually running in a real physical server stored in a datacenterwired to routers, switches and power. That server is a â€˜baremetalâ€™ server. If you are used to cloud providers and instances, you probably donâ€™t know thepains of baremetal managementâ€¦ including things such as connecting to thevirtual console (usually it requires an old java version) to debug issues,configuring pxe for provisioning baremetal hosts (or attach ISOs via the virtual consoleâ€¦ or insert a CD/DVD physically into the CD carry if you areâ€˜luckyâ€™ enoughâ€¦), configuring VLANs for traffic isolation, etc. That kind of operations is not â€˜cloudâ€™ ready and there are tools that providebaremetal management, such as maas or ironic. Ironic: OpenStack bare metal provisioning (or ironic) is an open source project (or even better, a number of open source projects) to manage baremetal hosts. Ironic avoids the administrator to deal with pxe configuration, manual deployments, etc. and provides a defined API and a series of plugins to interact with different baremetal models and vendors. Ironic is used in OpenStack to provide baremetal objects but there are someprojects (such as bifrost) to useIronic â€˜standaloneâ€™ (so, no OpenStack required) Metal3: Metal3 is a project aimed at providing a baremetal operator thatimplements the Cluster API framework required to be able to manage baremetalin a k8s way (easy peasy!). It uses ironic under the hood to avoid reinventing thewheel, but consider it as an implementation detail that may change. The Metal3 baremetal operator watches for BareMetalHost (CRD) objects defined as: apiVersion: metal3. io/v1alpha1kind: BareMetalHostmetadata: name: my-worker-0spec: online: true bootMACAddress: 00:11:22:33:44:55 bmc:  address: ipmi://my-worker-0. ipmi. example. com  credentialsName: my-worker-0-bmc-secretThere are a few more fields in the BareMetalHost object such as the image, hardware profile, etc. The Metal3 project is actually divided into two different components: baremetal-operator: The Metal3 baremetal-operator is the component that manages baremetal hosts. It exposes a new BareMetalHost custom resource in the k8s API that lets you manage hosts in a declarative way. cluster-api-provider-baremetal: The Metal3 cluster-api-provider-baremetal includes the integration with the Cluster API project. This provider currently includes a Machine actuator that acts as a client of the BareMetalHost custom resources. BareMetalHost vs Machine vs Node:  BareMetalHost is a Metal3 object Machine is a Cluster API object Node is where the pods run :)Those three concepts are linked in a 1:1:1 relationship meaning: A BareMetalHost created with Metal3 maps to a Machine object and once theinstallation procedure finishes, a new kubernetes node will be added to thecluster. $ kubectl get nodesNAME                     STATUS  ROLES  AGE  VERSIONmy-node-0. example. com            Ready  master  25h  v1. 14. 0$ kubectl get machines --all-namespacesNAMESPACE        NAME         INSTANCE  STATE  TYPE  REGION  ZONE  AGEopenshift-machine-api  my-node-0                          25h$ kubectl get baremetalhosts --allnamespacesNAMESPACE       NAME   STATUS PROVISIONING STATUS MACHINE BMC HARDWARE PROFILE ONLINE ERRORopenshift-machine-api my-node-0 OK   provisioned my-node-0. example. com ipmi://1. 2. 3. 4 unknown trueThe 1:1 relationship for the BareMetalHost and the Machine is stored in themachineRef field in the BareMetalHost object: $ kubectl get baremetalhost/my-node-0 -n openshift-machine-api -o jsonpath='{. spec. machineRef}'map[name:my-node-0 namespace:openshift-machine-api]In a Machine annotation: $ kubectl get machines my-node-0 -n openshift-machine-api -o jsonpath='{. metadata. annotations}'map[metal3. io/BareMetalHost:openshift-machine-api/my-node-0]The 1:1 relationship for the Machine and the Node currently requires toexecute the link-machine-and-node. sh script to modify the Machineobject . status field: . /link-machine-and-node. sh MACHINE NODEThen, the reference is stored in the . status. nodeRef. name field in theMachine object: $ kubectl get machine my-node-0 -o jsonpath='{. status. nodeRef. name}'my-node-0. example. comRecap: Being able to â€˜just scale a nodeâ€™ in k8s means a lot of underlying concepts and technologies involved behind the scenes :) Resources/links:  https://dzone. com/articles/introducing-the-kubernetes-cluster-api-project-2 https://blogs. vmware. com/cloudnative/2019/03/14/what-and-why-of-cluster-api/ https://github. com/kubernetes-sigs/cluster-api https://github. com/kubernetes-sigs/cluster-api-provider-aws https://itnext. io/deep-dive-to-cluster-api-a0b4e792d57d https://www. linux. com/blog/event/kubecon/2018/4/extending-kubernetes-cluster-api"
    }, {
    "id": 9,
    "url": "/blog/2019/05/13/The_new_stack_Metal3_Uses_OpenStack_Ironic_for_Declarative_Bare_Metal_Kubernetes.html",
    "title": "The new stack MetalÂ³ Uses OpenStackâ€™s Ironic for Declarative Bare Metal Kubernetes",
    "author" : "Pedro IbÃ¡Ã±ez Requena",
    "tags" : "hybrid, cloud, metal3, baremetal, stack, edge, openstack, ironic",
    "body": "The new stack MetalÂ³ Uses OpenStackâ€™s Ironic for Declarative Bare Metal Kubernetes: Mike Melanson talks in this article about the Open Infrastructure Summit in Denver, Colorado. Where bare metal was one of the main leads of the event. During this event, the OpenStack Foundation unveil a new project called MetalÂ³ (pronounced â€œmetal cubedâ€) that uses Ironic â€œas a foundation for declarative management of bare metal infrastructure for Kubernetesâ€. He also comments on how James Penick, Chris Hoge, senior strategic program manager at OpenStack Foundation,and Julia Kreger, OpenStack Ironic Project Team Leader, took to the stage to offer a demonstration of Metal3,the new project that provides â€œbare metal host provisioning integration for Kubernetes. â€ Some words from Kreger in an interview with The New Stack:  â€œI think the bigger trend that weâ€™re starting to see is a recognition that common tooling and substrate helps everyone succeed faster with more efficiency. â€  â€œThis is combined with a shift in the way operators are choosing to solve their problems at scale, specifically in regards to isolation, cost, or performance. â€ For further detail, check out the video of the keynote, which includes a demonstration of Metal3 being used to quickly provision three bare metal servers with Kubernetesor check the full article included below. References:  The new stack: MetalÂ³ Uses OpenStackâ€™s Ironic for Declarative Bare Metal Kubernetes Video of the keynote: OpenStack Ironic and Baremetal Infrastructure. All Abstracions start somewhere"
    }, {
    "id": 10,
    "url": "/blog/2019/04/30/Metal-Kubed-Baremetal-Provisioning-for-Kubernetes.html",
    "title": "MetalÂ³: Baremetal Provisioning for Kubernetes",
    "author" : "Russell Bryant",
    "tags" : "openshift, kubernetes, metal3",
    "body": "Originally posted at https://blog. russellbryant. net/2019/04/30/metal%c2%b3-metal-kubed-bare-metal-provisioning-for-kubernetes/ Project Introduction: There are a number of great open source tools for bare metal host provisioning, including Ironic. MetalÂ³ aims to build on these technologies to provide a Kubernetes native API for managing bare metal hosts via a provisioning stack that is also running on Kubernetes. We believe that Kubernetes Native Infrastructure, or managing your infrastructure just like your applications, is a powerful next step in the evolution of infrastructure management. The MetalÂ³ project is also building integration with the Kubernetes cluster-api project, allowing MetalÂ³ to be used as an infrastructure backend for Machine objects from the Cluster API. Metal3 Repository Overview: There is a MetalÂ³ overview and some more detailed design documents in the metal3-docs repository. The baremetal-operator is the component that manages bare metal hosts. It exposes a new BareMetalHost custom resource in the Kubernetes API that lets you manage hosts in a declarative way. Finally, the cluster-api-provider-baremetal repository includes integration with the cluster-api project. This provider currently includes a Machine actuator that acts as a client of the BareMetalHost custom resources. Demo: The project has been going for a few months now, and thereâ€™s enough now to show some working code. For this demonstration, Iâ€™ve started with a 3 node Kubernetes cluster installed using OpenShift. $ kubectl get nodesNAME    STATUS  ROLES  AGE  VERSIONmaster-0  Ready  master  24h  v1. 13. 4+d4ce02c1dmaster-1  Ready  master  24h  v1. 13. 4+d4ce02c1dmaster-2  Ready  master  24h  v1. 13. 4+d4ce02c1dMachine objects were created to reflect these 3 masters, as well. $ kubectl get machinesNAME       INSTANCE  STATE  TYPE  REGION  ZONE  AGEostest-master-0                       24hostest-master-1                       24hostest-master-2                       24hFor this cluster-api provider, a Machine has a corresponding BareMetalHost object, which corresponds to the piece of hardware we are managing. There is a design document that covers the relationship between Nodes, Machines, and BareMetalHosts. Since these hosts were provisioned earlier, they are in a special externally provisioned state, indicating that we enrolled them in management while they were already running in a desired state. If changes are needed going forward, the baremetal-operator will be able to automate them. $ kubectl get baremetalhostsNAME         STATUS  PROVISIONING STATUS   MACHINE      BMC             HARDWARE PROFILE  ONLINE  ERRORopenshift-master-0  OK    externally provisioned  ostest-master-0  ipmi://192. 168. 111. 1:6230           trueopenshift-master-1  OK    externally provisioned  ostest-master-1  ipmi://192. 168. 111. 1:6231           trueopenshift-master-2  OK    externally provisioned  ostest-master-2  ipmi://192. 168. 111. 1:6232           trueNow suppose weâ€™d like to expand this cluster by adding another bare metal host to serve as a worker node. First we need to create a new BareMetalHost object that adds this new host to the inventory of hosts managed by the baremetal-operator. Hereâ€™s the YAML for the new BareMetalHost: ---apiVersion: v1kind: Secretmetadata: name: openshift-worker-0-bmc-secrettype: Opaquedata: username: YWRtaW4= password: cGFzc3dvcmQ=---apiVersion: metalkube. org/v1alpha1kind: BareMetalHostmetadata: name: openshift-worker-0spec: online: true bmc:  address: ipmi://192. 168. 111. 1:6233  credentialsName: openshift-worker-0-bmc-secret bootMACAddress: 00:ab:4f:d8:9e:faNow to add the BareMetalHost and its IPMI credentials Secret to the cluster: $ kubectl create -f worker_crs. yamlsecret/openshift-worker-0-bmc-secret createdbaremetalhost. metalkube. org/openshift-worker-0 createdThe list of BareMetalHosts now reflects a new host in the inventory that is ready to be provisioned. It will remain in this ready state until it is claimed by a new Machine object. $ kubectl get baremetalhostsNAME         STATUS  PROVISIONING STATUS   MACHINE      BMC             HARDWARE PROFILE  ONLINE  ERRORopenshift-master-0  OK    externally provisioned  ostest-master-0  ipmi://192. 168. 111. 1:6230           trueopenshift-master-1  OK    externally provisioned  ostest-master-1  ipmi://192. 168. 111. 1:6231           trueopenshift-master-2  OK    externally provisioned  ostest-master-2  ipmi://192. 168. 111. 1:6232           trueopenshift-worker-0  OK    ready                   ipmi://192. 168. 111. 1:6233  unknown      trueWe have a MachineSet already created for workers, but it scaled down to 0. $ kubectl get machinesetsNAME       DESIRED  CURRENT  READY  AVAILABLE  AGEostest-worker-0  0     0               24hWe can scale this MachineSet to 1 to indicate that weâ€™d like a worker provisioned. The baremetal cluster-api provider will then look for an available BareMetalHost, claim it, and trigger provisioning of that host. $ kubectl scale machineset ostest-worker-0 --replicas=1 After the new Machine was created, our cluster-api provider claimed the available host and triggered it to be provisioned. $ kubectl get baremetalhostsNAME         STATUS  PROVISIONING STATUS   MACHINE         BMC             HARDWARE PROFILE  ONLINE  ERRORopenshift-master-0  OK    externally provisioned  ostest-master-0     ipmi://192. 168. 111. 1:6230           trueopenshift-master-1  OK    externally provisioned  ostest-master-1     ipmi://192. 168. 111. 1:6231           trueopenshift-master-2  OK    externally provisioned  ostest-master-2     ipmi://192. 168. 111. 1:6232           trueopenshift-worker-0  OK    provisioning       ostest-worker-0-jmhtc  ipmi://192. 168. 111. 1:6233  unknown      trueThis process takes some time. Under the hood, the baremetal-operator is driving Ironic through a provisioning process. This begins with wiping disks to ensure the host comes up in a clean state. It will eventually write the desired OS image to disk and then reboot into that OS. When complete, a new Kubernetes Node will register with the cluster. $ kubectl get baremetalhostsNAME         STATUS  PROVISIONING STATUS   MACHINE         BMC             HARDWARE PROFILE  ONLINE  ERRORopenshift-master-0  OK    externally provisioned  ostest-master-0     ipmi://192. 168. 111. 1:6230           trueopenshift-master-1  OK    externally provisioned  ostest-master-1     ipmi://192. 168. 111. 1:6231           trueopenshift-master-2  OK    externally provisioned  ostest-master-2     ipmi://192. 168. 111. 1:6232           trueopenshift-worker-0  OK    provisioned       ostest-worker-0-jmhtc  ipmi://192. 168. 111. 1:6233  unknown      true$ kubectl get nodesNAME    STATUS  ROLES  AGE  VERSIONmaster-0  Ready  master  24h  v1. 13. 4+d4ce02c1dmaster-1  Ready  master  24h  v1. 13. 4+d4ce02c1dmaster-2  Ready  master  24h  v1. 13. 4+d4ce02c1dworker-0  Ready  worker  68s  v1. 13. 4+d4ce02c1dThe following screen cast demonstrates this process, as well: Removing a bare metal host from the cluster is very similar. We just have to scale this MachineSet back down to 0. $ kubectl scale machineset ostest-worker-0 --replicas=0 Once the Machine has been deleted, the baremetal-operator will deprovision the bare metal host. $ kubectl get baremetalhostsNAME         STATUS  PROVISIONING STATUS   MACHINE      BMC             HARDWARE PROFILE  ONLINE  ERRORopenshift-master-0  OK    externally provisioned  ostest-master-0  ipmi://192. 168. 111. 1:6230           trueopenshift-master-1  OK    externally provisioned  ostest-master-1  ipmi://192. 168. 111. 1:6231           trueopenshift-master-2  OK    externally provisioned  ostest-master-2  ipmi://192. 168. 111. 1:6232           trueopenshift-worker-0  OK    deprovisioning               ipmi://192. 168. 111. 1:6233  unknown      falseOnce the deprovisioning process is complete, the bare metal host will be back to its ready state, available in the host inventory to be claimed by a future Machine object. $ kubectl get baremetalhostsNAME         STATUS  PROVISIONING STATUS   MACHINE      BMC             HARDWARE PROFILE  ONLINE  ERRORopenshift-master-0  OK    externally provisioned  ostest-master-0  ipmi://192. 168. 111. 1:6230           trueopenshift-master-1  OK    externally provisioned  ostest-master-1  ipmi://192. 168. 111. 1:6231           trueopenshift-master-2  OK    externally provisioned  ostest-master-2  ipmi://192. 168. 111. 1:6232           trueopenshift-worker-0  OK    ready                   ipmi://192. 168. 111. 1:6233  unknown      falseGetting Involved: All development is happening on github. We have a metal3-dev mailing list and use #cluster-api-baremetal on Kubernetes Slack to chat. Occasional project updates are posted to @metal3_io on Twitter. "
    }, {
    "id": 11,
    "url": "/blog/2019/04/12/Raise_some_horns_Red_Hat_s_MetalKube_aims_to_make_Kubernetes_on_bare_machines_simple.html",
    "title": "Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple",
    "author" : "Pedro IbÃ¡Ã±ez Requena",
    "tags" : "hybrid, cloud, metal3, baremetal, stack, edge, openstack, ironic",
    "body": "The Register; Raise some horns: Red Hatâ€™s MetalÂ³ aims to make Kubernetes on bare machines simple: Max Smolaks talks in this article about the OpenInfra Days in the UK, 2019: where MetalÂ³ was revealed earlier last week by Steve Hardy, Red Hatâ€™s senior principal software engineer. The Open Infrastructure Days in the UK is an event organised by the local Open Infrastructure community and supported by the OpenStack Foundation. The Open-source software developers at Red Hat are working on a tool that would simplify deployment and management of Kubernetes clusters on bare-metal servers. Steve told The reg:  â€œIn some situations, you wonâ€™t want to run a full OpenStack infrastructure-as-a-service layer to provide, potentially, for multiple Kubernetes clustersâ€. Hardy is a notable contributor to OpenStack, having previously worked on Heat and TripleO projects. He said one of the reasons for choosing Ironic was its active development â€“ and when new features get added to Ironic, the MetalÂ³ team gets them â€œfor freeâ€.  â€œOpenStack has always been a modular set of projects, and people have always had the opportunity to reuse components for different applications. This is just an example of where we are leveraging one particular component for infrastructure management, just as an alternative to using a full infrastructure API,â€ Hardy said. Thierry Carrez, veep of engineering at the OpenStack Foundation also told The Reg:  â€œI like the fact that the projects end up being reusable on their own, for the functions they bring to the table â€“ this helps us integrate with adjacent communitiesâ€. Hardy also commented:  Itâ€™s still early days for MetalÂ³ - the project has just six contributors, and thereâ€™s no telling when it might reach release. â€œItâ€™s a very, very young project but we are keen to get more community participation and feedback,â€. For further detail, check out the full article at The Register: Raise some horns: Red Hatâ€™s MetalKube aims to make Kubernetes on bare machines simple. References:  Steve Hardy: Red Hatâ€™s senior principal software engineer.  Thierry Carrez: veep of engineering at the OpenStack Foundation.  The Register: Raise some horns: Red Hatâ€™s MetalKube aims to make Kubernetes on bare machines simple"
    }, , {
    "id": 12,
    "url": "/blog/categories.html",
    "title": "Categories",
    "author" : "",
    "tags" : "",
    "body": " -   Blog  Read about the newest updates in the community. &lt;/section&gt;             Categories:        hybrid:               MetalÂ³: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019 January 20, 2020                 Introducing MetalÂ³: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat - KubeCon NA, November 2019 December 04, 2019                 Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019 November 13, 2019                 The new stack MetalÂ³ Uses OpenStackâ€™s Ironic for Declarative Bare Metal Kubernetes May 13, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             cloud:               MetalÂ³: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019 January 20, 2020                 Introducing MetalÂ³: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat - KubeCon NA, November 2019 December 04, 2019                 Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019 November 13, 2019                 The new stack MetalÂ³ Uses OpenStackâ€™s Ironic for Declarative Bare Metal Kubernetes May 13, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             metal3:               MetalÂ³: Kubernetes Native Bare Metal Cluster Management - MaÃ«l Kimmerlin - Kubernetes and CNCF Finland Meetup February 27, 2020                 A detailed walkthrough of the MetalÂ³ development environment February 18, 2020                 MetalÂ³: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019 January 20, 2020                 Introducing MetalÂ³: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat - KubeCon NA, November 2019 December 04, 2019                 Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019 November 13, 2019                 Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat November 07, 2019                 OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere - Chris Hoge, OpenStack Foundation; Julia Kreger, Red Hat October 31, 2019                 Baremetal Operator September 11, 2019                 Metal3 June 25, 2019                 The new stack MetalÂ³ Uses OpenStackâ€™s Ironic for Declarative Bare Metal Kubernetes May 13, 2019                 MetalÂ³: Baremetal Provisioning for Kubernetes April 30, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             baremetal:               MetalÂ³: Kubernetes Native Bare Metal Cluster Management - MaÃ«l Kimmerlin - Kubernetes and CNCF Finland Meetup February 27, 2020                 A detailed walkthrough of the MetalÂ³ development environment February 18, 2020                 MetalÂ³: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019 January 20, 2020                 Introducing MetalÂ³: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat - KubeCon NA, November 2019 December 04, 2019                 Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019 November 13, 2019                 Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat November 07, 2019                 OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere - Chris Hoge, OpenStack Foundation; Julia Kreger, Red Hat October 31, 2019                 The new stack MetalÂ³ Uses OpenStackâ€™s Ironic for Declarative Bare Metal Kubernetes May 13, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             stack:               The new stack MetalÂ³ Uses OpenStackâ€™s Ironic for Declarative Bare Metal Kubernetes May 13, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             edge:               MetalÂ³: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019 January 20, 2020                 Introducing MetalÂ³: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat - KubeCon NA, November 2019 December 04, 2019                 Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019 November 13, 2019                 The new stack MetalÂ³ Uses OpenStackâ€™s Ironic for Declarative Bare Metal Kubernetes May 13, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             openstack:               Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat November 07, 2019                 OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere - Chris Hoge, OpenStack Foundation; Julia Kreger, Red Hat October 31, 2019                 The new stack MetalÂ³ Uses OpenStackâ€™s Ironic for Declarative Bare Metal Kubernetes May 13, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             ironic:               Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat November 07, 2019                 The new stack MetalÂ³ Uses OpenStackâ€™s Ironic for Declarative Bare Metal Kubernetes May 13, 2019                 Raise some horns, Red Hat's MetalKube aims to make Kubernetes on bare machines simple April 12, 2019             openshift:               Baremetal Operator September 11, 2019                 Metal3 June 25, 2019                 MetalÂ³: Baremetal Provisioning for Kubernetes April 30, 2019             kubernetes:               MetalÂ³: Kubernetes Native Bare Metal Cluster Management - MaÃ«l Kimmerlin - Kubernetes and CNCF Finland Meetup February 27, 2020                 Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat November 07, 2019                 OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere - Chris Hoge, OpenStack Foundation; Julia Kreger, Red Hat October 31, 2019                 Baremetal Operator September 11, 2019                 Metal3 June 25, 2019                 MetalÂ³: Baremetal Provisioning for Kubernetes April 30, 2019             operator:               Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat November 07, 2019                 OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere - Chris Hoge, OpenStack Foundation; Julia Kreger, Red Hat October 31, 2019                 Baremetal Operator September 11, 2019             summit:               Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019 November 13, 2019             kubecon:               Introducing MetalÂ³: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat - KubeCon NA, November 2019 December 04, 2019             shiftdev:               MetalÂ³: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019 January 20, 2020             metal3-dev-env:               A detailed walkthrough of the MetalÂ³ development environment February 18, 2020             documentation:               A detailed walkthrough of the MetalÂ³ development environment February 18, 2020             development:               A detailed walkthrough of the MetalÂ³ development environment February 18, 2020             talk:               MetalÂ³: Kubernetes Native Bare Metal Cluster Management - MaÃ«l Kimmerlin - Kubernetes and CNCF Finland Meetup February 27, 2020             conference:               MetalÂ³: Kubernetes Native Bare Metal Cluster Management - MaÃ«l Kimmerlin - Kubernetes and CNCF Finland Meetup February 27, 2020             meetup:               MetalÂ³: Kubernetes Native Bare Metal Cluster Management - MaÃ«l Kimmerlin - Kubernetes and CNCF Finland Meetup February 27, 2020                     Categories:             hybrid        cloud        metal3        baremetal        stack        edge        openstack        ironic        openshift        kubernetes        operator        summit        kubecon        shiftdev        metal3-dev-env        documentation        development        talk        conference        meetup      "
    }, {
    "id": 13,
    "url": "/community-resources.html",
    "title": "Community Resources",
    "author" : "",
    "tags" : "",
    "body": " -     Community Resources    Join conversations with the other people who are involved in the creation, maintenance, and future of Metal3. io.     OperatorHub. io    Rook. io&lt;/section&gt;                Get Connected:                                                                                                                                                                                Mailing List                               Slack                     Twitter                                                                               GitHub                                                                                                       Blog                  Providers of Testing Resources:                     Nordix. Details here.         Bare Metal cloud resources sponsored by packet. net.         Netlify Open Source Plan.         Travis CI, free for open source projects.           "
    }, {
    "id": 14,
    "url": "/documentation.html",
    "title": "Documentation",
    "author" : "",
    "tags" : "",
    "body": " -     Documentation    The MetalÂ³ project (pronounced: Metal Kubed) exists to provide components that allow you to do bare metal host management for Kubernetes. MetalÂ³ works as a Kubernetes application, meaning it runs on Kubernetes and is managed through Kubernetes interfaces. &lt;/section&gt; const childrens = document. querySelectorAll(â€œ. mk-infograph__itemâ€); // childrens. style. backgroundColor = â€˜blackâ€™; const child = document. querySelector(â€œ. mk-infograph__itemâ€); // child. style. color = â€˜whiteâ€™; Array. prototype. forEach. call(document. querySelectorAll(â€œ. mk-infograph__itemâ€), function(  element ) {  element. onclick = addActive; }); function addActive(element) {  childrens. forEach(function(elem) {   elem. classList. remove(â€œmkâ€“selectedâ€);  });  element = this;  if (element. classList. contains(â€œmkâ€“selectedâ€)) {   element. classList. remove(â€œmkâ€“selectedâ€);  } else {   element. classList. add(â€œmkâ€“selectedâ€);  } }});&lt;/script&gt;           Metal3 Component Overview:                     It is helpful to understand the high level architecture of of the Machine API Integration. Click on each step to learn more about that particular component.                   Machine controller:           Bare metal actuator                          The first component is the Bare Metal Actuator, which is an implementation of the Machine Actuator interface defined by the cluster-api project. This actuator reacts to changes to Machine objects and acts as a client of the BareMetalHost custom resources managed by the Bare Metal                           Bare metal operator:           With CRDs representing bare metal inventory with configuration needed by its bare metal management workers.                             The architecture also includes a new Bare Metal Operator, which includes the following:             A Controller for a new Custom Resource, BareMetalHost. This custom resource represents an inventory of known (configured or automatically discovered) bare metal hosts. When a Machine is created the Bare Metal Actuator will claim one of these hosts to be provisioned as a new Kubernetes node.             In response to BareMetalHost updates, the controller will perform bare metal host provisioning actions as necessary to reach the desired state.             The creation of the BareMetalHost inventory can be done in two ways:                           Manually via creating BareMetalHost objects.               Optionally, automatically created via a bare metal host discovery process.                 For more information about Operators, see the operator-sdk.                                         Bare metal management pods:                           The operator manages a set of tools for controlling the power on the host, monitoring the host status, and provisioning images to the host. These tools run inside the pod with the operator, and do not require any configuration by the user.                                       APIs:    Enroll nodes by creating BareMetalHost resources. This would either bemanually or done by a component doing node discovery and introspection.   See the documentation in thebaremetal-operator repository for details.     Use the machine API to allocate a machine.   See the documentation in thecluster-api-provider-baremetalrepository for details.     The new Machine is associated with an available BareMetalHost, which triggersprovisioning of that host to join the cluster. This association is done bythe Actuator when it sets the MachineRef field on the BareMetalHost.  Design Documents: The design documents for Metal3 are all publicly available. Refer to the metal3-io/metal3-docs github repository for details. Around the Web: Conference Talks:  MetalÂ³: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019 Introducing metal3 kubernetes native bare metal host management - Kubecon NA 2019 Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019 OpenStack Ironic and Bare Metal Infrastructure: All Abstractions Start Somewhere - Chris Hoge, OpenStack Foundation; Julia Kreger, Red Hat Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red HatIn The News:  The New Stack: Metal3 Uses OpenStackâ€™s Ironic for Declarative Bare Metal Kubernetes The Register: Raise some horns: Red Hatâ€™s MetalKube aims to make Kubernetes on bare machines simpleBlog Posts:  MetalÂ³ Blog posts  "
    }, {
    "id": 15,
    "url": "/blog/",
    "title": "Blog",
    "author" : "",
    "tags" : "",
    "body": " -     Blog    Read about the newest updates in the community.     &lt;/section&gt;                   MetalÂ³: Kubernetes Native Bare Metal Cluster Management - MaÃ«l Kimmerlin - Kubernetes and CNCF Finland Meetup:         Thursday, 27/02/2020        By Alberto Losada        Conference talk: MetalÂ³: Kubernetes Native Bare Metal Cluster Management - MaÃ«l Kimmerlin On the 20th of January at the Kubernetes and CNCF Finland Meetup, MaÃ«l Kimmerlin gave a brilliant presentation about the status of the MetalÂ³ project. In this presentation, MaÃ«l starts giving a short introduction of the Cluster API. . .         Read More                    A detailed walkthrough of the MetalÂ³ development environment:         Tuesday, 18/02/2020        By Alberto Losada        Introduction to metal3-dev-env metal3-dev-env is a collection of scripts in a Github repository inside the MetalÂ³ project that aims to allow contributors and other interested users to run a fully functional MetalÂ³ environment for testing and have a first contact with the project. Actually, metal3-dev-env sets up an emulated environment. . .         Read More                    MetalÂ³: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019:         Monday, 20/01/2020        By Pedro IbÃ¡Ã±ez Requena        Conference talk: MetalÂ³: Deploy Kubernetes on Bare Metal - Yolanda Robla, Red Hat Some of the most influential minds in the developer industry were landing in the gorgeous ancient city of Split, Croatia, to talk in the Shift Dev 2019 - Developer Conference about the most cutting edge technologies, techniques. . .         Read More                    Introducing MetalÂ³: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat - KubeCon NA, November 2019:         Wednesday, 4/12/2019        By Pedro IbÃ¡Ã±ez Requena        Conference talk: Introducing MetalÂ³: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat MetalÂ³ (â€œmetal kubedâ€) is a new open source bare metal host provisioning tool created to enable Kubernetes-native infrastructure management. MetalÂ³ enables the management of bare metal hosts via custom resources managed through. . .         Read More                    Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019:         Wednesday, 13/11/2019        By Pedro IbÃ¡Ã±ez Requena        Conference talk: Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019, Paul Cormier, Burr Stutter and Garima Sharma A critical part of being successful in the hybrid cloud is being successful in your data center with your own infrastructure. In this video, Paul Cormier, Burr. . .         Read More                                             1                            2                            3                                                                                              Categories:             hybrid        cloud        metal3        baremetal        stack        edge        openstack        ironic        openshift        kubernetes        operator        summit        kubecon        shiftdev        metal3-dev-env        documentation        development        talk        conference        meetup      "
    }, , {
    "id": 16,
    "url": "/privacy-statement.html",
    "title": "Privacy Statement",
    "author" : "",
    "tags" : "",
    "body": " -   Privacy Statement&lt;/section&gt;           Privacy Statement for the MetalÂ³ Project: As Metal3. io and most of the infrastructure of the MetalÂ³ Project are currently hosted by Red Hat Inc. , this site falls under the Red Hat Privacy Policy. All terms of that privacy policy apply to this site. Should we change our hosting in the future, this Privacy Policy will be updated. How to Contact Us: If you have any questions about any of these practices or MetalÂ³â€™s use of your personal information, please feel free to contact us or file an Issue in our Github repo. MetalÂ³ will work with you to resolve any concerns you may have about this Statement. Changes to this Privacy Statement: MetalÂ³ reserves the right to change this policy from time to time. If we do make changes, the revised Privacy Statement will be posted on this site. A notice will be posted on our blog and/or mailing lists whenever this privacy statement is changed in a material way. This Privacy Statement was last amended on September 25, 2019. "
    }, , , {
    "id": 17,
    "url": "/try-it.html",
    "title": "Try it: Getting started with Metal3.io",
    "author" : "",
    "tags" : "",
    "body": " -  Instructions     Prerequisites   Metal3-dev-env setup   Using a custom image    Working with the Environment     Bare Metal Hosts   Provisioning Cluster and Machines   Deprovisioning Cluster and Machines         Centos target hosts only, image update          Directly Provisioning Bare Metal Hosts   Running Custom Baremetal-Operator   Running Custom Cluster API Provider Baremetal   Accessing Ironic API   Instructions: Prerequisites:  System with CentOS 7 or Ubuntu 18. 04 Bare metal preferred, as we will be creating VMs to emulate bare metal hosts Run as a user with passwordless sudo access Resource requirements for the host machine vary depending on the selectedLinux distribution of the target nodes:      Target distribution   host CPU   host memory (Gb)         Centos   4   32       Ubuntu   4   16   Metal3-dev-env setup: This is a high-level architecture of the metalÂ³-dev-env.  tl;dr - Clone metalÂ³-dev-envand run $ makeThe Makefile runs a series of scripts, described here:    01_prepare_host. sh - Installs all needed packages.     02_configure_host. sh - Create a set of VMs that will be managed as if theywere bare metal hosts. It also downloads some images needed for Ironic.     03_launch_mgmt_cluster. sh - Launch a management cluster using minikube andrun the baremetal-operator on that cluster.     04_verify. sh - Runs a set of tests that verify that the deployment completedsuccessfully  To tear down the environment, run $ make clean info â€œNoteâ€you can also run some tests provisioning and deprovisioning machines by running:  # for CAPI v1alpha1 based deployment$ make test# for CAPI v1alpha2 based deployment$ make test_v1a2# for CAPI v1alpha3 based deployment$ make test_v1a3 All configurations for the environment is stored in config_${user}. sh. Youcan configure the following       Name   Option   Allowed values   Default         EXTERNAL_SUBNET   This is the subnet used on the â€œbaremetalâ€ libvirt network, created as the primary network interface for the virtual bare metalhosts.       192. 168. 111. 0/24       SSH_PUB_KEY   This SSH key will be automatically injected into the provisioned host by the provision_host. sh script.       ~/. ssh/id_rsa. pub       CONTAINER_RUNTIME   Select the Container Runtime   â€œdockerâ€, â€œpodmanâ€   â€œpodmanâ€       BMOREPO   Set the Baremetal Operator repository to clone      https://github. com/metal3-io/baremetal-operator. git       BMOBRANCH   Set the Baremetal Operator branch to checkout   Â    master       CAPBMREPO   Set the Cluster Api baremetal provider repository to clone      https://github. com/metal3-io/cluster-api-provider-baremetal. git       CAPBMBRANCH   Set the Cluster Api baremetal provider branch to checkout   Â    master       FORCE_REPO_UPDATE   Force deletion of the BMO and CAPBM repositories before cloning them again   â€œtrueâ€, â€œfalseâ€   â€œfalseâ€       BMO_RUN_LOCAL   Run a local baremetal operator instead of deploying in Kubernetes   â€œtrueâ€, â€œfalseâ€   â€œfalseâ€       CAPBM_RUN_LOCAL   Run a local CAPI operator instead of deploying in Kubernetes   â€œtrueâ€, â€œfalseâ€   â€œfalseâ€       SKIP_RETRIES   Do not retry on failure during verifications or tests of the environment. This should be false. It could only be set to false for verifications of a dev env deployment that fully completed. Otherwise failures will appear as resources are not ready.    â€œtrueâ€, â€œfalseâ€   â€œfalseâ€       TEST_TIME_INTERVAL   Interval between retries after verification or test failure (seconds)      10       TEST_MAX_TIME   Number of maximum verification or test retries      120       BMC_DRIVER   Set the BMC driver   â€œipmiâ€, â€œredfishâ€   â€œipmiâ€       IMAGE_OS   OS of the image to boot the nodes from, overriden by IMAGE_* if set   â€œCentosâ€, â€œCirrosâ€, â€œFCOSâ€, â€œUbuntuâ€   â€œCentosâ€       IMAGE_NAME   Image for target hosts deployment   Â    â€œCentOS-7-x86_64-GenericCloud-1907. qcow2â€       IMAGE_LOCATION   Location of the image to download      http://cloud. centos. org/centos/7/images       IMAGE_USERNAME   Image username for ssh   Â    â€œcentosâ€       IRONIC_IMAGE   Container image for local ironic services   Â    â€œquay. io/metal3-io/ironicâ€       VBMC_IMAGE   Container image for vbmc container   Â    â€œquay. io/metal3-io/vbmcâ€       SUSHY_TOOLS_IMAGE   Container image for sushy-tools container   Â    â€œquay. io/metal3-io/sushy-toolsâ€       CAPI_VERSION   Version of Cluster API   â€œv1alpha1â€, â€œv1alpha2â€, â€œv1alpha3â€   â€œv1alpha1â€       CLUSTER_APIENDPOINT_IP   APIEndpoint IP for target cluster   â€œx. x. x. x/xâ€   â€œ192. 168. 111. 249â€       CLUSTER_PROVISIONING_INTERFACE   Cluster provisioning Interface   â€œironicendpointâ€   â€œironicendpointâ€       POD_CIDR   POD CIDR   â€œx. x. x. x/xâ€   â€œ192. 168. 0. 0/18â€   Using a custom image: You can override the three following variables: IMAGE_NAME,IMAGE_LOCATION, IMAGE_USERNAME. If a file with name IMAGE_NAME does notexist in the folder /opt/metal3-dev-env/ironic/html, then it will bedownloaded from IMAGE_LOCATION.  warning â€œWarningâ€If you see this error during the installation:  error: failed to connect to the hypervisorerror: Failed to connect socket to '/var/run/libvirt/libvirt-sock': Permission denied  You may need to log out then login again, and run make again. Working with the Environment: Bare Metal Hosts: This environment creates a set of VMs to manage as if they were bare metalhosts. You can see the VMs using virsh. $ sudo virsh list Id  Name              State---------------------------------------------------- 6   minikube            running 9   node_0             running 10  node_1             runningEach of the VMs (aside from the minikube management cluster VM) arerepresented by BareMetalHost objects in our management cluster. The yamlused to create these host objects is in bmhosts_crs. yaml. $ kubectl get baremetalhosts -n metal3NAME   STATUS  PROVISIONING STATUS  CONSUMER  BMC             HARDWARE PROFILE  ONLINE  ERRORnode-0  OK    ready              ipmi://192. 168. 111. 1:6230  unknown      truenode-1  OK    ready              ipmi://192. 168. 111. 1:6231  unknown      trueYou can also look at the details of a host, including the hardware informationgathered by doing pre-deployment introspection. $ kubectl get baremetalhost -n metal3 -o yaml node-0apiVersion: metal3. io/v1alpha1kind: BareMetalHostmetadata: annotations:  kubectl. kubernetes. io/last-applied-configuration: |   { apiVersion : metal3. io/v1alpha1 , kind : BareMetalHost , metadata :{ annotations :{}, name : node-0 , namespace : metal3 }, spec :{ bmc :{ address : ipmi://192. 168. 111. 1:6230 , credentialsName : node-0-bmc-secret }, bootMACAddress : 00:f8:16:dd:3b:9b , online :true}} creationTimestamp:  2020-02-05T09:09:44Z  finalizers: - baremetalhost. metal3. io generation: 1 name: node-0 namespace: metal3 resourceVersion:  16312  selfLink: /apis/metal3. io/v1alpha1/namespaces/metal3/baremetalhosts/node-0 uid: 99f4c905-b850-45e0-bf1b-61b12f91182bspec: bmc:  address: ipmi://192. 168. 111. 1:6230  credentialsName: node-0-bmc-secret bootMACAddress: 00:f8:16:dd:3b:9b online: truestatus: errorMessage:    goodCredentials:  credentials:   name: node-0-bmc-secret   namespace: metal3  credentialsVersion:  1242  hardware:  cpu:   arch: x86_64   clockMegahertz: 2399. 998   count: 4   model: Intel Xeon E3-12xx v2 (Ivy Bridge)  firmware:   bios:    date: 04/01/2014    vendor: SeaBIOS    version: 1. 10. 2-1ubuntu1  hostname: node-0  nics:  - ip: 192. 168. 111. 20   mac: 00:f8:16:dd:3b:9d   model: 0x1af4 0x0001   name: eth1   pxe: false   speedGbps: 0   vlanId: 0  - ip: 172. 22. 0. 47   mac: 00:f8:16:dd:3b:9b   model: 0x1af4 0x0001   name: eth0   pxe: true   speedGbps: 0   vlanId: 0  ramMebibytes: 8192  storage:  - hctl:  0:0:0:0    model: QEMU HARDDISK   name: /dev/sda   rotational: true   serialNumber: drivMetal3-dev-env setupe-scsi0-0-0-0   sizeBytes: 53687091200   vendor: QEMU  systemVendor:   manufacturer: QEMU   productName: Standard PC (Q35 + ICH9, 2009)   serialNumber:    hardwareProfile: unknown lastUpdated:  2020-02-05T10:10:49Z  operationHistory:  deprovision:   end: null   start: null  inspect:   end:  2020-02-05T09:15:08Z    start:  2020-02-05T09:11:33Z   provision:   end: null   start: null  register:   end:  2020-02-05T09:11:33Z    start:  2020-02-05T09:10:32Z  operationalStatus: OK poweredOn: true provisioning:  ID: b605df1d-7674-44ad-9810-20ad3e3c558b  image:   checksum:      url:     state: ready triedCredentials:  credentials:   name: node-0-bmc-secret   namespace: metal3  credentialsVersion:  1242 Provisioning Cluster and Machines: This section describes how to trigger provisioning of a cluster and hosts viaMachine objects as part of the Cluster API integration. This uses Cluster APIv1alpha2 andassumes that metal3-dev-env is deployed with the environment variableCAPI_VERSION set to v1alpha2. The v1alpha2 deployment can be done withUbuntu 18. 04 or Centos 7 target host images. Please make sure to meet resourcerequirements for successfull deployment: $ . /scripts/v1alphaX/provision_cluster. sh$ . /scripts/v1alphaX/provision_controlplane. sh$ . /scripts/v1alphaX/provision_worker. shAt this point, the Machine actuator will respond and try to claim aBareMetalHost for this Machine. You can check the logs of the actuatorhere: $ kubectl logs -n metal3 pod/capbm-controller-manager-7bbc6897c7-bp2pw -c manager09:10:38. 914458    controller-runtime/controller  msg = Starting Controller   controller = baremetalcluster 09:10:38. 926489    controller-runtime/controller  msg = Starting workers   controller = baremetalmachine   worker count =110:54:16. 943712    Host matched hostSelector for BareMetalMachine10:54:16. 943772    2 hosts available while choosing host for bare metal machine10:54:16. 944087    Associating machine with host10:54:17. 516274    Finished creating machine10:54:17. 518718    Provisioning BaremetalHostIf you look at the yaml representation of the Machine, you will see a newannotation that identifies which BareMetalHost was chosen to satisfy thisMachine request. $ kubectl get machine centos -n metal3 -o yaml. . .  annotations:  metal3. io/BareMetalHost: metal3/node-1. . . You can also see in the list of BareMetalHosts that one of the hosts is nowprovisioned and associated with a Machine. $ kubectl get baremetalhosts -n metal3NAME   STATUS  PROVISIONING STATUS  CONSUMER        BMC             HARDWARE PROFILE  ONLINE  ERRORnode-0  OK    provisioning     test1-md-0-m87bq    ipmi://192. 168. 111. 1:6230  unknown      truenode-1  OK    provisioning     test1-controlplane-0  ipmi://192. 168. 111. 1:6231  unknown      trueYou should be able to ssh into your host once provisioning is complete. Seethe libvirt DHCP leases to find the IP address for the host that wasprovisioned. In this case, itâ€™s node-1 in this case. $ sudo virsh net-dhcp-leases baremetalExpiry Time     MAC address    Protocol IP address        Hostname    Client ID or DUID-------------------------------------------------------------------------------------------------------------------2020-02-05 11:52:39 00:f8:16:dd:3b:9d ipv4   192. 168. 111. 20/24     node-0     -2020-02-05 11:59:18 00:f8:16:dd:3b:a1 ipv4   192. 168. 111. 21/24     node-1     -The default username for the CentOS image is centos. $ ssh centos@192. 168. 111. 21Deprovisioning Cluster and Machines: Deprovisioning of the cluster and machines is done just by deleting ClusterMachine objects. $ kubectl delete machine test1-md-0-m87bq -n metal3machine. cluster. x-k8s. io  test1-md-0-m87bq  deleted$ kubectl delete machine test1-controlplane-0 -n metal3machine. cluster. x-k8s. io  test1-controlplane-0  deleted$ kubectl delete cluster test1 -n metal3cluster. cluster. x-k8s. io  test1  deletedAt this point you can see that the BareMetalHost and Cluster are goingthrough a deprovisioning process. $ kubectl get baremetalhosts -n metal3NAME   STATUS  PROVISIONING STATUS  CONSUMER        BMC             HARDWARE PROFILE  ONLINE  ERRORnode-0  OK    deprovisioning    test1-md-0-m87bq    ipmi://192. 168. 111. 1:6230  unknown      falsenode-1  OK    deprovisioning    test1-controlplane-0  ipmi://192. 168. 111. 1:6231  unknown      false$ kubectl get cluster -n metal3NAME  PHASEtest1  deprovisioningCentos target hosts only, image update: If you want to deploy Ubuntu hosts, please skip to the next section. If you want to deploy Centos 7 for the target hosts, the Centos 7 image requiresan update of Cloud-init. An updated image can be downloadedhere. You can replace the existing centos image with the following commands : curl -LO http://artifactory. nordix. org/artifactory/airship/images/centos. qcow2mv centos. qcow2 /opt/metal3-dev-env/ironic/html/images/centos-updated. qcow2md5sum /opt/metal3-dev-env/ironic/html/images/centos-updated. qcow2 | \awk '{print $1}' &gt; \/opt/metal3-dev-env/ironic/html/images/centos-updated. qcow2. md5sumDirectly Provisioning Bare Metal Hosts: Itâ€™s also possible to provision via the BareMetalHost interface directlywithout using the Cluster API integration. There is a helper script available to trigger provisioning of one of thesehosts. To provision a host with CentOS 7, run: $ . /provision_host. sh node-0The BareMetalHost will go through the provisioning process, and willeventually reboot into the operating system we wrote to disk. $ kubectl get baremetalhost node-0 -n metal3NAME    STATUS  PROVISIONING STATUS  MACHINE  BMC             HARDWARE PROFILE  ONLINE  ERRORnode-0   OK    provisioned           ipmi://192. 168. 111. 1:6230  unknown      trueprovision_host. sh will inject your SSH public key into the VM. To find the IPaddress, you can check the DHCP leases on the baremetal libvirt network. $ sudo virsh net-dhcp-leases baremetal Expiry Time     MAC address    Protocol IP address        Hostname    Client ID or DUID------------------------------------------------------------------------------------------------------------------- 2019-05-06 19:03:46 00:1c:cc:c6:29:39 ipv4   192. 168. 111. 20/24     node-0     - 2019-05-06 19:04:18 00:1c:cc:c6:29:3d ipv4   192. 168. 111. 21/24     node-1     -The default user for the CentOS image is centos. ssh centos@192. 168. 111. 20There is another helper script to deprovision a host. $ . /deprovision_host. sh node-0You will then see the host go into a deprovisioning status: $ kubectl get baremetalhost node-0 -n metal3NAME    STATUS  PROVISIONING STATUS  MACHINE  BMC             HARDWARE PROFILE  ONLINE  ERRORnode-0   OK    deprovisioning         ipmi://192. 168. 111. 1:6230  unknown      trueRunning Custom Baremetal-Operator: The baremetal-operator comes up running in the cluster by default, using animage built from the metal3-io/baremetal-operator repository. If youâ€™d like to test changes to thebaremetal-operator, you can follow this process. First, you must scale down the deployment of the baremetal-operator runningin the cluster. kubectl scale deployment metal3-baremetal-operator -n metal3 --replicas=0To be able to run baremetal-operator locally, you need to installoperator-sdk. After that, you can runthe baremetal-operator including any custom changes. cd ~/go/src/github. com/metal3-io/baremetal-operatormake runRunning Custom Cluster API Provider Baremetal: There are two Cluster API related managers running in the cluster. Oneincludes set of generic controllers, and the other includes a custom Machinecontroller for baremetal. If you want to try changes tocluster-api-provider-baremetal, you want to shut down the custom Machinecontroller manager first. $ kubectl scale statefulset cluster-api-provider-baremetal-controller-manager -n metal3 --replicas=0Then you can run the custom Machine controller manager out of your local git tree. cd ~/go/src/github. com/metal3-io/cluster-api-provider-baremetalmake runAccessing Ironic API: Sometimes you may want to look directly at Ironic to debug something. The metal3-dev-env repository contains a clouds. yaml file withconnection settings for Ironic. metal3-dev-env will install the openstack command line tool on theprovisioning host as part of setting up the cluster. The openstack toolwill look for clouds. yaml in the current directory or you can copy it to~/. config/openstack/clouds. yaml. Version 3. 19. 0 or higher is needed tointeract with Ironic using clouds. yaml. Example: [notstack@metal3 metal3-dev-env]$ export OS_CLOUD=metal3[notstack@metal3 metal3-dev-env]$ openstack baremetal node list+--------------------------------------+--------+---------------+-------------+--------------------+-------------+| UUID                 | Name  | Instance UUID | Power State | Provisioning State | Maintenance |+--------------------------------------+--------+---------------+-------------+--------------------+-------------+| 882cf206-d688-43fa-bf4c-3282fcb00b12 | node-0 | None     | None    | enroll       | False    || ac257479-d6c6-47c1-a649-64a88e6ff312 | node-1 | None     | None    | enroll       | False    |+--------------------------------------+--------+---------------+-------------+--------------------+-------------+To view a particular nodeâ€™s details, run the below command. Thelast_error, maintenance_reason, and provisioning_state fields areuseful for troubleshooting to find out why a node did not deploy. [notstack@metal3 metal3-dev-env]$ export OS_CLOUD=metal3[notstack@metal3 metal3-dev-env]$ openstack baremetal node show 882cf206-d688-43fa-bf4c-3282fcb00b12+------------------------+------------------------------------------------------------+| Field         | Value                           |+------------------------+------------------------------------------------------------+| allocation_uuid    | None                            || automated_clean    | None                            || bios_interface     | no-bios                          || boot_interface     | ipxe                            || chassis_uuid      | None                            || clean_step       | {}                             || conductor       | localhost. localdomain                   || conductor_group    |                              || console_enabled    | False                           || console_interface   | no-console                         || created_at       | 2019-10-07T19:37:36+00:00                 || deploy_interface    | direct                           || deploy_step      | {}                             || description      | None                            || driver         | ipmi                            || driver_info      | {u'ipmi_port': u'6230', u'ipmi_username': u'admin', u'deploy_kernel': u'http://172. 22. 0. 2/images/ironic-python-agent. kernel', u'ipmi_address': u'192. 168. 111. 1', u'deploy_ramdisk': u'http://172. 22. 0. 2/images/ironic-python-agent. initramfs', u'ipmi_password': u'******'} || driver_internal_info  | {u'agent_enable_ata_secure_erase': True, u'agent_erase_devices_iterations': 1, u'agent_erase_devices_zeroize': True, u'disk_erasure_concurrency': 1, u'agent_continue_if_ata_erase_failed': False}                                     || extra         | {}                             || fault         | clean failure                       || inspect_interface   | inspector                         || inspection_finished_at | None                            || inspection_started_at | None                            || instance_info     | {}                             || instance_uuid     | None                            || last_error       | None                            || maintenance      | True                            || maintenance_reason   | Timeout reached while cleaning the node. Please check if the ramdisk responsible for the cleaning is running on the node. Failed on step {}.                                                                 || management_interface  | ipmitool                          || name          | master-0                          || network_interface   | noop                            || owner         | None                            || power_interface    | ipmitool                          || power_state      | power on                          || properties       | {u'cpu_arch': u'x86_64', u'root_device': {u'name': u'/dev/sda'}, u'local_gb': u'50'}                                                                                            || protected       | False                           || protected_reason    | None                            || provision_state    | clean wait                         || provision_updated_at  | 2019-10-07T20:09:13+00:00                 || raid_config      | {}                             || raid_interface     | no-raid                          || rescue_interface    | no-rescue                         || reservation      | None                            || resource_class     | baremetal                         || storage_interface   | noop                            || target_power_state   | None                            || target_provision_state | available                         || target_raid_config   | {}                             || traits         | []                             || updated_at       | 2019-10-07T20:09:13+00:00                 || uuid          | 882cf206-d688-43fa-bf4c-3282fcb00b12            || vendor_interface    | ipmitool                          |+-------------------------------------------------------------------------------------+"
    }, , , , {
    "id": 18,
    "url": "/blog/page2/",
    "title": "Blog",
    "author" : "",
    "tags" : "",
    "body": " -     Blog    Read about the newest updates in the community.     &lt;/section&gt;       {% for post in paginator. posts %}            {{ post. title }}:         {{ post. date | date:  %A, %-d/%m/%Y  }}        By {{ post. author }}        {{ post. content | strip_html | truncatewords: 50 }}         Read More        {% endfor %}     {% if paginator. total_pages &gt; 1 %}           {% if paginator. previous_page %}                                                        {% endif %}      {% for page in (1. . paginator. total_pages) %}       {% if page == paginator. page %}        {{ page }}       {% elsif page == 1 %}        {{ page }}       {% else %}        {{ page }}       {% endif %}      {% endfor %}      {% if paginator. next_page %}                                                     {% endif %}          {% endif %}    {% include blog-aside. html %}"
    }, {
    "id": 19,
    "url": "/blog/page3/",
    "title": "Blog",
    "author" : "",
    "tags" : "",
    "body": " -     Blog    Read about the newest updates in the community.     &lt;/section&gt;       {% for post in paginator. posts %}            {{ post. title }}:         {{ post. date | date:  %A, %-d/%m/%Y  }}        By {{ post. author }}        {{ post. content | strip_html | truncatewords: 50 }}         Read More        {% endfor %}     {% if paginator. total_pages &gt; 1 %}           {% if paginator. previous_page %}                                                        {% endif %}      {% for page in (1. . paginator. total_pages) %}       {% if page == paginator. page %}        {{ page }}       {% elsif page == 1 %}        {{ page }}       {% else %}        {{ page }}       {% endif %}      {% endfor %}      {% if paginator. next_page %}                                                     {% endif %}          {% endif %}    {% include blog-aside. html %}"
    }, , ];

var idx = lunr(function () {
    this.ref('id')
    this.field('title', { boost: 2 })
    this.field('body')
    this.field('author')
    this.field('url')
    this.field('tags', { boost: 2 })
    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}

function getQueryVariable(variable) {
  var query = window.location.search.substring(1);
  var vars = query.split('&');

  for (var i = 0; i < vars.length; i++) {
    var pair = vars[i].split('=');

    if (pair[0] === variable) {
      return decodeURIComponent(pair[1].replace(/\+/g, '%20'));
    }
  }
}


var searchTerm = getQueryVariable('query');
if (searchTerm) {
  lunr_search(searchTerm)
}

</script>
<style>
    #lunrsearchresults {padding-top: 0.2rem;}
    .lunrsearchresult {padding-bottom: 1rem;}
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>
</main>
<footer class="mk-main-footer">
  <p>&copy; 2019 metal3.io <a href="/privacy-statement.html">Privacy Statement</a></p>
</footer>
</div><!--wrapper-->
<script>
var toggle = document.querySelector('#toggle');
var menu = document.querySelector('#main_nav');
var menuItems = document.querySelectorAll('#main_nav li a');

toggle.addEventListener('click', function(){
if (menu.classList.contains('is-active')) {
  this.setAttribute('aria-expanded', 'false');
  menu.classList.remove('is-active');
} else {
  menu.classList.add('is-active');
  this.setAttribute('aria-expanded', 'true');
  //menuItems[0].focus();
}
});
</script>
    <script src="/assets/js/copy.js"></script>
    <!-- This comes from DTM/DPAL and must be latest entry in body-->

    <script type="text/javascript">
        if (("undefined" !== typeof _satellite) && ("function" === typeof _satellite.pageBottom)) {
            _satellite.pageBottom();
        }
    </script>
</body>
</html>

