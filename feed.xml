<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="https://metal3.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://metal3.io/" rel="alternate" type="text/html" /><updated>2026-02-27T18:29:37-06:00</updated><id>https://metal3.io/feed.xml</id><title type="html">Metal³ - Metal Kubed</title><subtitle>Metal3.io aims to build on baremetal host provisioning technologies to provide a Kubernetes native API for managing bare metal hosts via a provisioning stack that is also running on Kubernetes.</subtitle><entry><title type="html">Deploying OCI Container Images to Bare Metal with a Custom IPA Hardware Manager</title><link href="https://metal3.io/blog/2026/02/01/Deploying_OCI_Images_with_Custom_IPA_Hardware_Manager.html" rel="alternate" type="text/html" title="Deploying OCI Container Images to Bare Metal with a Custom IPA Hardware Manager" /><published>2026-02-01T00:00:00-06:00</published><updated>2026-02-01T00:00:00-06:00</updated><id>https://metal3.io/blog/2026/02/01/Deploying_OCI_Images_with_Custom_IPA_Hardware_Manager</id><content type="html" xml:base="https://metal3.io/blog/2026/02/01/Deploying_OCI_Images_with_Custom_IPA_Hardware_Manager.html"><![CDATA[<p>What if you could deploy any OCI container image directly to bare metal,
without building traditional disk images? Back in 2021, Dmitry Tantsur
<a href="https://owlet.today/posts/integrating-coreos-installer-with-ironic/">implemented custom deploy steps</a>
for Ironic, enabling alternative deployment methods beyond the standard
image-based approach. This feature powers OpenShift’s bare metal
provisioning with CoreOS, yet it remains surprisingly unknown to the
broader Metal3 community. This post aims to change that by providing an
example implementation of a custom IPA hardware manager that deploys
Debian-based container images with EFI boot, LVM root filesystem, and
optional RAID1 mirroring.</p>

<h2 id="the-problem-with-traditional-image-based-deployments">The Problem with Traditional Image-Based Deployments</h2>

<p>Traditional bare metal provisioning with Metal3 and Ironic typically
requires pre-built disk images. You need to maintain these images,
update them regularly, and ensure they contain all necessary drivers
and configurations. This approach has some drawbacks:</p>

<ol>
  <li><strong>Image building complexity</strong> - Building and maintaining OS disk
images is not as trivial as creating container images</li>
  <li><strong>Software RAID limitations</strong> - Image-based deployments with mdadm
RAID and EFI boot require workarounds</li>
</ol>

<p>What if we could leverage the container ecosystem instead? Container
registries already solve the distribution problem, and OCI images are
versioned, layered, simple to build and widely available. This approach
allows you to:</p>

<ul>
  <li>Use standard container images from any registry</li>
  <li>Avoid maintaining custom disk images</li>
  <li>Easily switch between OS versions by updating <code class="language-plaintext highlighter-rouge">spec.image.url</code></li>
  <li>Get RAID1 redundancy with minimal configuration</li>
</ul>

<h2 id="introducing-the-deb_oci_efi_lvm-hardware-manager">Introducing the deb_oci_efi_lvm Hardware Manager</h2>

<p>The <a href="https://github.com/s3rj1k/ironic-python-agent/blob/custom_deploy/ironic_python_agent/hardware_managers/deb_oci_efi_lvm.py"><code class="language-plaintext highlighter-rouge">DebOCIEFILVMHardwareManager</code></a>
is a custom IPA hardware manager that deploys Debian-based OCI container
images directly to bare metal. It
provides:</p>

<ul>
  <li><strong>EFI boot support</strong> - UEFI boot with GRUB, which unlike systemd-boot,
supports booting from LVM on top of mdadm software RAID</li>
  <li><strong>LVM root filesystem</strong> - Flexible volume management for the root
partition</li>
  <li><strong>Optional RAID1</strong> - Software mirroring across two disks for
redundancy</li>
  <li><strong>Cloud-init integration</strong> - Ironic <a href="https://book.metal3.io/bmo/instance_customization.html#implementation-notes">configdrive</a>
data is written directly to the root filesystem, no separate configdrive
partition</li>
  <li><strong>Multi-architecture</strong> - Supports x86_64 and ARM64 via OCI multi-arch
images</li>
</ul>

<h2 id="how-it-works">How It Works</h2>

<p>The deployment process extracts an OCI image using Google’s <code class="language-plaintext highlighter-rouge">crane</code> tool,
then installs the necessary boot infrastructure on top. The hardware
manager supports three methods for specifying the OCI image (in priority
order):</p>

<ol>
  <li><code class="language-plaintext highlighter-rouge">spec.image.url</code> with <code class="language-plaintext highlighter-rouge">oci://</code> prefix (e.g., <code class="language-plaintext highlighter-rouge">oci://debian:12</code>)</li>
  <li>Configdrive metadata annotation <code class="language-plaintext highlighter-rouge">bmh.metal3.io/oci_image</code></li>
  <li>Default fallback: <code class="language-plaintext highlighter-rouge">ubuntu:24.04</code></li>
</ol>

<p>Root device hints can be specified using either standard BareMetalHost
<code class="language-plaintext highlighter-rouge">rootDeviceHints</code> fields or a simplified format via the
<code class="language-plaintext highlighter-rouge">bmh.metal3.io/root_device_hints</code> annotation (e.g., <code class="language-plaintext highlighter-rouge">serial=ABC123</code> or
<code class="language-plaintext highlighter-rouge">wwn=0x123456</code>). For RAID1 configurations, provide two space-separated
values (e.g., <code class="language-plaintext highlighter-rouge">serial=ABC123 DEF456</code>).</p>

<blockquote>
  <p><strong>Note:</strong> Alternatively, <code class="language-plaintext highlighter-rouge">podman</code> can be used instead of <code class="language-plaintext highlighter-rouge">crane</code> for OCI
image extraction, as it is readily available in CentOS Stream 9 and also
has an export command. This would require code modifications to the
hardware manager.</p>
</blockquote>

<p>The hardware manager performs these steps during deployment:</p>

<ol>
  <li><strong>Resolve OCI image</strong> - Check <code class="language-plaintext highlighter-rouge">image_source</code>, configdrive, or use default</li>
  <li><strong>Resolve target disks</strong> - Parse root device hints (serial or WWN)</li>
  <li><strong>Clean existing data</strong> - Wipe partitions, RAID arrays, and LVM based on
disk wipe mode (<code class="language-plaintext highlighter-rouge">all</code> for RAID1, <code class="language-plaintext highlighter-rouge">target</code> for single disk by default)</li>
  <li><strong>Partition disks</strong> - Create 2GB EFI partition and LVM partition
(with RAID1 if two disks are specified)</li>
  <li><strong>Create filesystems</strong> - FAT32 for EFI, ext4 for root LV</li>
  <li><strong>Extract OCI image</strong> - Use <code class="language-plaintext highlighter-rouge">crane export</code> piped to <code class="language-plaintext highlighter-rouge">tar</code> for rootfs</li>
  <li><strong>Install packages</strong> - Add cloud-init, GRUB, kernel, mdadm, lvm2</li>
  <li><strong>Configure boot</strong> - Set up GRUB, initramfs, and fstab</li>
  <li><strong>Install bootloader</strong> - GRUB to both EFI partitions for RAID1</li>
</ol>

<h3 id="disk-layout">Disk Layout</h3>

<p>The hardware manager creates the following partition layout:</p>

<table>
  <thead>
    <tr>
      <th>Partition</th>
      <th>Size</th>
      <th>Filesystem</th>
      <th>Label</th>
      <th>Mount Point</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1 (EFI)</td>
      <td>2 GB</td>
      <td>FAT32</td>
      <td>EFI</td>
      <td>/boot/efi</td>
    </tr>
    <tr>
      <td>2 (LVM/RAID)</td>
      <td>Remaining</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
    </tr>
  </tbody>
</table>

<p>The LVM configuration:</p>

<table>
  <thead>
    <tr>
      <th>Component</th>
      <th>Name</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Volume Group</td>
      <td>vg_root</td>
      <td>Contains all logical volumes</td>
    </tr>
    <tr>
      <td>Logical Volume</td>
      <td>lv_root</td>
      <td>Root filesystem (100% of VG)</td>
    </tr>
    <tr>
      <td>Filesystem</td>
      <td>ext4</td>
      <td>Label: ROOTFS</td>
    </tr>
  </tbody>
</table>

<p>For RAID1 configurations, both disks get identical partition tables,
with partition 2 forming a RAID1 array that serves as the LVM physical
volume.</p>

<h2 id="configuration">Configuration</h2>

<h3 id="basic-single-disk-deployment">Basic Single-Disk Deployment</h3>

<p>For a simple single-disk deployment, configure your BareMetalHost and
Metal3MachineTemplate as follows:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">metal3.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">BareMetalHost</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-server</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">metal3</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">online</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">bootMode</span><span class="pi">:</span> <span class="s">UEFI</span>
  <span class="c1"># Preferred method: Use spec.image.url with oci:// prefix</span>
  <span class="na">image</span><span class="pi">:</span>
    <span class="na">url</span><span class="pi">:</span> <span class="s2">"</span><span class="s">oci://debian:12"</span>
  <span class="na">rootDeviceHints</span><span class="pi">:</span>
    <span class="na">serialNumber</span><span class="pi">:</span> <span class="s2">"</span><span class="s">DISK_SERIAL_NUMBER"</span>
</code></pre></div></div>

<p>Alternatively, you can use annotations or simplified hint formats:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">metal3.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">BareMetalHost</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-server-alt</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">metal3</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="c1"># Alternative: Override default ubuntu:24.04 via annotation</span>
    <span class="na">bmh.metal3.io/oci_image</span><span class="pi">:</span> <span class="s2">"</span><span class="s">debian:12"</span>
    <span class="c1"># Alternative: Use simplified hint format</span>
    <span class="na">bmh.metal3.io/root_device_hints</span><span class="pi">:</span> <span class="s2">"</span><span class="s">serial=DISK_SERIAL_NUMBER"</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">online</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">bootMode</span><span class="pi">:</span> <span class="s">UEFI</span>
</code></pre></div></div>

<p>The hardware manager supports three methods for specifying the OCI image
(in priority order):</p>

<ol>
  <li><strong>spec.image.url</strong> with <code class="language-plaintext highlighter-rouge">oci://</code> prefix (highest priority, recommended)</li>
  <li><strong>Annotation</strong> <code class="language-plaintext highlighter-rouge">bmh.metal3.io/oci_image</code> passed via Metal3DataTemplate</li>
  <li><strong>Default</strong> <code class="language-plaintext highlighter-rouge">ubuntu:24.04</code> (fallback)</li>
</ol>

<p>Root device hints support both standard format (<code class="language-plaintext highlighter-rouge">serialNumber: "ABC123"</code>)
and simplified format via annotation (<code class="language-plaintext highlighter-rouge">bmh.metal3.io/root_device_hints: "serial=ABC123"</code>).</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Metal3MachineTemplate</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-worker-template</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">metal3</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">customDeploy</span><span class="pi">:</span>
        <span class="na">method</span><span class="pi">:</span> <span class="s2">"</span><span class="s">deb_oci_efi_lvm"</span>
      <span class="na">dataTemplate</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">my-data-template</span>
</code></pre></div></div>

<h3 id="raid1-configuration">RAID1 Configuration</h3>

<p>For production deployments requiring disk redundancy, specify two disk
serial numbers. The hardware manager supports multiple formats:</p>

<h4 id="method-1-standard-format-with-space-separated-values">Method 1: Standard format with space-separated values</h4>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">metal3.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">BareMetalHost</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-ha-server</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">metal3</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">online</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">bootMode</span><span class="pi">:</span> <span class="s">UEFI</span>
  <span class="na">image</span><span class="pi">:</span>
    <span class="na">url</span><span class="pi">:</span> <span class="s2">"</span><span class="s">oci://debian:13"</span>
  <span class="na">rootDeviceHints</span><span class="pi">:</span>
    <span class="c1"># Two space-separated serial numbers enable RAID1</span>
    <span class="na">serialNumber</span><span class="pi">:</span> <span class="s2">"</span><span class="s">DISK1_SERIAL</span><span class="nv"> </span><span class="s">DISK2_SERIAL"</span>
</code></pre></div></div>

<h4 id="method-2-simplified-format-via-annotation">Method 2: Simplified format via annotation</h4>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">metal3.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">BareMetalHost</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-ha-server-alt</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">metal3</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="na">bmh.metal3.io/oci_image</span><span class="pi">:</span> <span class="s2">"</span><span class="s">debian:13"</span>
    <span class="c1"># Simplified RAID1 hint format</span>
    <span class="na">bmh.metal3.io/root_device_hints</span><span class="pi">:</span> <span class="s2">"</span><span class="s">serial=DISK1_SERIAL</span><span class="nv"> </span><span class="s">DISK2_SERIAL"</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">online</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">bootMode</span><span class="pi">:</span> <span class="s">UEFI</span>
</code></pre></div></div>

<p>With RAID1 enabled, the hardware manager will:</p>

<ul>
  <li>Clean both disks (remove existing partitions, RAID arrays, and LVM)</li>
  <li>Create identical partition layouts on both disks</li>
  <li>Set up a RAID1 array (<code class="language-plaintext highlighter-rouge">/dev/md0</code>) for the LVM physical volume</li>
  <li>Install GRUB to both EFI partitions</li>
  <li>Configure a GRUB update hook to sync EFI partitions via rsync</li>
</ul>

<h3 id="disk-wipe-mode-configuration">Disk Wipe Mode Configuration</h3>

<p>By default, the hardware manager wipes all block devices for RAID1
configurations (to prevent stray RAID/LVM metadata issues) and only target
disks for single-disk setups. You can override this behavior:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">metal3.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">BareMetalHost</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-server</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">metal3</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="c1"># Control disk cleaning behavior</span>
    <span class="c1"># "all" - Wipe all block devices (recommended for RAID1)</span>
    <span class="c1"># "target" - Wipe only target disk(s) from root device hints</span>
    <span class="na">bmh.metal3.io/disk_wipe_mode</span><span class="pi">:</span> <span class="s2">"</span><span class="s">all"</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">online</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">bootMode</span><span class="pi">:</span> <span class="s">UEFI</span>
  <span class="na">image</span><span class="pi">:</span>
    <span class="na">url</span><span class="pi">:</span> <span class="s2">"</span><span class="s">oci://ubuntu:24.04"</span>
  <span class="na">rootDeviceHints</span><span class="pi">:</span>
    <span class="na">serialNumber</span><span class="pi">:</span> <span class="s2">"</span><span class="s">DISK_SERIAL_NUMBER"</span>
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">disk_wipe_mode</code> annotation is useful when:</p>

<ul>
  <li>You have multiple disks and want to ensure clean RAID/LVM state (<code class="language-plaintext highlighter-rouge">all</code>)</li>
  <li>You want to preserve data on non-target disks (<code class="language-plaintext highlighter-rouge">target</code>)</li>
  <li>You’re migrating from a previous RAID configuration</li>
</ul>

<h3 id="metal3datatemplate-configuration">Metal3DataTemplate Configuration</h3>

<p>When using annotations (instead of <code class="language-plaintext highlighter-rouge">spec.image.url</code>), configure your
Metal3DataTemplate to pass them to the configdrive:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Metal3DataTemplate</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-data-template</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">metal3</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">clusterName</span><span class="pi">:</span> <span class="s">my-cluster</span>
  <span class="na">metaData</span><span class="pi">:</span>
    <span class="na">fromAnnotations</span><span class="pi">:</span>
    <span class="c1"># Optional: Pass OCI image annotation (only if not using spec.image.url)</span>
    <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">oci_image</span>
      <span class="na">object</span><span class="pi">:</span> <span class="s">baremetalhost</span>
      <span class="na">annotation</span><span class="pi">:</span> <span class="s2">"</span><span class="s">bmh.metal3.io/oci_image"</span>
    <span class="c1"># Optional: Pass simplified root device hint</span>
    <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">root_device_hints</span>
      <span class="na">object</span><span class="pi">:</span> <span class="s">baremetalhost</span>
      <span class="na">annotation</span><span class="pi">:</span> <span class="s2">"</span><span class="s">bmh.metal3.io/root_device_hints"</span>
    <span class="c1"># Optional: Pass disk wipe mode</span>
    <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">disk_wipe_mode</span>
      <span class="na">object</span><span class="pi">:</span> <span class="s">baremetalhost</span>
      <span class="na">annotation</span><span class="pi">:</span> <span class="s2">"</span><span class="s">bmh.metal3.io/disk_wipe_mode"</span>
    <span class="na">objectNames</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">name</span>
      <span class="na">object</span><span class="pi">:</span> <span class="s">machine</span>
    <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">local-hostname</span>
      <span class="na">object</span><span class="pi">:</span> <span class="s">machine</span>
    <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">local_hostname</span>
      <span class="na">object</span><span class="pi">:</span> <span class="s">machine</span>
    <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">metal3-name</span>
      <span class="na">object</span><span class="pi">:</span> <span class="s">baremetalhost</span>
    <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">metal3-namespace</span>
      <span class="na">object</span><span class="pi">:</span> <span class="s">baremetalhost</span>
  <span class="na">networkData</span><span class="pi">:</span>
    <span class="na">links</span><span class="pi">:</span>
      <span class="na">ethernets</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">id</span><span class="pi">:</span> <span class="s">enp1s0</span>
        <span class="na">macAddress</span><span class="pi">:</span>
          <span class="na">fromHostInterface</span><span class="pi">:</span> <span class="s">enp1s0</span>
        <span class="na">type</span><span class="pi">:</span> <span class="s">phy</span>
    <span class="na">networks</span><span class="pi">:</span>
      <span class="na">ipv4</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">id</span><span class="pi">:</span> <span class="s">baremetalv4</span>
        <span class="na">ipAddressFromIPPool</span><span class="pi">:</span> <span class="s">my-ip-pool</span>
        <span class="na">link</span><span class="pi">:</span> <span class="s">enp1s0</span>
        <span class="na">routes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">gateway</span><span class="pi">:</span>
            <span class="na">fromIPPool</span><span class="pi">:</span> <span class="s">my-ip-pool</span>
          <span class="na">network</span><span class="pi">:</span> <span class="s">0.0.0.0</span>
          <span class="na">prefix</span><span class="pi">:</span> <span class="m">0</span>
    <span class="na">services</span><span class="pi">:</span>
      <span class="na">dns</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">8.8.8.8</span>
</code></pre></div></div>

<blockquote>
  <p><strong>Note:</strong> When using <code class="language-plaintext highlighter-rouge">spec.image.url</code> with the <code class="language-plaintext highlighter-rouge">oci://</code> prefix, you don’t
need to pass the <code class="language-plaintext highlighter-rouge">oci_image</code> annotation through Metal3DataTemplate. The
hardware manager reads directly from <code class="language-plaintext highlighter-rouge">instance_info.image_source</code>. This is
the recommended approach for newer deployments.</p>
</blockquote>

<h2 id="building-an-ipa-image-with-the-hardware-manager">Building an IPA Image with the Hardware Manager</h2>

<p>To use this hardware manager, you need to build a custom IPA ramdisk
image using
<a href="https://opendev.org/openstack/ironic-python-agent-builder">ironic-python-agent-builder</a>.
This tool uses <a href="https://docs.openstack.org/diskimage-builder/latest/">diskimage-builder</a>
(DIB) to create bootable ramdisk images containing the IPA and any
custom elements you need.</p>

<h3 id="required-packages">Required Packages</h3>

<p>The hardware manager requires several packages to be present in the
IPA ramdisk:</p>

<table>
  <thead>
    <tr>
      <th>Package</th>
      <th>Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">crane</code></td>
      <td>OCI image extraction from container registries</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">mdadm</code></td>
      <td>Software RAID array management</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">lvm2</code></td>
      <td>Logical Volume Manager for root filesystem</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">parted</code></td>
      <td>Disk partitioning</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">dosfstools</code></td>
      <td>FAT32 filesystem creation for EFI partition</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">grub2-efi-*</code></td>
      <td>UEFI bootloader installation</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">curl</code></td>
      <td>Downloading files during deployment</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">rsync</code></td>
      <td>EFI partition synchronization for RAID</td>
    </tr>
  </tbody>
</table>

<h3 id="custom-dib-elements">Custom DIB Elements</h3>

<p>DIB elements are modular components that customize the image build.
Each element is a directory containing scripts that run at different
phases of the build:</p>

<table>
  <thead>
    <tr>
      <th>Directory</th>
      <th>Phase</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">extra-data.d/</code></td>
      <td>Pre-build</td>
      <td>Copy files into build environment</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">install.d/</code></td>
      <td>Chroot</td>
      <td>Run inside chroot during build</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">post-install.d/</code></td>
      <td>Post-install</td>
      <td>Run after package installation</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">finalise.d/</code></td>
      <td>Finalize</td>
      <td>Run at end of build process</td>
    </tr>
  </tbody>
</table>

<p>Scripts are named with a numeric prefix (e.g., <code class="language-plaintext highlighter-rouge">50-crane</code>) to control
execution order.</p>

<!-- markdownlint-disable MD033 -->

<details>
  <summary>DIB element: crane (OCI image tool)</summary>
  <div>

    <!-- markdownlint-enable MD033 -->

    <p>Create a DIB element to install Google’s <code class="language-plaintext highlighter-rouge">crane</code> tool for OCI image
extraction. Create the following directory structure:</p>

    <div class="language-text highlighter-rouge"><div class="highlight"><pre class="syntax"><code>crane/
├── element-deps
└── install.d/
    └── 50-crane
</code></pre></div>    </div>

    <p>The <code class="language-plaintext highlighter-rouge">element-deps</code> file can be empty or list dependencies. The install
script (<code class="language-plaintext highlighter-rouge">install.d/50-crane</code>):</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c">#!/bin/bash</span>

<span class="c"># https://docs.openstack.org/diskimage-builder/latest/developer/developing_elements.html</span>

<span class="k">if</span> <span class="o">[</span> <span class="s2">"</span><span class="k">${</span><span class="nv">DIB_DEBUG_TRACE</span><span class="k">:-</span><span class="nv">0</span><span class="k">}</span><span class="s2">"</span> <span class="nt">-gt</span> 0 <span class="o">]</span><span class="p">;</span> <span class="k">then
    </span><span class="nb">set</span> <span class="nt">-x</span>
<span class="k">fi

</span><span class="nb">set</span> <span class="nt">-eu</span>
<span class="nb">set</span> <span class="nt">-o</span> pipefail

<span class="nv">CRANE_VERSION</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">DIB_CRANE_VERSION</span><span class="k">:-</span><span class="nv">latest</span><span class="k">}</span><span class="s2">"</span>

<span class="c"># Detect architecture</span>
<span class="nv">ARCH</span><span class="o">=</span><span class="si">$(</span><span class="nb">uname</span> <span class="nt">-m</span><span class="si">)</span>
<span class="k">case</span> <span class="s2">"</span><span class="k">${</span><span class="nv">ARCH</span><span class="k">}</span><span class="s2">"</span> <span class="k">in
    </span>x86_64<span class="p">)</span>
        <span class="nv">CRANE_ARCH</span><span class="o">=</span><span class="s2">"x86_64"</span>
        <span class="p">;;</span>
    aarch64<span class="p">)</span>
        <span class="nv">CRANE_ARCH</span><span class="o">=</span><span class="s2">"arm64"</span>
        <span class="p">;;</span>
    <span class="k">*</span><span class="p">)</span>
        <span class="nb">echo</span> <span class="s2">"Unsupported architecture: </span><span class="k">${</span><span class="nv">ARCH</span><span class="k">}</span><span class="s2">"</span>
        <span class="nb">exit </span>1
        <span class="p">;;</span>
<span class="k">esac</span>

<span class="nb">echo</span> <span class="s2">"Installing crane (</span><span class="k">${</span><span class="nv">CRANE_VERSION</span><span class="k">}</span><span class="s2">) for </span><span class="k">${</span><span class="nv">CRANE_ARCH</span><span class="k">}</span><span class="s2">..."</span>

<span class="c"># Get the download URL</span>
<span class="k">if</span> <span class="o">[</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CRANE_VERSION</span><span class="k">}</span><span class="s2">"</span> <span class="o">=</span> <span class="s2">"latest"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
    </span><span class="nv">DOWNLOAD_URL</span><span class="o">=</span><span class="si">$(</span>curl <span class="nt">-s</span> https://api.github.com/repos/google/go-containerregistry/releases/latest |
        <span class="nb">grep</span> <span class="s2">"browser_download_url.*Linux_</span><span class="k">${</span><span class="nv">CRANE_ARCH</span><span class="k">}</span><span class="s2">.tar.gz"</span> |
        <span class="nb">cut</span> <span class="nt">-d</span> <span class="s1">'"'</span> <span class="nt">-f</span> 4<span class="si">)</span>
<span class="k">else
    </span><span class="nv">DOWNLOAD_URL</span><span class="o">=</span><span class="s2">"https://github.com/google/go-containerregistry/releases/download/</span><span class="k">${</span><span class="nv">CRANE_VERSION</span><span class="k">}</span><span class="s2">/go-containerregistry_Linux_</span><span class="k">${</span><span class="nv">CRANE_ARCH</span><span class="k">}</span><span class="s2">.tar.gz"</span>
<span class="k">fi

if</span> <span class="o">[</span> <span class="nt">-z</span> <span class="s2">"</span><span class="k">${</span><span class="nv">DOWNLOAD_URL</span><span class="k">}</span><span class="s2">"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
    </span><span class="nb">echo</span> <span class="s2">"Failed to determine crane download URL"</span>
    <span class="nb">exit </span>1
<span class="k">fi

</span><span class="nb">echo</span> <span class="s2">"Downloading crane from: </span><span class="k">${</span><span class="nv">DOWNLOAD_URL</span><span class="k">}</span><span class="s2">"</span>

<span class="c"># Download and extract crane</span>
<span class="nv">TEMP_DIR</span><span class="o">=</span><span class="si">$(</span><span class="nb">mktemp</span> <span class="nt">-d</span><span class="si">)</span>
curl <span class="nt">-sL</span> <span class="s2">"</span><span class="k">${</span><span class="nv">DOWNLOAD_URL</span><span class="k">}</span><span class="s2">"</span> | <span class="nb">tar</span> <span class="nt">-xz</span> <span class="nt">-C</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TEMP_DIR</span><span class="k">}</span><span class="s2">"</span>

<span class="c"># Install crane binary</span>
<span class="nb">install</span> <span class="nt">-m</span> 755 <span class="s2">"</span><span class="k">${</span><span class="nv">TEMP_DIR</span><span class="k">}</span><span class="s2">/crane"</span> /usr/local/bin/crane

<span class="c"># Cleanup</span>
<span class="nb">rm</span> <span class="nt">-rf</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TEMP_DIR</span><span class="k">}</span><span class="s2">"</span>

<span class="c"># Verify installation</span>
<span class="k">if </span>crane version<span class="p">;</span> <span class="k">then
    </span><span class="nb">echo</span> <span class="s2">"crane installed successfully"</span>
<span class="k">else
    </span><span class="nb">echo</span> <span class="s2">"crane installation verification failed"</span>
    <span class="nb">exit </span>1
<span class="k">fi</span>
</code></pre></div>    </div>

  </div>
</details>

<!-- markdownlint-disable MD033 -->

<details>
  <summary>DIB element: packages-install (extra packages)</summary>
  <div>

    <!-- markdownlint-enable MD033 -->

    <p>Create a DIB element that installs packages from the <code class="language-plaintext highlighter-rouge">DIB_EXTRA_PACKAGES</code>
environment variable:</p>

    <div class="language-text highlighter-rouge"><div class="highlight"><pre class="syntax"><code>packages-install/
├── element-deps
└── install.d/
    └── 50-packages-install
</code></pre></div>    </div>

    <p>The install script (<code class="language-plaintext highlighter-rouge">install.d/50-packages-install</code>):</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c">#!/bin/bash</span>

<span class="c"># https://docs.openstack.org/diskimage-builder/latest/developer/developing_elements.html</span>

<span class="k">if</span> <span class="o">[</span> <span class="s2">"</span><span class="k">${</span><span class="nv">DIB_DEBUG_TRACE</span><span class="k">:-</span><span class="nv">0</span><span class="k">}</span><span class="s2">"</span> <span class="nt">-gt</span> 0 <span class="o">]</span><span class="p">;</span> <span class="k">then
    </span><span class="nb">set</span> <span class="nt">-x</span>
<span class="k">fi

</span><span class="nb">set</span> <span class="nt">-eu</span>
<span class="nb">set</span> <span class="nt">-o</span> pipefail

<span class="c"># Enable CRB (CodeReady Builder) repository and install EPEL</span>
<span class="nb">echo</span> <span class="s2">"Enabling CRB repository..."</span>
dnf config-manager <span class="nt">--set-enabled</span> crb <span class="o">||</span> <span class="nb">true</span>

<span class="c"># Detect CentOS version and install appropriate EPEL</span>
<span class="k">if</span> <span class="o">[</span> <span class="nt">-f</span> /etc/os-release <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
    <span class="c"># shellcheck disable=SC1091</span>
    <span class="nb">.</span> /etc/os-release
    <span class="k">case</span> <span class="s2">"</span><span class="k">${</span><span class="nv">VERSION_ID</span><span class="p">%%.*</span><span class="k">}</span><span class="s2">"</span> <span class="k">in
        </span>9<span class="p">)</span>
            <span class="nb">echo</span> <span class="s2">"Installing EPEL for CentOS 9..."</span>
            dnf <span class="nb">install</span> <span class="nt">-y</span> https://dl.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm <span class="o">||</span> <span class="nb">true</span>
            <span class="p">;;</span>
        10<span class="p">)</span>
            <span class="nb">echo</span> <span class="s2">"Installing EPEL for CentOS 10..."</span>
            dnf <span class="nb">install</span> <span class="nt">-y</span> https://dl.fedoraproject.org/pub/epel/epel-release-latest-10.noarch.rpm <span class="o">||</span> <span class="nb">true</span>
            <span class="p">;;</span>
        <span class="k">*</span><span class="p">)</span>
            <span class="nb">echo</span> <span class="s2">"Unknown CentOS version: </span><span class="k">${</span><span class="nv">VERSION_ID</span><span class="k">}</span><span class="s2">, skipping EPEL installation"</span>
            <span class="p">;;</span>
    <span class="k">esac</span>
<span class="k">fi

if</span> <span class="o">[</span> <span class="nt">-z</span> <span class="s2">"</span><span class="k">${</span><span class="nv">DIB_EXTRA_PACKAGES</span><span class="k">:-}</span><span class="s2">"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
    </span><span class="nb">echo</span> <span class="s2">"No extra packages specified via DIB_EXTRA_PACKAGES, skipping"</span>
    <span class="nb">exit </span>0
<span class="k">fi

</span><span class="nb">echo</span> <span class="s2">"Updating system packages..."</span>
dnf update <span class="nt">-y</span>

<span class="nb">echo</span> <span class="s2">"Installing extra packages: </span><span class="k">${</span><span class="nv">DIB_EXTRA_PACKAGES</span><span class="k">}</span><span class="s2">"</span>

<span class="c"># shellcheck disable=SC2086</span>
dnf <span class="nb">install</span> <span class="nt">-y</span> <span class="k">${</span><span class="nv">DIB_EXTRA_PACKAGES</span><span class="k">}</span>

<span class="nb">echo</span> <span class="s2">"Cleaning package cache..."</span>
dnf clean all

<span class="nb">echo</span> <span class="s2">"Extra packages installation complete"</span>
</code></pre></div>    </div>

  </div>
</details>

<h3 id="building-the-image">Building the Image</h3>

<p>Set the <code class="language-plaintext highlighter-rouge">ELEMENTS_PATH</code> to include your custom elements directory, then
run the builder:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nb">export </span><span class="nv">ELEMENTS_PATH</span><span class="o">=</span><span class="s2">"/path/to/your/dib-elements"</span>

<span class="nb">export </span><span class="nv">DIB_EXTRA_PACKAGES</span><span class="o">=</span><span class="s2">"jq yq mdadm lvm2 curl parted util-linux </span><span class="se">\</span><span class="s2">
    squashfs-tools xfsprogs dosfstools grub2-efi-x64 grub2-tools rsync"</span>

ironic-python-agent-builder <span class="se">\</span>
    <span class="nt">-o</span> ipa-custom <span class="se">\</span>
    <span class="nt">-e</span> extra-hardware <span class="se">\</span>
    <span class="nt">-e</span> crane <span class="se">\</span>
    <span class="nt">-e</span> packages-install <span class="se">\</span>
    <span class="nt">--release</span> 9-stream centos
</code></pre></div></div>

<p>This produces two files:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">ipa-custom.kernel</code> - The Linux kernel</li>
  <li><code class="language-plaintext highlighter-rouge">ipa-custom.initramfs</code> - The ramdisk containing IPA and tools</li>
</ul>

<p>For ARM64 builds, the grub packages differ:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nb">export </span><span class="nv">DIB_EXTRA_PACKAGES</span><span class="o">=</span><span class="s2">"jq yq mdadm lvm2 curl parted util-linux </span><span class="se">\</span><span class="s2">
    squashfs-tools xfsprogs dosfstools grub2-efi-aa64 grub2-tools rsync"</span>
</code></pre></div></div>

<h2 id="installing-the-hardware-manager">Installing the Hardware Manager</h2>

<p>The hardware manager must be placed in the IPA hardware managers directory
and registered in <code class="language-plaintext highlighter-rouge">setup.cfg</code>.</p>

<p><strong>File location:</strong></p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="syntax"><code>ironic_python_agent/hardware_managers/deb_oci_efi_lvm.py
</code></pre></div></div>

<p><strong>setup.cfg entry point:</strong></p>

<p>Add the following entry to the <code class="language-plaintext highlighter-rouge">ironic_python_agent.hardware_managers</code>
section in <code class="language-plaintext highlighter-rouge">setup.cfg</code>:</p>

<div class="language-ini highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nn">[entry_points]</span>
<span class="py">ironic_python_agent.hardware_managers</span> <span class="p">=</span>
    <span class="py">deb_oci_efi_lvm</span> <span class="p">=</span> <span class="s">ironic_python_agent.hardware_managers.deb_oci_efi_lvm:DebOCIEFILVMHardwareManager</span>
</code></pre></div></div>

<p>This registers the hardware manager as a plugin, allowing IPA to
discover and load it at runtime.</p>

<h3 id="source-code">Source Code</h3>

<p>The implementation is shown below in expandable sections. Full source:
<a href="https://github.com/s3rj1k/ironic-python-agent/blob/custom_deploy/ironic_python_agent/hardware_managers/deb_oci_efi_lvm.py">deb_oci_efi_lvm.py</a>.</p>

<blockquote>
  <p><strong>Note:</strong> The code below uses a custom <code class="language-plaintext highlighter-rouge">run_command</code> helper function
instead of IPA’s built-in
<a href="https://opendev.org/openstack/ironic-python-agent/src/branch/master/ironic_python_agent/utils.py"><code class="language-plaintext highlighter-rouge">ironic_python_agent.utils.execute</code></a>.
This was a deliberate choice to minimize dependencies on IPA internals,
avoiding the need to keep the hardware manager in constant sync with
IPA changes. However, reusing IPA’s existing utilities is a valid
alternative approach.</p>
</blockquote>

<!-- markdownlint-disable MD033 -->

<details>
<summary>Imports and constants</summary>

Standard library and IPA imports, plus configuration constants for
device paths, filesystem labels, and retry parameters.

```python
# SPDX-License-Identifier: Apache-2.0
# SPDX-FileCopyrightText: 2025 s3rj1k

"""Debian/Ubuntu OCI EFI LVM deployment hardware manager.

This hardware manager deploys Debian-based OCI container images with:
- EFI boot partition
- LVM on root partition
- Optional RAID1 support for two-disk configurations
"""

import os
import platform
import re
import shutil
import stat as stat_module
import subprocess
import tempfile
import time

import yaml

from oslo_log import log

from ironic_python_agent import device_hints
from ironic_python_agent import hardware

LOG = log.getLogger(__name__)

# Default OCI image (can be overridden via node metadata 'oci_image')
DEFAULT_OCI_IMAGE = "ubuntu:24.04"

# Device/filesystem constants
RAID_DEVICE = "/dev/md0"
VG_NAME = "vg_root"
LV_NAME = "lv_root"
ROOT_FS_LABEL = "ROOTFS"
BOOT_FS_LABEL = "EFI"
BOOT_FS_LABEL2 = "EFI2"
DEVICE_PROBE_MAX_ATTEMPTS = 5
DEVICE_PROBE_DELAY = 5
DEVICE_WAIT_MAX_ATTEMPTS = 5
DEVICE_WAIT_DELAY = 5
```

</details>

<details>
<summary>run_command</summary>

Wrapper around `subprocess.run` with logging support.

```python
def run_command(cmd, check=True, capture_output=True, timeout=300):
    """Run a shell command with logging.

    :param cmd: Command as list of strings
    :param check: Whether to raise on non-zero exit
    :param capture_output: Whether to capture stdout/stderr
    :param timeout: Command timeout in seconds
    :returns: CompletedProcess object
    :raises: subprocess.CalledProcessError on failure if check=True
    """
    LOG.debug("Running command: %s", " ".join(cmd))
    result = subprocess.run(
        cmd, check=check, capture_output=capture_output, text=True, timeout=timeout
    )
    if result.stdout:
        LOG.debug("stdout: %s", result.stdout)
    if result.stderr:
        LOG.debug("stderr: %s", result.stderr)
    return result
```

</details>

<details>
<summary>is_efi_system</summary>

Checks if the system booted in UEFI mode by testing for `/sys/firmware/efi`.

```python
def is_efi_system():
    """Check if the system is booted in EFI mode.

    :returns: True if running under EFI, False otherwise
    """
    return os.path.isdir("/sys/firmware/efi")
```

</details>

<details>
<summary>probe_device</summary>

Runs `partprobe` and waits for device to appear in the kernel.

```python
def probe_device(device):
    """Probe device until it is visible in the kernel.

    :param device: Device path (e.g., /dev/sda)
    :raises: RuntimeError if device doesn't appear after max attempts
    """
    for attempt in range(DEVICE_PROBE_MAX_ATTEMPTS):
        run_command(["partprobe", device], check=False)
        time.sleep(DEVICE_PROBE_DELAY)
        if os.path.exists(device):
            LOG.debug("Device %s visible after %d attempt(s)", device, attempt + 1)
            return
    raise RuntimeError(
        f"Device {device} not visible after " f"{DEVICE_PROBE_MAX_ATTEMPTS} attempts"
    )
```

</details>

<details>
<summary>has_interactive_users</summary>

Checks for logged-in users via `who` command, used to pause deployment
for debugging via BMC console.

```python
def has_interactive_users():
    """Check if there are any interactive users logged in.

    Uses 'who' command to check for logged-in users, which indicates
    someone has connected via BMC console for debugging.

    :returns: Boolean indicating if interactive users are logged in
    """
    try:
        result = run_command(["who"], check=True, timeout=5)
        # who returns empty output if no users are logged in
        users = result.stdout.strip()
        if users:
            LOG.debug("Interactive users detected: %s", users)
            return True
        return False
    except (subprocess.CalledProcessError, subprocess.TimeoutExpired, OSError) as e:
        LOG.warning("Failed to check for interactive users: %s", e)
        return False
```

</details>

<details>
<summary>get_configdrive_data</summary>

Extracts configdrive dictionary from node's `instance_info`.

```python
def get_configdrive_data(node):
    """Extract configdrive data from node instance_info.

    :param node: Node dictionary containing instance_info
    :returns: Dictionary containing configdrive data
    :raises: ValueError if node is invalid or configdrive data is missing
    """
    if node is None:
        raise ValueError("Node cannot be None")
    if not isinstance(node, dict):
        raise ValueError("Node must be a dictionary")

    instance_info = node.get("instance_info", {})
    if not isinstance(instance_info, dict):
        raise ValueError("instance_info must be a dictionary")

    configdrive = instance_info.get("configdrive")
    if configdrive is None:
        raise ValueError("configdrive not found in instance_info")

    if not isinstance(configdrive, dict):
        raise ValueError("configdrive must be a dictionary")

    LOG.info("Extracted configdrive data: %s", configdrive)
    return configdrive
```

</details>

<details>
<summary>parse_prefixed_hint_string</summary>

Parses simplified hint format like `serial=ABC123` or `wwn=0x123456` into
IPA hint dictionary format. Supports RAID1 with space-separated values.

```python
def parse_prefixed_hint_string(hint_string):
    """Parse a prefixed hint string into a hints dictionary.

    Supports simplified format for cloud-init/annotation use cases:
    - 'serial=ABC123' -&gt; {'serial': 's== ABC123'}
    - 'wwn=0x123456' -&gt; {'wwn': 's== 0x123456'}
    - 'serial=ABC123 DEF456' -&gt; {'serial': 's== ABC123 DEF456'} (RAID1)
    - 'wwn=0x123 0x456' -&gt; {'wwn': 's== 0x123 0x456'} (RAID1)

    :param hint_string: String with format 'hint_type=value1 [value2]'
    :returns: Dictionary containing root_device hints
    :raises: ValueError if format is invalid
    """
    if not hint_string or not isinstance(hint_string, str):
        raise ValueError("Hint string must be a non-empty string")

    hint_string = hint_string.strip()
    if "=" not in hint_string:
        raise ValueError(
            'Hint string must contain "=" separator. '
            'Expected format: "serial=VALUE" or "wwn=VALUE"'
        )

    # Split on first equals only
    parts = hint_string.split("=", 1)
    if len(parts) != 2:
        raise ValueError("Invalid hint string format")

    hint_type = parts[0].strip().lower()
    hint_values = parts[1].strip()

    if hint_type not in ("serial", "wwn"):
        raise ValueError(
            f'Unsupported hint type "{hint_type}". '
            'Only "serial" and "wwn" are supported.'
        )

    if not hint_values:
        raise ValueError(f"No value provided for {hint_type} hint")

    # Add s== operator prefix (string equality)
    hint_with_operator = f"s== {hint_values}"

    LOG.info(
        'Parsed prefixed hint string "%s" -&gt; {"%s": "%s"}',
        hint_string,
        hint_type,
        hint_with_operator,
    )

    return {hint_type: hint_with_operator}
```

</details>

<details>
<summary>get_root_device_hints</summary>

Extracts root device hints from configdrive annotation or node's
`instance_info`. Supports both simplified string format
(`serial=ABC123`) and standard dictionary format.

```python
def get_root_device_hints(node, configdrive_data):
    """Extract root_device hints from node instance_info or annotation.

    Priority order:
    1. configdrive meta_data.root_device_hints (prefixed string format)
    1. node.instance_info.root_device (dict format with operators)

    :param node: Node dictionary containing instance_info
    :param configdrive_data: Configdrive dictionary
    :returns: Dictionary containing root_device hints
    :raises: ValueError if node is invalid or root_device not found anywhere
    """
    if node is None:
        raise ValueError("Node cannot be None")
    if not isinstance(node, dict):
        raise ValueError("Node must be a dictionary")

    instance_info = node.get("instance_info", {})
    if not isinstance(instance_info, dict):
        raise ValueError("instance_info must be a dictionary")

    # Check annotation first (via configdrive metadata)
    meta_data = configdrive_data.get("meta_data", {})
    annotation_hints = meta_data.get("root_device_hints")

    if annotation_hints is not None:
        # Annotations use prefixed string format only
        if not isinstance(annotation_hints, str):
            raise ValueError(
                "root_device_hints from annotation must be a string "
                'in format "serial=VALUE" or "wwn=VALUE"'
            )

        parsed_hints = parse_prefixed_hint_string(annotation_hints)
        LOG.info("Using root_device hints from annotation: %s", parsed_hints)
        return parsed_hints

    # Fall back to instance_info
    root_device = instance_info.get("root_device")
    if root_device is not None:
        if not isinstance(root_device, dict):
            raise ValueError("root_device must be a dictionary")
        LOG.info("Using root_device hints from instance_info: %s", root_device)
        return root_device

    # Neither source provided root_device hints
    raise ValueError("root_device hints not found in instance_info or annotation")
```

</details>

<details>
<summary>find_device_by_hints</summary>

Uses IPA's `device_hints` module to find a block device by serial or WWN.

```python
def find_device_by_hints(hints):
    """Find a single block device matching the given hints.

    :param hints: Dictionary containing device hints (serial or wwn)
    :returns: Device path (e.g., /dev/sda)
    :raises: ValueError if no device or multiple devices match
    """
    devices = hardware.list_all_block_devices()
    LOG.debug("list_all_block_devices returned type: %s", type(devices).__name__)
    LOG.info("Found %d block devices", len(devices))
    serialized_devs = [dev.serialize() for dev in devices]

    matched_raw = device_hints.find_devices_by_hints(serialized_devs, hints)
    matched = list(matched_raw)

    if not matched:
        raise ValueError(f"No device found matching hints: {hints}")

    if len(matched) &gt; 1:
        device_names = [dev["name"] for dev in matched]
        raise ValueError(
            f"Multiple devices match hints: {device_names}. "
            f"Hints must match exactly one device."
        )

    return matched[0]["name"]
```

</details>

<details>
<summary>parse_hint_values</summary>

Parses hint strings, stripping operator prefixes and splitting multiple
values for RAID1 configurations.

```python
def parse_hint_values(hint):
    """Parse hint value, handling operator prefixes like 's=='.

    Returns list of values without the operator prefix.
    For RAID1: 's== SERIAL1 SERIAL2' -&gt; ['SERIAL1', 'SERIAL2']
    For single: 's== SERIAL1' -&gt; ['SERIAL1']
    For plain: 'SERIAL1 SERIAL2' -&gt; ['SERIAL1', 'SERIAL2']

    :param hint: Hint string value (may include operator prefix)
    :returns: List of values without operator prefix
    """
    if not hint:
        return []

    parts = hint.split()

    # Check if first part is an operator (e.g., 's==', 'int', etc.)
    operators = ("s==", "s!=", "<in>", "<or>", "int", "float")
    if parts and parts[0] in operators:
        return parts[1:]  # Skip the operator

    return parts
```

&lt;/details&gt;

<details>
<summary>resolve_root_devices</summary>

Resolves device paths from hints. Returns one device for single-disk
or two devices for RAID1 configuration.

```python
def resolve_root_devices(root_device_hints):
    """Resolve root device path(s) from hints.

    Only serial or wwn hints are supported. If the hint contains two
    space-separated values, both devices are resolved for RAID1 setup.

    :param root_device_hints: Dictionary containing root device hints
    :returns: Tuple of device paths - (primary,) for single device or
              (primary, secondary) for RAID1 configuration
    :raises: ValueError if device cannot be resolved or hints are invalid
    """
    if root_device_hints is None:
        raise ValueError("root_device_hints cannot be None")

    if not isinstance(root_device_hints, dict):
        raise ValueError("root_device_hints must be a dictionary")

    # Validate that only serial or wwn hints are present
    serial_hint = root_device_hints.get("serial")
    wwn_hint = root_device_hints.get("wwn")

    if not serial_hint and not wwn_hint:
        raise ValueError("root_device_hints must contain serial or wwn hint")

    # Check for unsupported hint types
    supported_hints = {"serial", "wwn"}
    provided_hints = set(root_device_hints.keys())
    unsupported = provided_hints - supported_hints

    if unsupported:
        raise ValueError(
            f"Unsupported root_device hints: {unsupported}. "
            f"Only serial and wwn are supported."
        )

    LOG.info("Resolving root devices from hints: %s", root_device_hints)

    # Parse hints - may contain one or two values (with optional operator)
    serial_values = parse_hint_values(serial_hint)
    wwn_values = parse_hint_values(wwn_hint)

    # Determine if this is a RAID1 configuration
    is_raid = len(serial_values) == 2 or len(wwn_values) == 2

    if is_raid:
        LOG.info("RAID1 configuration detected")

    # Resolve primary device
    primary_hints = {}
    if serial_values:
        primary_hints["serial"] = serial_values[0]
    if wwn_values:
        primary_hints["wwn"] = wwn_values[0]

    primary_device = find_device_by_hints(primary_hints)
    LOG.info("Resolved primary device: %s", primary_device)

    if not is_raid:
        return (primary_device,)

    # Resolve secondary device for RAID1
    secondary_hints = {}
    if len(serial_values) == 2:
        secondary_hints["serial"] = serial_values[1]
    if len(wwn_values) == 2:
        secondary_hints["wwn"] = wwn_values[1]

    secondary_device = find_device_by_hints(secondary_hints)
    LOG.info("Resolved secondary device: %s", secondary_device)

    return (primary_device, secondary_device)
```

</details>

<details>
<summary>get_oci_image</summary>

Gets OCI image reference with priority: `spec.image.url` (with `oci://`
prefix) &gt; configdrive annotation &gt; default `ubuntu:24.04`.

```python
def get_oci_image(node, configdrive_data):
    """Get OCI image from instance_info, metadata, or use default.

    Priority order:
    1. node.instance_info.image_source with oci:// prefix
    1. configdrive meta_data.oci_image (from annotation)
    1. DEFAULT_OCI_IMAGE

    :param node: Node dictionary containing instance_info
    :param configdrive_data: Configdrive dictionary
    :returns: OCI image reference string (without oci:// prefix)
    """
    oci_image = None

    # Check instance_info first
    instance_info = node.get("instance_info", {})
    image_source = instance_info.get("image_source", "").strip()

    if image_source.startswith("oci://"):
        oci_image = image_source.removeprefix("oci://").strip()
        if not oci_image:
            LOG.warning(
                "Empty OCI image after stripping oci:// prefix, "
                "falling back to annotation/default"
            )
            oci_image = None
        else:
            LOG.info("Using OCI image from instance_info: %s", oci_image)
    else:
        # Fall back to annotation (via configdrive metadata)
        meta_data = configdrive_data.get("meta_data", {})
        annotation_image = (meta_data.get("oci_image") or "").strip()

        if annotation_image:
            oci_image = annotation_image
            LOG.info("Using OCI image from annotation: %s", oci_image)
        else:
            # Fall back to default
            oci_image = DEFAULT_OCI_IMAGE
            LOG.info("Using default OCI image: %s", oci_image)

    return oci_image
```

</details>

<details>
<summary>get_disk_wipe_mode</summary>

Determines disk cleaning behavior based on annotation or setup type. Returns
`all` to wipe all block devices (default for RAID1) or `target` to wipe only
specified disks (default for single disk).

```python
def get_disk_wipe_mode(configdrive_data, is_raid):
    """Get disk wipe mode from configdrive or use default based on setup.

    Priority order:
    1. configdrive meta_data.disk_wipe_mode (from annotation)
    1. Default: "all" for RAID1, "target" for single disk

    :param configdrive_data: Configdrive dictionary
    :param is_raid: Boolean indicating if this is a RAID setup
    :returns: String "all" or "target"
    :raises: ValueError if disk_wipe_mode has invalid value
    """
    meta_data = configdrive_data.get("meta_data", {})
    wipe_mode = (meta_data.get("disk_wipe_mode") or "").strip().lower()

    if wipe_mode:
        if wipe_mode not in ("all", "target"):
            raise ValueError(
                f'Invalid disk_wipe_mode "{wipe_mode}". '
                'Valid values are: "all", "target"'
            )
        LOG.info("Using disk wipe mode from annotation: %s", wipe_mode)
        return wipe_mode

    # Use default based on setup type
    default_mode = "all" if is_raid else "target"
    LOG.info(
        "Using default disk wipe mode for %s setup: %s",
        "RAID1" if is_raid else "single disk",
        default_mode,
    )
    return default_mode
```

</details>

<details>
<summary>get_architecture_config</summary>

Returns architecture-specific settings for x86_64 or ARM64, including
GRUB packages and UEFI target.

```python
def get_architecture_config(oci_image):
    """Get architecture-specific configuration.

    :param oci_image: OCI image reference to use
    :returns: Dictionary with oci_image, oci_platform, uefi_target,
              and grub_packages
    :raises: RuntimeError if architecture is not supported
    """
    machine = platform.machine()

    if machine == "x86_64":
        return {
            "oci_image": oci_image,
            "oci_platform": "linux/amd64",
            "uefi_target": "x86_64-efi",
            "grub_packages": ["grub-efi-amd64", "grub-efi-amd64-signed", "shim-signed"],
        }
    elif machine == "aarch64":
        return {
            "oci_image": oci_image,
            "oci_platform": "linux/arm64",
            "uefi_target": "arm64-efi",
            "grub_packages": ["grub-efi-arm64", "grub-efi-arm64-bin"],
        }
    else:
        raise RuntimeError(f"Unsupported architecture: {machine}")
```

</details>

<details>
<summary>wait_for_device</summary>

Waits for a block device to become available with retries.

```python
def wait_for_device(device):
    """Wait for a block device to become available.

    :param device: Device path (e.g., /dev/sda)
    :returns: True if device is available
    :raises: RuntimeError if device doesn't appear
    """
    for attempt in range(DEVICE_WAIT_MAX_ATTEMPTS):
        if os.path.exists(device):
            try:
                mode = os.stat(device).st_mode
                if stat_module.S_ISBLK(mode):
                    LOG.info("Device %s is available", device)
                    return True
            except OSError:
                pass
        LOG.debug(
            "Waiting for device %s (attempt %d/%d)",
            device,
            attempt + 1,
            DEVICE_WAIT_MAX_ATTEMPTS,
        )
        time.sleep(DEVICE_WAIT_DELAY)

    raise RuntimeError(f"Device {device} did not become available")
```

</details>

<details>
<summary>get_partition_path</summary>

Returns partition path, handling NVMe and MMC naming conventions.

```python
def get_partition_path(device, partition_number):
    """Get the partition path for a device.

    :param device: Base device path (e.g., /dev/sda)
    :param partition_number: Partition number
    :returns: Partition path (e.g., /dev/sda1 or /dev/nvme0n1p1)
    """
    if re.match(r".*/nvme\d+n\d+$", device) or re.match(r".*/mmcblk\d+$", device):
        return f"{device}p{partition_number}"

    return f"{device}{partition_number}"
```

</details>

<details>
<summary>clean_device</summary>

Removes existing partitions, RAID arrays, LVM structures, and wipes
the device.

```python
def clean_device(device):
    """Clean a device of existing partitions, RAID, and LVM.

    :param device: Device path to clean
    """
    LOG.info("Cleaning device: %s", device)

    # Stop any RAID arrays using this device
    try:
        result = run_command(["lsblk", "-nlo", "NAME,TYPE", device], check=False)
        for line in result.stdout.strip().split("\n"):
            parts = line.split()
            if len(parts) &gt;= 2 and parts[1] in (
                "raid1",
                "raid0",
                "raid5",
                "raid6",
                "raid10",
            ):
                raid_dev = f"/dev/{parts[0]}"
                run_command(["mdadm", "--stop", raid_dev], check=False)
    except Exception:
        pass

    # Remove LVM if present (check device and all its partitions)
    try:
        # Get all block devices (device + partitions)
        result = run_command(["lsblk", "-nlo", "NAME", device], check=False)
        all_devs = []
        for line in result.stdout.strip().split("\n"):
            name = line.strip()
            if name:
                all_devs.append(f"/dev/{name}")

        # Find all VGs that use any of these devices
        vgs_to_remove = set()
        for dev in all_devs:
            result = run_command(["pvs", dev], check=False)
            if result.returncode == 0:
                vg_result = run_command(
                    ["pvs", "--noheadings", "-o", "vg_name", dev], check=False
                )
                vg_name = vg_result.stdout.strip()
                if vg_name:
                    vgs_to_remove.add(vg_name)

        # Deactivate, remove all LVs and VGs
        for vg_name in vgs_to_remove:
            # Deactivate all LVs in this VG
            run_command(["lvchange", "-an", vg_name], check=False)

            lv_result = run_command(
                ["lvs", "--noheadings", "-o", "lv_path", vg_name], check=False
            )
            for lv_path in lv_result.stdout.strip().split("\n"):
                lv_path = lv_path.strip()
                if lv_path:
                    # Try dmsetup remove for stubborn LVs
                    dm_name = lv_path.replace("/dev/", "").replace("/", "-")
                    run_command(
                        ["dmsetup", "remove", "--retry", "-f", dm_name], check=False
                    )
                    run_command(["lvremove", "-ff", lv_path], check=False)
            run_command(["vgremove", "-ff", vg_name], check=False)

        # Remove PVs from all devices
        for dev in all_devs:
            run_command(["pvremove", "-ff", "-y", dev], check=False)
    except Exception:
        pass

    # Zero RAID superblocks
    run_command(["mdadm", "--zero-superblock", "--force", device], check=False)

    # Zero superblocks on partitions
    try:
        result = run_command(["lsblk", "-nlo", "NAME", device], check=False)
        base_name = os.path.basename(device)
        for line in result.stdout.strip().split("\n"):
            name = line.strip()
            if name and name != base_name:
                part_dev = f"/dev/{name}"
                run_command(
                    ["mdadm", "--zero-superblock", "--force", part_dev], check=False
                )
                run_command(["wipefs", "--all", "--force", part_dev], check=False)
    except Exception:
        pass

    # Wipe device
    run_command(["wipefs", "--all", "--force", device], check=False)
    run_command(["sgdisk", "--zap-all", device], check=False)

    # Sync filesystem buffers and wait for udev to settle
    run_command(["sync"], check=False)
    run_command(["udevadm", "settle"], check=False)

    # Probe until device is visible again
    probe_device(device)

    LOG.info("Device %s cleaned", device)
```

</details>

<details>
<summary>clean_all_devices</summary>

Cleans all block devices on the system to remove stray RAID/LVM metadata.
Useful when `disk_wipe_mode` is set to `all` (default for RAID1 setups).

```python
def clean_all_devices():
    """Clean all block devices to remove stray RAID/LVM metadata.

    Useful for nodes that may have multiple disks with old metadata
    from previous deployments.
    """
    LOG.info("Cleaning all block devices on the system")

    try:
        devices = hardware.list_all_block_devices()
        LOG.info("Found %d block devices to clean", len(devices))

        for device_obj in devices:
            device = device_obj.name
            try:
                clean_device(device)
            except Exception as e:
                LOG.warning("Error cleaning device %s: %s", device, e)

        LOG.info("Finished cleaning all block devices")
    except Exception as e:
        LOG.error("Error listing block devices: %s", e)
```

</details>

<details>
<summary>clean_partition_signatures</summary>

Cleans RAID, LVM, and filesystem signatures from a partition without
removing the partition itself. Used internally by `partition_disk()` to
clean partitions before creating RAID arrays, ensuring no stray metadata
causes issues.

```python
def clean_partition_signatures(partition):
    """Clean RAID, LVM, and filesystem signatures from a partition.

    Does not remove the partition itself, only metadata/signatures.

    :param partition: Partition path to clean
    """
    LOG.debug("Cleaning signatures from partition: %s", partition)
    run_command(["pvremove", "-ff", "-y", partition], check=False)
    run_command(["wipefs", "--all", "--force", partition], check=False)
    run_command(["mdadm", "--zero-superblock", "--force", partition], check=False)
```

</details>

<details>
<summary>partition_disk</summary>

Creates GPT partition table with EFI and LVM partitions. Sets up RAID1
array if second device is provided. Calls `clean_partition_signatures()`
before RAID creation to ensure clean metadata.

```python
def partition_disk(
    device, vg_name, lv_name, second_device=None, raid_device=RAID_DEVICE, homehost=None
):
    """Partition disk with EFI and LVM (optionally on RAID).

    :param device: Primary device path
    :param vg_name: Volume group name
    :param lv_name: Logical volume name
    :param second_device: Optional second device for RAID
    :param raid_device: RAID device path
    :param homehost: Hostname for RAID array
    :returns: Tuple of (is_raid, pv_device)
    """
    LOG.info("Partitioning disk: %s", device)

    wait_for_device(device)

    # Ensure udev has finished processing before partitioning
    run_command(["udevadm", "settle"], check=False)

    # Create GPT partition table
    run_command(["parted", "-s", device, "mklabel", "gpt"])

    # Create EFI partition (2GB)
    run_command(
        [
            "parted",
            "-s",
            "-a",
            "optimal",
            device,
            "mkpart",
            "primary",
            "fat32",
            "2MiB",
            "2050MiB",
        ]
    )
    run_command(["parted", "-s", device, "set", "1", "esp", "on"])

    # Create data partition (rest of disk)
    run_command(
        ["parted", "-s", "-a", "optimal", device, "mkpart", "primary", "2050MiB", "99%"]
    )

    is_raid = second_device is not None

    if is_raid:
        run_command(["parted", "-s", device, "set", "2", "raid", "on"])
    else:
        run_command(["parted", "-s", device, "set", "2", "lvm", "on"])

    # Wipe new partitions
    try:
        result = run_command(["lsblk", "-nlo", "NAME", device], check=False)
        base_name = os.path.basename(device)
        for line in result.stdout.strip().split("\n"):
            name = line.strip()
            if name and name != base_name:
                run_command(["wipefs", "-a", f"/dev/{name}"], check=False)
    except Exception:
        pass

    data_partition = get_partition_path(device, 2)
    pv_device = data_partition

    if is_raid:
        probe_device(device)
        probe_device(second_device)

        # Clone partition table to second device
        sfdisk_result = run_command(["sfdisk", "-d", device])
        LOG.debug("Cloning partition table to %s", second_device)
        sfdisk_proc = subprocess.run(
            ["sfdisk", "--force", second_device],
            input=sfdisk_result.stdout,
            capture_output=True,
            text=True,
            check=False,
        )
        if sfdisk_proc.stdout:
            LOG.debug("sfdisk stdout: %s", sfdisk_proc.stdout)
        if sfdisk_proc.stderr:
            LOG.debug("sfdisk stderr: %s", sfdisk_proc.stderr)
        if sfdisk_proc.returncode != 0:
            raise subprocess.CalledProcessError(
                sfdisk_proc.returncode,
                ["sfdisk", "--force", second_device],
                sfdisk_proc.stdout,
                sfdisk_proc.stderr,
            )

        # Randomize partition GUIDs on second device
        run_command(["sgdisk", "--partition-guid=1:R", second_device])
        run_command(["sgdisk", "--partition-guid=2:R", second_device])

        second_data_partition = get_partition_path(second_device, 2)
        probe_device(second_data_partition)

        if not homehost:
            raise RuntimeError("homehost required for RAID configuration")

        # Clean new partitions before creating RAID
        LOG.info("Cleaning partition signatures before RAID creation")
        clean_partition_signatures(data_partition)
        clean_partition_signatures(second_data_partition)

        # Create RAID array
        run_command(
            [
                "mdadm",
                "--create",
                raid_device,
                "--level=1",
                "--raid-devices=2",
                "--metadata=1.2",
                "--name=root",
                "--bitmap=internal",
                f"--homehost={homehost}",
                "--force",
                "--run",
                "--assume-clean",
                data_partition,
                second_data_partition,
            ]
        )

        # Sync filesystem buffers before continuing
        run_command(["sync"], check=False)
        time.sleep(5)
        pv_device = raid_device
    else:
        probe_device(device)

    # Create LVM
    run_command(["pvcreate", "-ff", "-y", "--zero", "y", pv_device])
    run_command(["vgcreate", "-y", vg_name, pv_device])
    run_command(["lvcreate", "-y", "-W", "y", "-n", lv_name, "-l", "100%FREE", vg_name])

    LOG.info("Disk partitioned successfully, is_raid=%s", is_raid)
    return is_raid, pv_device
```

</details>

<details>
<summary>create_filesystems</summary>

Creates FAT32 filesystem on EFI partition and ext4 on root LV.

```python
def create_filesystems(
    efi_partition,
    root_lv_path,
    boot_label=BOOT_FS_LABEL,
    root_label=ROOT_FS_LABEL,
    second_efi_partition=None,
    boot_label2=BOOT_FS_LABEL2,
):
    """Create filesystems on partitions.

    :param efi_partition: EFI partition path
    :param root_lv_path: Root LV path
    :param boot_label: EFI partition label
    :param root_label: Root partition label
    :param second_efi_partition: Second EFI partition for RAID
    :param boot_label2: Second EFI partition label
    """
    LOG.info("Creating filesystems")

    run_command(["mkfs.vfat", "-F", "32", "-n", boot_label, efi_partition])

    if second_efi_partition:
        run_command(
            ["mkfs.vfat", "-F", "32", "-n", boot_label2, second_efi_partition],
            check=False,
        )

    run_command(["mkfs.ext4", "-F", "-L", root_label, root_lv_path])

    LOG.info("Filesystems created")
```

</details>

<details>
<summary>setup_chroot</summary>

Mounts `/proc`, `/sys`, `/dev` and sets up DNS resolution in chroot.

```python
def setup_chroot(chroot_dir):
    """Set up chroot environment with necessary mounts.

    :param chroot_dir: Path to chroot directory
    """
    LOG.info("Setting up chroot: %s", chroot_dir)

    run_command(["mount", "-t", "proc", "proc", f"{chroot_dir}/proc"])
    run_command(["mount", "-t", "sysfs", "sys", f"{chroot_dir}/sys"])
    run_command(["mount", "--bind", "/dev", f"{chroot_dir}/dev"])
    run_command(["mount", "--bind", "/dev/pts", f"{chroot_dir}/dev/pts"])

    os.makedirs(f"{chroot_dir}/run", exist_ok=True)

    # Set up resolv.conf
    resolv_link = os.path.join(chroot_dir, "etc", "resolv.conf")
    if os.path.islink(resolv_link):
        target = os.readlink(resolv_link)
        if target.startswith("/"):
            target_path = os.path.join(chroot_dir, target.lstrip("/"))
        else:
            target_path = os.path.join(chroot_dir, "etc", target)

        os.makedirs(os.path.dirname(target_path), exist_ok=True)
        shutil.copy("/etc/resolv.conf", target_path)
    else:
        shutil.copy("/etc/resolv.conf", resolv_link)

    LOG.info("Chroot setup complete")
```

</details>

<details>
<summary>teardown_chroot</summary>

Unmounts chroot bind mounts in reverse order.

```python
def teardown_chroot(chroot_dir):
    """Tear down chroot environment.

    :param chroot_dir: Path to chroot directory
    """
    LOG.info("Tearing down chroot: %s", chroot_dir)

    mounts = [
        f"{chroot_dir}/run",
        f"{chroot_dir}/dev/pts",
        f"{chroot_dir}/dev",
        f"{chroot_dir}/sys",
        f"{chroot_dir}/proc",
    ]

    for mount in mounts:
        try:
            result = run_command(["mountpoint", "-q", mount], check=False)
            if result.returncode == 0:
                run_command(["umount", "-l", mount])
        except Exception as e:
            LOG.warning("Error unmounting %s: %s", mount, e)

    LOG.info("Chroot teardown complete")
```

</details>

<details>
<summary>extract_oci_image</summary>

Extracts OCI image filesystem using `crane export` piped to `tar`.

```python
def extract_oci_image(image, platform, dest_dir):
    """Extract OCI image rootfs using crane.

    :param image: OCI image reference (e.g., ubuntu:24.04)
    :param platform: Target platform (e.g., linux/amd64)
    :param dest_dir: Destination directory for rootfs
    """
    LOG.info("Extracting OCI image %s (%s) to %s", image, platform, dest_dir)

    # Use crane export to extract the image filesystem
    # crane export outputs a tar stream, pipe to tar for extraction
    crane_cmd = ["crane", "export", "--platform", platform, image, "-"]
    tar_cmd = ["tar", "-xf", "-", "-C", dest_dir]

    LOG.info("Running: %s | %s", " ".join(crane_cmd), " ".join(tar_cmd))

    # Create pipeline: crane export | tar extract
    crane_proc = subprocess.Popen(
        crane_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE
    )

    tar_proc = subprocess.Popen(
        tar_cmd, stdin=crane_proc.stdout, stdout=subprocess.PIPE, stderr=subprocess.PIPE
    )

    # Allow crane to receive SIGPIPE if tar exits
    crane_proc.stdout.close()

    # Wait for tar to complete
    tar_stdout, tar_stderr = tar_proc.communicate(timeout=1800)

    # Wait for crane to complete
    crane_proc.wait()

    if crane_proc.returncode != 0:
        _, crane_stderr = crane_proc.communicate()
        raise RuntimeError(
            f"crane export failed with code {crane_proc.returncode}: "
            f"{crane_stderr.decode() if crane_stderr else 'unknown error'}"
        )

    if tar_proc.returncode != 0:
        raise RuntimeError(
            f"tar extract failed with code {tar_proc.returncode}: "
            f"{tar_stderr.decode() if tar_stderr else 'unknown error'}"
        )

    if tar_stderr:
        LOG.debug("tar stderr: %s", tar_stderr.decode())

    LOG.info("OCI image extraction complete")
```

</details>

<details>
<summary>install_packages</summary>

Installs cloud-init, GRUB, kernel, and other required packages via apt.

```python
def install_packages(chroot_dir, grub_packages):
    """Install required packages in chroot.

    :param chroot_dir: Path to chroot directory
    :param grub_packages: List of GRUB packages to install
    """
    LOG.info("Installing packages in chroot")

    # Remove snap packages if present
    snap_path = os.path.join(chroot_dir, "usr", "bin", "snap")
    if os.path.exists(snap_path):
        snap_patterns = [
            "!/^Name|^core|^snapd|^lxd/",
            "/^lxd/",
            "/^core/",
            "/^snapd/",
            "!/^Name/",
        ]
        for pattern in snap_patterns:
            try:
                run_command(
                    [
                        "chroot",
                        chroot_dir,
                        "sh",
                        "-c",
                        f"snap list 2&gt;/dev/null | awk '{pattern} ' | "
                        "xargs -rI{} snap remove --purge {}",
                    ],
                    check=False,
                )
            except Exception:
                pass

    # Update package lists
    run_command(["chroot", chroot_dir, "apt-get", "update"])

    # Remove unwanted packages one by one, ignoring errors for missing packages
    for pkg in ["lxd", "lxd-agent-loader", "lxd-installer", "snapd"]:
        run_command(
            ["chroot", chroot_dir, "apt-get", "--purge", "remove", "-y", pkg],
            check=False,
        )

    # Install required packages
    packages = [
        "cloud-init",
        "curl",
        "efibootmgr",
        "grub-common",
        "initramfs-tools",
        "lvm2",
        "mdadm",
        "netplan.io",
        "rsync",
        "sudo",
        "systemd-sysv",
    ] + grub_packages
    run_command(["chroot", chroot_dir, "apt-get", "install", "-y"] + packages)

    # Install kernel based on distro
    try:
        os_release_path = os.path.join(chroot_dir, "etc", "os-release")
        distro_id = None
        version_id = None
        if os.path.exists(os_release_path):
            with open(os_release_path, "r", encoding="utf-8") as f:
                for line in f:
                    if line.startswith("ID="):
                        distro_id = line.split("=")[1].strip().strip('"')
                    elif line.startswith("VERSION_ID="):
                        version_id = line.split("=")[1].strip().strip('"')

        if distro_id == "ubuntu" and version_id:
            # Ubuntu: install HWE kernel
            run_command(
                [
                    "chroot",
                    chroot_dir,
                    "apt-get",
                    "install",
                    "-y",
                    f"linux-generic-hwe-{version_id}",
                ],
                check=False,
            )
        elif distro_id == "debian":
            # Debian: install standard kernel metapackage
            arch = platform.machine()
            if arch == "x86_64":
                kernel_pkg = "linux-image-amd64"
            elif arch == "aarch64":
                kernel_pkg = "linux-image-arm64"
            else:
                kernel_pkg = "linux-image-" + arch
            run_command(
                ["chroot", chroot_dir, "apt-get", "install", "-y", kernel_pkg],
                check=False,
            )
    except Exception as e:
        LOG.warning("Error installing kernel: %s", e)

    # Clean up removed packages
    try:
        result = run_command(["chroot", chroot_dir, "dpkg", "-l"], check=False)
        rc_packages = []
        for line in result.stdout.split("\n"):
            if line.startswith("rc "):
                parts = line.split()
                if len(parts) &gt;= 2:
                    rc_packages.append(parts[1])

        if rc_packages:
            run_command(
                ["chroot", chroot_dir, "apt-get", "purge", "-y"] + rc_packages,
                check=False,
            )
    except Exception:
        pass

    run_command(
        ["chroot", chroot_dir, "apt-get", "autoremove", "--purge", "-y"], check=False
    )

    LOG.info("Package installation complete")
```

</details>

<details>
<summary>write_hosts_file</summary>

Writes `/etc/hosts` with localhost and IPv6 entries.

```python
def write_hosts_file(mount_point, hostname):
    """Write /etc/hosts file with proper entries.

    :param mount_point: Root mount point
    :param hostname: System hostname
    """
    LOG.info("Writing /etc/hosts file")

    hosts_path = os.path.join(mount_point, "etc", "hosts")

    with open(hosts_path, "w", encoding="utf-8") as f:
        f.write(f"127.0.0.1\tlocalhost\t{hostname}\n")
        f.write("\n")
        f.write("# The following lines are desirable for IPv6 capable hosts\n")
        f.write("::1\tip6-localhost\tip6-loopback\n")
        f.write("fe00::0\tip6-localnet\n")
        f.write("ff00::0\tip6-mcastprefix\n")
        f.write("ff02::1\tip6-allnodes\n")
        f.write("ff02::2\tip6-allrouters\n")
        f.write("ff02::3\tip6-allhosts\n")

    LOG.info("/etc/hosts written with hostname: %s", hostname)
```

</details>

<details>
<summary>configure_cloud_init</summary>

Configures cloud-init NoCloud datasource with metadata, userdata, and
network config from configdrive.

```python
def configure_cloud_init(mount_point, configdrive_data):
    """Configure cloud-init with configdrive data.

    :param mount_point: Root mount point
    :param configdrive_data: Configdrive dictionary
    """
    LOG.info("Configuring cloud-init")

    cloud_init_cfg_dir = os.path.join(mount_point, "etc", "cloud", "cloud.cfg.d")
    os.makedirs(cloud_init_cfg_dir, exist_ok=True)

    nocloud_seed_dir = os.path.join(
        mount_point, "var", "lib", "cloud", "seed", "nocloud-net"
    )
    os.makedirs(nocloud_seed_dir, exist_ok=True)

    # Write datasource config
    datasource_cfg = os.path.join(cloud_init_cfg_dir, "99-nocloud-seed.cfg")
    with open(datasource_cfg, "w", encoding="utf-8") as f:
        f.write(
            """datasource_list: [ NoCloud, None ]
datasource:
  NoCloud:
    seedfrom: file:///var/lib/cloud/seed/nocloud-net/
"""
        )

    # Write meta-data
    meta_data = configdrive_data.get("meta_data", {})
    meta_data_path = os.path.join(nocloud_seed_dir, "meta-data")
    with open(meta_data_path, "w", encoding="utf-8") as f:
        yaml.safe_dump(meta_data, f, default_flow_style=False)

    # Write user-data
    user_data = configdrive_data.get("user_data", "")
    user_data_path = os.path.join(nocloud_seed_dir, "user-data")
    with open(user_data_path, "w", encoding="utf-8") as f:
        f.write(user_data if user_data else "")

    # Write network-config if present
    network_data = configdrive_data.get("network_data", {})
    if network_data:
        network_config_path = os.path.join(nocloud_seed_dir, "network-config")
        with open(network_config_path, "w", encoding="utf-8") as f:
            yaml.safe_dump(network_data, f, default_flow_style=False)

    # Set permissions
    for filename in os.listdir(nocloud_seed_dir):
        filepath = os.path.join(nocloud_seed_dir, filename)
        os.chmod(filepath, 0o600)

    LOG.info("Cloud-init configuration complete")
```

</details>

<details>
<summary>write_fstab</summary>

Writes `/etc/fstab` with root and EFI entries, plus second EFI for RAID.

```python
def write_fstab(mount_point, root_label, boot_label, is_raid, boot_label2=None):
    """Write /etc/fstab.

    :param mount_point: Root mount point
    :param root_label: Root partition label
    :param boot_label: EFI partition label
    :param is_raid: Whether RAID is configured
    :param boot_label2: Second EFI partition label
    """
    LOG.info("Writing fstab")

    fstab_path = os.path.join(mount_point, "etc", "fstab")
    with open(fstab_path, "w", encoding="utf-8") as f:
        f.write(f"LABEL={root_label}\t/\text4\terrors=remount-ro\t0\t1\n")
        f.write(f"LABEL={boot_label}\t/boot/efi\tvfat\tumask=0077,nofail\t0\t1\n")

        if is_raid and boot_label2:
            f.write(
                f"LABEL={boot_label2}\t/boot/efi2\tvfat\t"
                f"umask=0077,nofail,noauto\t0\t2\n"
            )

    LOG.info("fstab written")
```

</details>

<details>
<summary>write_mdadm_conf</summary>

Writes `/etc/mdadm/mdadm.conf` with RAID array configuration.

```python
def write_mdadm_conf(mount_point):
    """Write mdadm configuration.

    :param mount_point: Root mount point
    """
    LOG.info("Writing mdadm.conf")

    mdadm_dir = os.path.join(mount_point, "etc", "mdadm")
    os.makedirs(mdadm_dir, exist_ok=True)

    mdadm_conf_path = os.path.join(mdadm_dir, "mdadm.conf")

    with open(mdadm_conf_path, "w", encoding="utf-8") as f:
        f.write("HOMEHOST <system>\n")
        f.write("MAILADDR root\n")

    # Append ARRAY lines from mdadm --detail --scan
    result = run_command(["mdadm", "--detail", "--scan", "--verbose"])
    with open(mdadm_conf_path, "a", encoding="utf-8") as f:
        for line in result.stdout.split("\n"):
            if line.startswith("ARRAY"):
                f.write(line + "\n")

    LOG.info("mdadm.conf written")
```

&lt;/details&gt;

<details>
<summary>configure_initramfs</summary>

Configures initramfs-tools to include LVM and RAID modules.

```python
def configure_initramfs(chroot_dir, is_raid):
    """Configure initramfs-tools for LVM and optionally RAID.

    This ensures initramfs includes LVM modules.

    :param chroot_dir: Chroot directory path
    :param is_raid: Whether RAID is configured
    """
    LOG.info("Configuring initramfs-tools")

    initramfs_conf_dir = os.path.join(chroot_dir, "etc", "initramfs-tools", "conf.d")
    os.makedirs(initramfs_conf_dir, exist_ok=True)

    # Disable resume (no swap partition)
    resume_conf = os.path.join(initramfs_conf_dir, "resume")
    with open(resume_conf, "w", encoding="utf-8") as f:
        f.write("RESUME=none\n")

    # Force LVM inclusion in initramfs
    # This is needed because during chroot, LVM volumes may not be
    # detected by the initramfs-tools hooks
    initramfs_conf = os.path.join(
        chroot_dir, "etc", "initramfs-tools", "initramfs.conf"
    )
    if os.path.exists(initramfs_conf):
        with open(initramfs_conf, "r", encoding="utf-8") as f:
            content = f.read()
        # Set MODULES to "most" to include storage drivers
        content = re.sub(r"^MODULES=.*$", "MODULES=most", content, flags=re.MULTILINE)
        with open(initramfs_conf, "w", encoding="utf-8") as f:
            f.write(content)

    # Add LVM modules explicitly
    modules_file = os.path.join(chroot_dir, "etc", "initramfs-tools", "modules")
    lvm_modules = ["dm-mod", "dm-snapshot", "dm-mirror", "dm-zero"]
    if is_raid:
        lvm_modules.extend(["raid1", "md-mod"])

    existing_modules = set()
    if os.path.exists(modules_file):
        with open(modules_file, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#"):
                    existing_modules.add(line)

    with open(modules_file, "a", encoding="utf-8") as f:
        for module in lvm_modules:
            if module not in existing_modules:
                f.write(f"{module}\n")

    LOG.info("initramfs-tools configuration complete")
```

</details>

<details>
<summary>setup_grub_defaults</summary>

Configures `/etc/default/grub` with root device and RAID options.

```python
def setup_grub_defaults(chroot_dir, root_label, is_raid):
    """Configure GRUB defaults.

    :param chroot_dir: Chroot directory path
    :param root_label: Root partition label
    :param is_raid: Whether RAID is configured
    """
    LOG.info("Setting up GRUB defaults")

    grub_default = os.path.join(chroot_dir, "etc", "default", "grub")

    with open(grub_default, "r", encoding="utf-8") as f:
        content = f.read()

    # Build GRUB_CMDLINE_LINUX
    cmdline = f"root=LABEL={root_label}"
    if is_raid:
        cmdline += " rd.auto=1"

    # Update GRUB_CMDLINE_LINUX
    content = re.sub(
        r"^#*\s*GRUB_CMDLINE_LINUX=.*$",
        f'GRUB_CMDLINE_LINUX="{cmdline}"',
        content,
        flags=re.MULTILINE,
    )

    # Update GRUB_DISABLE_LINUX_UUID
    if "GRUB_DISABLE_LINUX_UUID=" in content:
        content = re.sub(
            r"^#*\s*GRUB_DISABLE_LINUX_UUID=.*$",
            "GRUB_DISABLE_LINUX_UUID=true",
            content,
            flags=re.MULTILINE,
        )
    else:
        content += "\nGRUB_DISABLE_LINUX_UUID=true\n"

    # Add rootdelay for RAID
    if is_raid:
        if "GRUB_CMDLINE_LINUX_DEFAULT=" in content:
            if "rootdelay=" not in content:
                content = re.sub(
                    r'^(#*\s*GRUB_CMDLINE_LINUX_DEFAULT="[^"]*)',
                    r"\1 rootdelay=10",
                    content,
                    flags=re.MULTILINE,
                )
        else:
            content += '\nGRUB_CMDLINE_LINUX_DEFAULT="rootdelay=10"\n'

    with open(grub_default, "w", encoding="utf-8") as f:
        f.write(content)

    LOG.info("GRUB defaults configured")
```

</details>

<details>
<summary>setup_grub_efi_sync</summary>

Creates GRUB hook script to sync EFI partitions for RAID redundancy.

```python
def setup_grub_efi_sync(chroot_dir, boot_label2):
    """Set up GRUB hook to sync EFI partitions for RAID.

    :param chroot_dir: Chroot directory path
    :param boot_label2: Second EFI partition label
    """
    LOG.info("Setting up GRUB EFI sync hook")

    grub_hook = os.path.join(chroot_dir, "etc", "grub.d", "90_copy_to_boot_efi2")

    with open(grub_hook, "w", encoding="utf-8") as f:
        f.write(
            f"""#!/bin/sh
# Sync GRUB updates to both EFI partitions for RAID redundancy
set -e

if mountpoint --quiet --nofollow /boot/efi; then
    mount LABEL={boot_label2} /boot/efi2 || :
    rsync --times --recursive --delete /boot/efi/ /boot/efi2/
    umount -l /boot/efi2
fi
exit 0
"""
        )

    os.chmod(grub_hook, 0o755)  # nosec B103
    LOG.info("GRUB EFI sync hook created")
```

</details>

<details>
<summary>class DebOCIEFILVMHardwareManager</summary>

Main hardware manager class implementing the `deb_oci_efi_lvm` deploy step.
Orchestrates the full deployment workflow.

```python
class DebOCIEFILVMHardwareManager(hardware.HardwareManager):
    """Hardware manager for OCI EFI LVM RAID deployment."""

    HARDWARE_MANAGER_NAME = "DebOCIEFILVMHardwareManager"
    HARDWARE_MANAGER_VERSION = "1.0"

    def evaluate_hardware_support(self):
        LOG.info("DebOCIEFILVMHardwareManager: " "evaluate_hardware_support called")
        return hardware.HardwareSupport.SERVICE_PROVIDER

    def get_deploy_steps(self, node, ports):
        LOG.info("DebOCIEFILVMHardwareManager: get_deploy_steps called")

        return [
            {
                "step": "deb_oci_efi_lvm",
                "priority": 0,
                "interface": "deploy",
                "reboot_requested": False,
                "argsinfo": {},
            },
        ]

    def deb_oci_efi_lvm(self, node, ports):
        """Deploy Debian-based OCI image with EFI, LVM, and optional RAID.

        :param node: Node dictionary containing deployment configuration
        :param ports: List of port dictionaries for the node
        :raises: ValueError if configuration is invalid
        :raises: RuntimeError if deployment fails
        """
        LOG.info("DebOCIEFILVMHardwareManager: " "deb_oci_efi_lvm called")
        LOG.info("DebOCIEFILVMHardwareManager: node: %s", node)
        LOG.info("DebOCIEFILVMHardwareManager: ports: %s", ports)

        if not is_efi_system():
            raise RuntimeError(
                "This deployment requires EFI boot mode. "
                "System is not booted in EFI mode."
            )

        try:
            # Extract configuration from node
            configdrive_data = get_configdrive_data(node)
            root_device_hints = get_root_device_hints(node, configdrive_data)
            resolved_devices = resolve_root_devices(root_device_hints)
            meta_data = configdrive_data.get("meta_data", {})
            metal3_name = meta_data.get("metal3-name")

            root_device_path = resolved_devices[0]
            second_device = resolved_devices[1] if len(resolved_devices) &gt; 1 else None

            LOG.info(
                "DebOCIEFILVMHardwareManager: " "root_device_path: %s", root_device_path
            )
            if second_device:
                LOG.info(
                    "DebOCIEFILVMHardwareManager: " "second_device: %s (RAID1)",
                    second_device,
                )

            # Get OCI image and architecture-specific configuration
            oci_image = get_oci_image(node, configdrive_data)
            arch_config = get_architecture_config(oci_image)
            LOG.info(
                "DebOCIEFILVMHardwareManager: " "architecture config: %s", arch_config
            )

            # Get disk wipe mode
            is_raid_setup = second_device is not None
            wipe_mode = get_disk_wipe_mode(configdrive_data, is_raid_setup)

            # Clean devices based on wipe mode
            if wipe_mode == "all":
                LOG.info("Cleaning all block devices (wipe_mode: all)")
                clean_all_devices()
                wait_for_device(root_device_path)
                if second_device:
                    wait_for_device(second_device)
            else:  # wipe_mode == 'target'
                LOG.info("Cleaning only target device(s) (wipe_mode: target)")
                wait_for_device(root_device_path)
                clean_device(root_device_path)
                if second_device:
                    wait_for_device(second_device)
                    clean_device(second_device)

            # Partition disk
            is_raid, pv_device = partition_disk(
                root_device_path,
                VG_NAME,
                LV_NAME,
                second_device=second_device,
                raid_device=RAID_DEVICE,
                homehost=metal3_name,
            )

            # Get partition paths
            efi_partition = get_partition_path(root_device_path, 1)
            second_efi_partition = None
            if is_raid and second_device:
                second_efi_partition = get_partition_path(second_device, 1)

            root_lv_path = f"/dev/{VG_NAME}/{LV_NAME}"

            # Create filesystems
            create_filesystems(
                efi_partition,
                root_lv_path,
                boot_label=BOOT_FS_LABEL,
                root_label=ROOT_FS_LABEL,
                second_efi_partition=second_efi_partition,
                boot_label2=BOOT_FS_LABEL2,
            )

            # Mount root filesystem
            root_mount = tempfile.mkdtemp()
            run_command(["mount", root_lv_path, root_mount])

            try:
                # Extract OCI image rootfs
                extract_oci_image(
                    arch_config["oci_image"], arch_config["oci_platform"], root_mount
                )

                # Mount EFI partition
                efi_mount = os.path.join(root_mount, "boot", "efi")
                os.makedirs(efi_mount, exist_ok=True)
                run_command(["mount", efi_partition, efi_mount])

                try:
                    # Set up chroot
                    setup_chroot(root_mount)

                    try:
                        # Install packages
                        install_packages(root_mount, arch_config["grub_packages"])

                        # Configure cloud-init
                        configure_cloud_init(root_mount, configdrive_data)

                        # Write /etc/hosts
                        write_hosts_file(root_mount, metal3_name)

                        # Write fstab
                        write_fstab(
                            root_mount,
                            ROOT_FS_LABEL,
                            BOOT_FS_LABEL,
                            is_raid,
                            BOOT_FS_LABEL2,
                        )

                        # Configure GRUB
                        setup_grub_defaults(root_mount, ROOT_FS_LABEL, is_raid)

                        # RAID-specific configuration
                        if is_raid:
                            write_mdadm_conf(root_mount)
                            setup_grub_efi_sync(root_mount, BOOT_FS_LABEL2)

                            efi2_mount = os.path.join(root_mount, "boot", "efi2")
                            os.makedirs(efi2_mount, exist_ok=True)

                        # Install GRUB to EFI
                        run_command(
                            [
                                "chroot",
                                root_mount,
                                "grub-install",
                                f'--target={arch_config["uefi_target"]}',
                                "--efi-directory=/boot/efi",
                                "--bootloader-id=ubuntu",
                                "--recheck",
                            ]
                        )

                        # Configure initramfs for LVM (required for Debian)
                        configure_initramfs(root_mount, is_raid)

                        # Update GRUB config and initramfs
                        run_command(["chroot", root_mount, "update-grub"])
                        run_command(
                            [
                                "chroot",
                                root_mount,
                                "update-initramfs",
                                "-u",
                                "-k",
                                "all",
                            ]
                        )

                        # Install GRUB to second EFI partition for RAID
                        if is_raid and second_efi_partition:
                            efi2_mount = os.path.join(root_mount, "boot", "efi2")
                            try:
                                run_command(["mount", second_efi_partition, efi2_mount])
                                run_command(
                                    [
                                        "rsync",
                                        "-a",
                                        f"{root_mount}/boot/efi/",
                                        f"{root_mount}/boot/efi2/",
                                    ]
                                )
                                run_command(
                                    [
                                        "chroot",
                                        root_mount,
                                        "grub-install",
                                        f'--target={arch_config["uefi_target"]}',
                                        "--efi-directory=/boot/efi2",
                                        "--bootloader-id=ubuntu",
                                        "--recheck",
                                    ]
                                )
                            except Exception as e:
                                LOG.warning(
                                    "Error installing GRUB to second EFI: %s", e
                                )
                            finally:
                                result = run_command(
                                    ["mountpoint", "-q", efi2_mount], check=False
                                )
                                if result.returncode == 0:
                                    run_command(["umount", "-l", efi2_mount])

                    finally:
                        teardown_chroot(root_mount)

                finally:
                    # Unmount EFI partition
                    result = run_command(["mountpoint", "-q", efi_mount], check=False)
                    if result.returncode == 0:
                        run_command(["umount", "-l", efi_mount])

            finally:
                # Unmount root filesystem
                result = run_command(["mountpoint", "-q", root_mount], check=False)
                if result.returncode == 0:
                    run_command(["umount", "-l", root_mount])

                # Clean up temporary directories
                if root_mount and os.path.exists(root_mount):
                    try:
                        os.rmdir(root_mount)
                        LOG.debug("Cleaned up root mount directory: %s", root_mount)
                    except Exception as e:
                        LOG.warning(
                            "Failed to clean up root mount dir %s: %s", root_mount, e
                        )

            LOG.info(
                "DebOCIEFILVMHardwareManager: " "deb_oci_efi_lvm completed successfully"
            )

        except Exception as e:
            LOG.error("DebOCIEFILVMHardwareManager: " "deb_oci_efi_lvm failed: %s", e)
            raise

        finally:
            # Wait for interactive users to logout
            if has_interactive_users():
                LOG.info(
                    "DebOCIEFILVMHardwareManager: "
                    "interactive users detected, waiting for logout"
                )
                while has_interactive_users():
                    LOG.info(
                        "DebOCIEFILVMHardwareManager: "
                        "users still logged in, checking again "
                        "in 60 seconds"
                    )
                    time.sleep(60)
                LOG.info(
                    "DebOCIEFILVMHardwareManager: " "all interactive users logged out"
                )
```

</details>

<!-- markdownlint-enable MD033 -->

## Supported OCI Images

The hardware manager works with any Debian-based OCI image that has a
functional `apt` package manager. OCI multi-arch images are supported.
Tested images include:

- `ubuntu:24.04`
- `debian:13`

The key benefit of this approach is the ability to create custom OCI
images with your specific OS configuration, packages, and settings.
You can build and maintain your own Docker images and use them directly
as the root filesystem for bare metal deployments. The deployment process
installs additional packages (kernel, GRUB, cloud-init) on top of the
base image.

## Debugging Deployments

If a deployment fails, you can connect to the server via BMC console
during the IPA phase. The hardware manager includes a feature that
waits for interactive users to log out before completing, allowing
you to inspect the system state.

## Limitations and Considerations

The following are limitations of this specific `deb_oci_efi_lvm`
implementation, not of Metal3's custom deploy mechanism itself. The
custom deploy framework is flexible and allows implementing alternative
hardware managers with different capabilities.

1. **EFI only** - This implementation requires UEFI boot mode
1. **Debian-based only** - The package installation assumes `apt` is
   available
1. **Network required** - The IPA needs network access to pull OCI
   images from registries and install packages in target system
1. **Root device hints** - Only `serial` and `wwn` hints are supported
   for disk selection

## Conclusion

The `deb_oci_efi_lvm` hardware manager demonstrates how custom deploy
steps can extend Ironic's capabilities beyond traditional image-based
deployments. The source code and GitHub Actions for building custom IPA
images are available at
[s3rj1k/ironic-python-agent](https://github.com/s3rj1k/ironic-python-agent/tree/custom_deploy).

## Future Improvements

A potential enhancement could add native support for converting OpenStack
`network_data.json` format to cloud-init v1 network configuration during
deployment.

## References

- [Integrating CoreOS Installer with Ironic](https://owlet.today/posts/integrating-coreos-installer-with-ironic/) -
  Dmitry Tantsur's original blog post on custom deploy steps
- [Ironic Deploy Steps Documentation](https://docs.openstack.org/ironic/latest/contributor/deploy-steps.html)
- [Metal3 Custom Deploy Steps Design](https://github.com/metal3-io/metal3-docs/blob/main/design/baremetal-operator/deploy-steps.md)
- [OpenShift CoreOS Install Hardware Manager](https://github.com/openshift/ironic-agent-image/blob/main/hardware_manager/ironic_coreos_install.py)
</system></details></or></in></details>]]></content><author><name>Serhii Ivanov</name></author><category term="metal3" /><category term="ironic" /><category term="IPA" /><category term="OCI" /><category term="deployment" /><category term="bare metal" /><summary type="html"><![CDATA[What if you could deploy any OCI container image directly to bare metal, without building traditional disk images? Back in 2021, Dmitry Tantsur implemented custom deploy steps for Ironic, enabling alternative deployment methods beyond the standard image-based approach. This feature powers OpenShift’s bare metal provisioning with CoreOS, yet it remains surprisingly unknown to the broader Metal3 community. This post aims to change that by providing an example implementation of a custom IPA hardware manager that deploys Debian-based container images with EFI boot, LVM root filesystem, and optional RAID1 mirroring.]]></summary></entry><entry><title type="html">Metal3.io Becomes a CNCF Incubating Project</title><link href="https://metal3.io/blog/2025/08/27/metal3-becomes-cncf-incubating-project.html" rel="alternate" type="text/html" title="Metal3.io Becomes a CNCF Incubating Project" /><published>2025-08-27T00:00:00-05:00</published><updated>2025-08-27T00:00:00-05:00</updated><id>https://metal3.io/blog/2025/08/27/metal3-becomes-cncf-incubating-project</id><content type="html" xml:base="https://metal3.io/blog/2025/08/27/metal3-becomes-cncf-incubating-project.html"><![CDATA[<p>We are pleased to share some incredible news with our community!  The CNCF
<a href="https://www.cncf.io/people/technical-oversight-committee/">Technical Oversight Committee</a> has officially voted to accept Metal3 as an
incubating project. This milestone represents years of hard work, collaboration,
and innovation, and we couldn’t be more excited about what lies ahead!</p>

<h2 id="our-journey-from-sandbox-to-incubation">Our Journey from Sandbox to Incubation</h2>

<p>What started as a collaboration between Red Hat and Ericsson in 2019 has
blossomed into something truly special. When we joined the CNCF sandbox in
September 2020, we knew we had something powerful: a way to make bare metal
infrastructure as Kubernetes-native as any cloud platform. Today, that vision
has grown far beyond our initial dreams.</p>

<h2 id="the-numbers-tell-our-story">The Numbers Tell Our Story</h2>

<p>We’re incredibly proud of what our community has accomplished together:</p>

<ul>
  <li><strong>57 active contributing organizations</strong> from around the globe</li>
  <li><strong>186 amazing contributors</strong> who’ve shaped our project</li>
  <li><strong>8,368 merged pull requests</strong> representing countless hours of collaboration</li>
  <li><strong>1,523 GitHub stars</strong> from supporters worldwide</li>
  <li><strong>187 releases</strong> of continuous improvement</li>
</ul>

<p>But beyond the numbers, what makes us truly happy is seeing organizations like
Fujitsu, Ikea, SUSE, Ericsson, and Red Hat successfully deploying Metal3 in
production environments.</p>

<h2 id="what-makes-us-proud">What Makes Us Proud</h2>

<p>Metal3 has evolved into so much more than a bare metal provisioning tool. We’ve
built a comprehensive platform that:</p>

<ul>
  <li>Seamlessly integrates with Cluster API for Kubernetes lifecycle management</li>
  <li>Provides robust IP address management through our IPAM component</li>
  <li>Offers enterprise-grade security with automated vulnerability scanning</li>
  <li>Supports firmware management and day-2 operations</li>
  <li>Runs entirely on Kubernetes using native APIs</li>
</ul>

<p>Our new Ironic Standalone Operator has revolutionized deployment simplicity,
making it easier than ever for teams to get started with Metal3.</p>

<h2 id="looking-forward-with-excitement">Looking Forward with Excitement</h2>

<p>The roadmap ahead fills us with anticipation! In 2025, we’re planning:</p>

<ul>
  <li>Enhanced multi-tenancy support</li>
  <li>ARM architecture support beyond x86_64</li>
  <li>Improved DHCP-less provisioning capabilities</li>
  <li>New API revisions across our components</li>
  <li>Continued simplification of the user experience</li>
</ul>

<h2 id="the-adventure-continues">The Adventure Continues</h2>

<p>Joining CNCF incubation isn’t the end of our journey – it’s an exciting new
chapter! With the foundation’s support and our amazing community behind us,
we’re more energized than ever to push the boundaries of what’s possible with
bare metal Kubernetes infrastructure.</p>

<p><strong>Thank you</strong> for being part of this incredible adventure. Here’s to making bare
metal as cloud-native as the clouds themselves!</p>]]></content><author><name>Honza Pokorný</name></author><category term="metal3" /><category term="cncf" /><category term="community" /><category term="announcement" /><summary type="html"><![CDATA[We are pleased to share some incredible news with our community! The CNCF Technical Oversight Committee has officially voted to accept Metal3 as an incubating project. This milestone represents years of hard work, collaboration, and innovation, and we couldn’t be more excited about what lies ahead!]]></summary></entry><entry><title type="html">Introducing Baremetal Operator end-to-end test suite</title><link href="https://metal3.io/blog/2024/12/13/Introducing-BMO-E2E.html" rel="alternate" type="text/html" title="Introducing Baremetal Operator end-to-end test suite" /><published>2024-12-13T00:00:00-06:00</published><updated>2024-12-13T00:00:00-06:00</updated><id>https://metal3.io/blog/2024/12/13/Introducing-BMO-E2E</id><content type="html" xml:base="https://metal3.io/blog/2024/12/13/Introducing-BMO-E2E.html"><![CDATA[<p>In the beginning, there was
<a href="https://github.com/metal3-io/metal3-dev-env">metal3-dev-env</a>. It could set up a
virtualized “baremetal” lab and test all the components together. As Metal3
matured, it grew in complexity and capabilities, with release branches, API
versions, etc. Metal3-dev-env did everything from cloning the repositories and
building the container images, to deploying the controllers and running tests,
on top of setting up the virtual machines and the networks, of course. Needless
to say, it became hard to understand and easy to misuse.</p>

<p>We tried reducing the scope a bit by introducing end to end tests <a href="https://github.com/metal3-io/cluster-api-provider-metal3/tree/main/test">directly in
the Cluster API provider
Metal3</a>
(CAPM3). However, metal3-dev-env was still very much entangled with CAPM3. It
was at this point that I got tired of trying to gradually fix it and took the
initiative to start from scratch with end to end tests in <a href="https://github.com/metal3-io/baremetal-operator">Baremetal Operator
(BMO)</a> instead.</p>

<p>Up until that point, we had been testing BMO through CAPM3 and the cluster API
flow. It worked, but it was very inefficient. From the perspective on the
Baremetal Operator, a test could look something like this:</p>

<ol>
  <li>Register 5 BareMetalHosts</li>
  <li>Inspect the 5 BareMetalHosts</li>
  <li>Provision the 5 BareMetalHosts all with the same image</li>
  <li>Deprovision 1 BareMetalHost</li>
  <li>Provision it again with another image</li>
  <li>Deprovision another BareMetalHost</li>
  <li>Provision it again with the other image</li>
  <li>Continue in the same way with the rest of the BareMetalHosts…</li>
  <li>Deprovision all BareMetalHosts</li>
</ol>

<p>As you can see, it is very repetitive, constantly doing the same thing again and
again. As a consequence of this and the complexity of metal3-dev-env, it was
quite an effort to thoroughly test something related to BMO code. I was
constantly questioning myself and the test environment. “Is it testing the code
I wrote?” “Is it doing the relevant scenario?” “Is the configuration correct?”</p>

<h2 id="baremetal-operator-end-to-end-tests-are-born">Baremetal Operator end to end tests are born</h2>

<p>Sometimes it is easier to start from scratch, so <a href="https://github.com/metal3-io/baremetal-operator/pull/1303">this is what we
did</a>. The Baremetal
Operator end to end tests started out as a small script that only set up
minikube, some VMs and a baseboard management controller (BMC) emulator. The
goal was simple: do the minimum required to simulate a baremetal lab. From this,
it was quite easy to build a test module that was responsible for deploying the
necessary controllers and running some tests.</p>

<p>Notice the separation of concerns here! The test module expects a baremetal lab
environment to be already existing and the script that sets up the environment
is not involved in anyway with the tests or deployment of the controllers. This
design is deliberate, with a clear goal that the test module should be useful
across multiple environments. It should be possible to run the test suite
against real baremetal labs with multiple different configurations. I am hoping
that we will get a chance next year to try it for real in a baremetal lab.</p>

<h2 id="how-does-it-work">How does it work?</h2>

<p>The flexibility of the end to end module is possible through a configuration
file. It can be used to configure everything from the image URL and checksum to
the timeout limits. Since Ironic can be deployed in many different ways, it was
also necessary to make this flexible. The user can optionally set up Ironic
before the test, or provide a kustomization that will be applied automatically.
A separate configuration file declares the BMCs that should be used in the
tests.</p>

<p>The <a href="https://github.com/metal3-io/baremetal-operator/tree/main/test/e2e/config">configuration that we use in
CI</a>
shows how these files look like. As a proof of concept for the flexibility of
the tests, it can be noted that we already have two different configurations.
One for running the tests with Ironic and one for running them with BMO in
fixture mode. The first is the “normal” mode, the latter means that BMO does not
communicate with Ironic at all, it just pretends. While that obviously isn’t
useful for any thorough tests, it still provides a quick and light weight test
suite, and ensures that we do not get too attached to one particular
configuration.</p>

<p>The test suite itself is made with Ginkgo and Gomega. Instead of building a long
chain of checks and scenarios we have attempted to do small, isolated tests.
This makes it possible to run multiple in parallel and shorten the test suite
duration, as well as easily identify where exactly errors occur. In order to
accomplish this, we make heavy use of the <a href="https://book.metal3.io/bmo/status_annotation">status
annotation</a> so that we can skip
inspection when possible.</p>

<h2 id="where-are-we-today">Where are we today?</h2>

<p>It is already several months since we switched over to the BMO e2e test suite as
the primary, and only required tests for pull requests in the BMO repository. We
run the <a href="https://github.com/metal3-io/baremetal-operator/blob/main/.github/workflows/e2e-test.yml">end to end test suite as GitHub
workflows</a>
and it covers more than the metal3-dev-env and CAPM3 based tests from BMO
perspective. That does not mean that we are done though. At the time of writing,
there are <a href="https://github.com/orgs/metal3-io/projects/5/views/2">several GitHub
issues</a> for improving and
extending the tests. The progress has significantly slowed though, as can
perhaps be expected, since the most essentials parts were implemented.</p>

<h2 id="the-future">The future</h2>

<p>In the future we hope to make the BMO end to end module and tooling more useful
for local development and testing. It should be easy to spin up a minimal
environment and test specific scenarios, also using Tilt. Additionally, we want
to “rebase” the CAPM3 end to end tests on this work. It should be possible to
reuse the code and tooling for simulating a baremetal lab so that we can get rid
of the entanglement with metal3-dev-env.</p>]]></content><author><name>Lennart Jern</name></author><category term="metal3" /><category term="cluster API" /><category term="provider" /><category term="edge" /><summary type="html"><![CDATA[In the beginning, there was metal3-dev-env. It could set up a virtualized “baremetal” lab and test all the components together. As Metal3 matured, it grew in complexity and capabilities, with release branches, API versions, etc. Metal3-dev-env did everything from cloning the repositories and building the container images, to deploying the controllers and running tests, on top of setting up the virtual machines and the networks, of course. Needless to say, it became hard to understand and easy to misuse.]]></summary></entry><entry><title type="html">Scaling Kubernetes with Metal3: Simulating 1000 Clusters with Fake Ironic Agents</title><link href="https://metal3.io/blog/2024/10/24/Scaling-Kubernetes-with-Metal3-on-Fake-Node.html" rel="alternate" type="text/html" title="Scaling Kubernetes with Metal3: Simulating 1000 Clusters with Fake Ironic Agents" /><published>2024-10-24T00:00:00-05:00</published><updated>2024-10-24T00:00:00-05:00</updated><id>https://metal3.io/blog/2024/10/24/Scaling-Kubernetes-with-Metal3-on-Fake-Node</id><content type="html" xml:base="https://metal3.io/blog/2024/10/24/Scaling-Kubernetes-with-Metal3-on-Fake-Node.html"><![CDATA[<p>If you’ve ever tried scaling out Kubernetes clusters in a bare-metal
environment, you’ll know that large-scale testing comes with serious challenges.
Most of us don’t have access to enough physical servers—or even virtual
machines—to simulate the kinds of large-scale environments we need for stress
testing, especially when deploying hundreds or thousands of clusters.</p>

<p>That’s where this experiment comes in.</p>

<p>Using Metal3, we simulated a massive environment—provisioning 1000 single-node
Kubernetes clusters—without any actual hardware. The trick? A combination of
Fake Ironic Python Agents (IPA) and Fake Kubernetes API servers. These tools
allowed us to run an entirely realistic Metal3 provisioning workflow while
simulating thousands of nodes and clusters, all without needing a single real
machine.</p>

<p>The motivation behind this was simple: to create a scalable testing environment
that lets us validate Metal3’s performance, workflow, and reliability without
needing an expensive hardware lab or virtual machine fleet. By simulating nodes
and clusters, we could push the limits of Metal3’s provisioning process
cost-effectively and time-efficiently.</p>

<p>In this post, I’ll explain exactly how it all works, from setting up multiple
Ironic services to faking hardware nodes and clusters and sharing the lessons
learned. Whether you’re a Metal3 user or just curious about how to test
large-scale Kubernetes environments, it’ll surely be a good read. Let’s get
started!</p>

<h2 id="prerequisites--setup">Prerequisites &amp; Setup</h2>

<p>Before diving into the fun stuff, let’s ensure we’re on the same page. You don’t
need to be a Metal3 expert to follow along, but having a bit of background will
help!</p>

<h3 id="what-youll-need-to-know">What You’ll Need to Know</h3>

<p>Let’s start by ensuring you’re familiar with some essential tools and concepts
that power Metal3 workflow. If you’re confident in your Metal3 skills, please
feel free to skip this part.</p>

<h4 id="a-typical-metal3-workflow">A typical Metal3 Workflow</h4>

<p>The following diagram explains a typical Metal3 workflow. We will, then, go into
details of every component.</p>

<p><img src="/assets/2024-10-24-Scaling-Kubernetes-with-Metal3-on-Fake-Node/metal3-typical-workflow.jpg" alt="Metal3 Typical
Workflow" /></p>

<h4 id="cluster-api-capi">Cluster API (CAPI)</h4>

<p>CAPI is a project that simplifies the deployment and management of Kubernetes
clusters. It provides a consistent way to create, update, and scale clusters
through Kubernetes-native APIs. The magic of CAPI is that it abstracts away many
of the underlying details so that you can manage clusters on different platforms
(cloud, bare metal, etc.) in a unified way.</p>

<h4 id="cluster-api-provider-metal3-capm3">Cluster API Provider Metal3 (CAPM3)</h4>

<p>CAPM3 extends CAPI to work specifically with Metal3 environments. It connects
the dots between CAPI, BMO, and Ironic, allowing Kubernetes clusters to be
deployed on bare-metal infrastructure. It handles tasks like provisioning new
nodes, registering them with Kubernetes, and scaling clusters.</p>

<h4 id="bare-metal-operator-bmo">Bare Metal Operator (BMO)</h4>

<p>BMO is a controller that runs inside a Kubernetes cluster and works alongside
Ironic to manage bare-metal infrastructure. It automates the lifecycle of
bare-metal hosts, managing things like registering new hosts, powering them on
or off, and monitoring their status.</p>

<h5 id="bare-metal-host-bmh">Bare Metal Host (BMH)</h5>

<p>A BMH is the Kubernetes representation of a bare-metal node. It contains
information about how to reach the node it represents, and BMO monitors its
desired state closely. When BMO notices that a BMH object state is requested to
change (either by a human user or CAPM3), it will decide what needs to be done
and tell Ironic.</p>

<h4 id="ironic--ironic-python-agent-ipa">Ironic &amp; Ironic Python Agent (IPA)</h4>

<ul>
  <li>Ironic is a bare-metal provisioning tool that handles tasks like booting
servers, deploying bootable media (e.g., operating systems) to disk, and
configuring hardware. Think of Ironic as the piece of software that manages
actual physical servers. In a Metal3 workflow, Ironic receives orders from BMO
and translates them into actionable steps. Ironic has multiple ways to interact
with the machines, and one of them is the so-called “ agent-based direct deploy”
method, which is commonly used by BMO. The agent mentioned is called <strong>Ironic
Python Agent</strong> (IPA), which is a piece of software that runs on each bare-metal
node and carries out Ironic’s instructions. It interacts with the hardware
directly, like wiping disks, configuring networks, and handling boot processes.</li>
</ul>

<p>In a typical Metal3 workflow, BMO reads the desired state of the node from the
BMH object, translates the Kubernetes reconciling logic to concrete actions, and
forwards them to Ironic, which, as part of the provisioning process, tells IPA
the exact steps it needs to perform to get the nodes to desired states. During
the first boot after node image installation, Kubernetes components will be
installed on the nodes by cloud-init, and once the process succeeds, Ironic
and IPA finish the provisioning process, and CAPI and CAPM3 will verify the
health of the newly provisioned Kubernetes cluster(s).</p>

<h2 id="the-experiment-simulating-1000-kubernetes-clusters">The Experiment: Simulating 1000 Kubernetes Clusters</h2>

<p>This experiment aimed to push Metal3 to simulate 1000 single-node Kubernetes
clusters on fake hardware. Instead of provisioning real machines, we used Fake
Ironic Python Agents (Fake IP) and Fake Kubernetes API Servers (FKAS) to
simulate nodes and control planes, respectively. This setup allowed us to test a
massive environment without the need for physical infrastructure.</p>

<p>Since our goal is to verify the Metal3 limit, our setup will let all the Metal3
components (except for IPA, which runs inside and will be scaled with the nodes)
to keep working as they do in a typical workflow. In fact, none of the
components should be aware that they are running with fake hardware.</p>

<p>Take the figure we had earlier as a base, here is the revised workflow with fake
nodes.</p>

<p><img src="/assets/2024-10-24-Scaling-Kubernetes-with-Metal3-on-Fake-Node/metal3-simulation-workflow.jpg" alt="Metal3 Simulation
Workflow" /></p>

<h3 id="step-1-setting-up-the-environment">Step 1: Setting Up the environment</h3>

<p>As you may have known, a typical Metal3 workflow requires several components:
bootstrap Kubernetes cluster, possible external networks, bare-metal nodes, etc.
As we are working on simulating the environment, we will start with a newly
spawned Ubuntu VM, create a cluster with minikube, add networks with libvirt,
and so on (If you’re familiar with Metal3’s dev-env, this step is similar to
what script
<a href="https://github.com/metal3-io/metal3-dev-env/blob/main/01_prepare_host.sh">01</a>,
<a href="https://github.com/metal3-io/metal3-dev-env/blob/main/02_configure_host.sh">02</a>
and a part of
<a href="https://github.com/metal3-io/metal3-dev-env/blob/main/03_launch_mgmt_cluster.sh">03</a>
do). We will not discuss this part, but you can find the related setup from
<a href="https://github.com/Nordix/metal3-clusterapi-docs/blob/main/Support/Multitenancy/Scalability-with-Fake-Nodes/vm-setup.sh">this
script</a>
if interested.</p>

<p><strong>Note</strong>: If you intend to follow along, note that going to 1000 nodes requires
a large environment and will take a long time. In our setup, we had a VM with 24
cores and 32GB of RAM, of which we assigned 14 cores and 20GB of RAM to the
minikube VM, and the process took roughly 48 hours. If your environment is less
powerful, consider reducing the nodes you want to provision. Something like 100
nodes will require minimal resources and time while still being impressive.</p>

<h3 id="step-2-install-bmo-and-ironic">Step 2: Install BMO and Ironic</h3>

<p>In Metal3’s typical workflow, we usually rely on Kustomize to install Ironic and
BMO. Kustomize helps us define configurations for Kubernetes resources, making
it easier to customize and deploy services. However, our current Kustomize
overlay for Metal3 configures only a single Ironic instance. This setup works
well for smaller environments, but it becomes a bottleneck when scaling up and
handling thousands of nodes.</p>

<p>That’s where Ironic’s <strong>special mode</strong> comes into play. Ironic has the ability
to run <strong>multiple Ironic conductors</strong> while sharing the same database. The best
part? Workload balancing between conductors happens automatically, which means
that no matter which Ironic conductor receives a request, the load is evenly
distributed across all conductors, ensuring efficient provisioning. Achieving
this requires separating <strong>ironic conductor</strong> from the database, which allows us
to scale up the conductor part. Each <strong>conductor</strong> will have its own
<code class="language-plaintext highlighter-rouge">PROVISIONING_IP</code>, hence the need to have a specialized <code class="language-plaintext highlighter-rouge">configMap.</code></p>

<p>We used <a href="https://helm.sh/">Helm</a> for this purpose. In our Helm chart, the
<strong>Ironic conductor</strong> container and <strong>HTTP server (httpd)</strong> container are
separated into a new pod, and the rest of the ironic package (mostly
MariaDB-ironic database) stays in another pod. A list of PROVISIONING_IPs is
provided by the chart’s <code class="language-plaintext highlighter-rouge">values.yaml</code>, and for each IP, an  <strong>ironic conductor</strong>
pod is created, along with a config map whose values are rendered with the IP’s
value. This way, we can dynamically scale up/down ironic (or, more specifically,
<strong>ironic conductors</strong>) by simply adding/removing ips.</p>

<p>Another piece of information that we need to keep in mind is the ipa-downloader
container. In our current metal3-dev-env, the IPA-downloader container runs as
an init Container for ironic, and its job is to download the IPA image to a
Persistent Volume. This image contains the <strong>Ironic Python Agent</strong>, and it is
assumed to exist by Ironic. For the multiple-conductor scenario, running the
same init-container for all the conductors, at the same time, could be slow
and/or fail due to network issue. To make it work, we made a small “hack” in the
chart: the ipa image will exist in a specific location inside the minikube host,
and all the conductor pods will mount to that same location. In production, a
more throughout solution might be to keep the IPA-downloader as an
init-container, but points the image to the local image server, which we set up
in the previous step.</p>

<p>BMO, on the other hand, still works well with kustomize, as we do not need to
scale it. As with typical metal3 workflow, BMO and Ironic must share some
authentication to work with TLS.</p>

<p>You can check out the full Ironic helm chart
<a href="https://github.com/Nordix/metal3-clusterapi-docs/tree/main/Support/Multitenancy/Scalability-with-Fake-Nodes/ironic">here</a>.</p>

<h3 id="step-3-creating-fake-nodes-with-fake-ironic-python-agents">Step 3: Creating Fake Nodes with Fake Ironic Python Agents</h3>

<p>As we mentioned at the beginning, instead of using real hardware, we will use a
new tool called <strong>Fake Ironic Python Agent</strong>, or <strong>Fake IPA</strong> to simulate the
nodes.</p>

<p>Setting up <strong>Fake IPA</strong> is relatively straightforward, as <strong>Fake IPA</strong> runs as
containers on the host machine, but first, we need to create the list of “nodes”
that we will use (Fake IPA requires to have that list ready when it starts). A
“node” typically looks like this</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="o">{</span>
      <span class="s2">"uuid"</span>: <span class="nv">$uuid</span>,
      <span class="s2">"name"</span>: <span class="nv">$node_name</span>,
      <span class="s2">"power_state"</span>: <span class="s2">"Off"</span>,
      <span class="s2">"external_notifier"</span>: <span class="s2">"True"</span>,
      <span class="s2">"nics"</span>: <span class="o">[</span>
        <span class="o">{</span><span class="s2">"mac"</span>: <span class="nv">$macaddr</span>, <span class="s2">"ip"</span>: <span class="s2">"192.168.0.100"</span><span class="o">}</span>
      <span class="o">]</span>,
<span class="o">}</span>
</code></pre></div></div>

<p>All of the variables (<code class="language-plaintext highlighter-rouge">uuid</code>, <code class="language-plaintext highlighter-rouge">node_name</code>, <code class="language-plaintext highlighter-rouge">macaddress</code>) can be dynamically
generated in any way you want (check <a href="https://github.com/Nordix/metal3-clusterapi-docs/blob/main/Support/Multitenancy/Scalability-with-Fake-Nodes/generate_unique_nodes.sh">this
script</a>
out if you need an idea). Still, we must store this information to generate the
BMH objects that match those “nodes.” The <code class="language-plaintext highlighter-rouge">ip</code> is, on the other hand, not
essential. It could be anything.</p>

<p>We must also start up the <strong>sushy-tools</strong> container in this step. It is a tool
that simulates the <a href="https://www.techtarget.com/searchnetworking/definition/baseboard-management-controller">Baseboard Management
Controller</a>
for non-bare-metal hardware, and we have been using it extensively inside Metal3
dev-env and CI to control and provision VMs as if they are bare-metal nodes. In
a bare-metal setup, Ironic will ask the BMC to install IPA on the node, and in
our setup, <strong>sushy-tools</strong> will get the same request, but it will simply fake
the installation and, in the end, forward <strong>Ironic</strong> traffic to the <strong>Fake IPA</strong>
container.</p>

<p>Another piece of information we will need is the cert that <strong>Ironic</strong> will use
in its communication with <strong>IPA</strong>. IPA is supposed to get it from Ironic, but as
<strong>Fake IPA</strong> cannot do that (at least not yet), we must get the cert and provide
it in <strong>Fake IPA</strong> config.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nb">mkdir </span>cert
kubectl get secret <span class="nt">-n</span> baremetal-operator-system ironic-cert <span class="nt">-o</span> json <span class="se">\</span>
  <span class="nt">-o</span><span class="o">=</span><span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.data.ca</span><span class="se">\.</span><span class="s2">crt}"</span> | <span class="nb">base64</span> <span class="nt">-d</span> <span class="o">&gt;</span>cert/ironic-ca.crt
</code></pre></div></div>

<p>Also note that one set of <strong>sushy-tools</strong> and <strong>Fake IPA</strong> containers won’t be
enough to provision 1000 nodes. Just like <strong>Ironic</strong>, they need to be scaled up
extensively (about 20-30 pairs will be sufficient for 1000 nodes), but
fortunately, the scaling is straightforward: We just need to give them different
ports. Both of these components also require a Python-based config file. For
convenience, in this setup, we create a big file and provide it to both of them,
using the following shell script:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="k">for </span>i <span class="k">in</span> <span class="si">$(</span><span class="nb">seq </span>1 <span class="s2">"</span><span class="nv">$N_SUSHY</span><span class="s2">"</span><span class="si">)</span><span class="p">;</span> <span class="k">do
  </span><span class="nv">container_conf_dir</span><span class="o">=</span><span class="s2">"</span><span class="nv">$SUSHY_CONF_DIR</span><span class="s2">/sushy-</span><span class="nv">$i</span><span class="s2">"</span>

  <span class="c"># Use round-robin to choose fake-ipa and sushy-tools containers for the node</span>
  <span class="nv">fake_ipa_port</span><span class="o">=</span><span class="k">$((</span><span class="m">9901</span> <span class="o">+</span> <span class="o">((</span><span class="nv">$i</span> <span class="o">%</span> <span class="k">${</span><span class="nv">N_FAKE_IPA</span><span class="k">:-</span><span class="nv">1</span><span class="k">}))</span><span class="o">))</span>
  <span class="nv">sushy_tools_port</span><span class="o">=</span><span class="k">$((</span><span class="m">8000</span> <span class="o">+</span> i<span class="k">))</span>
  ports+<span class="o">=(</span><span class="k">${</span><span class="nv">sushy_tools_port</span><span class="k">}</span><span class="o">)</span>

  <span class="c"># This is only so that we have the list of the needed ports for other</span>
  <span class="c"># purposes, like configuring the firewalls.</span>
  ports+<span class="o">=(</span><span class="k">${</span><span class="nv">fake_ipa_port</span><span class="k">}</span><span class="o">)</span>

  <span class="nb">mkdir</span> <span class="nt">-p</span> <span class="s2">"</span><span class="k">${</span><span class="nv">container_conf_dir</span><span class="k">}</span><span class="s2">"</span>

  <span class="c"># Generate the htpasswd file, which is required by sushy-tools</span>
  <span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="sh">'</span><span class="no">EOF</span><span class="sh">' &gt;"</span><span class="k">${</span><span class="nv">container_conf_dir</span><span class="k">}</span><span class="sh">"/htpasswd
admin:</span><span class="nv">$2b$12$/</span><span class="sh">dVOBNatORwKpF.ss99KB.vESjfyONOxyH.UgRwNyZi1Xs/W2pGVS
</span><span class="no">EOF

</span>  <span class="c"># Set configuration options</span>
  <span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> &gt;"</span><span class="k">${</span><span class="nv">container_conf_dir</span><span class="k">}</span><span class="sh">"/conf.py
import collections

SUSHY_EMULATOR_LIBVIRT_URI = "</span><span class="k">${</span><span class="nv">LIBVIRT_URI</span><span class="k">}</span><span class="sh">"
SUSHY_EMULATOR_IGNORE_BOOT_DEVICE = False
SUSHY_EMULATOR_VMEDIA_VERIFY_SSL = False
SUSHY_EMULATOR_AUTH_FILE = "/root/sushy/htpasswd"
SUSHY_EMULATOR_FAKE_DRIVER = True
SUSHY_EMULATOR_LISTEN_PORT = "</span><span class="k">${</span><span class="nv">sushy_tools_port</span><span class="k">}</span><span class="sh">"
EXTERNAL_NOTIFICATION_URL = "http://</span><span class="k">${</span><span class="nv">ADVERTISE_HOST</span><span class="k">}</span><span class="sh">:</span><span class="k">${</span><span class="nv">fake_ipa_port</span><span class="k">}</span><span class="sh">"
FAKE_IPA_API_URL = "</span><span class="k">${</span><span class="nv">API_URL</span><span class="k">}</span><span class="sh">"
FAKE_IPA_URL = "http://</span><span class="k">${</span><span class="nv">ADVERTISE_HOST</span><span class="k">}</span><span class="sh">:</span><span class="k">${</span><span class="nv">fake_ipa_port</span><span class="k">}</span><span class="sh">"
FAKE_IPA_INSPECTION_CALLBACK_URL = "</span><span class="k">${</span><span class="nv">CALLBACK_URL</span><span class="k">}</span><span class="sh">"
FAKE_IPA_ADVERTISE_ADDRESS_IP = "</span><span class="k">${</span><span class="nv">ADVERTISE_HOST</span><span class="k">}</span><span class="sh">"
FAKE_IPA_ADVERTISE_ADDRESS_PORT = "</span><span class="k">${</span><span class="nv">fake_ipa_port</span><span class="k">}</span><span class="sh">"
FAKE_IPA_CAFILE = "/root/cert/ironic-ca.crt"
SUSHY_FAKE_IPA_LISTEN_IP = "</span><span class="k">${</span><span class="nv">ADVERTISE_HOST</span><span class="k">}</span><span class="sh">"
SUSHY_FAKE_IPA_LISTEN_PORT = "</span><span class="k">${</span><span class="nv">fake_ipa_port</span><span class="k">}</span><span class="sh">"
SUSHY_EMULATOR_FAKE_IPA = True
SUSHY_EMULATOR_FAKE_SYSTEMS = </span><span class="si">$(</span><span class="nb">cat </span>nodes.json<span class="si">)</span><span class="sh">
</span><span class="no">EOF

</span>  <span class="c"># Start sushy-tools</span>
  docker run <span class="nt">-d</span> <span class="nt">--net</span> host <span class="nt">--name</span> <span class="s2">"sushy-tools-</span><span class="k">${</span><span class="nv">i</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="nt">-v</span> <span class="s2">"</span><span class="k">${</span><span class="nv">container_conf_dir</span><span class="k">}</span><span class="s2">"</span>:/root/sushy <span class="se">\</span>
    <span class="s2">"</span><span class="k">${</span><span class="nv">SUSHY_TOOLS_IMAGE</span><span class="k">}</span><span class="s2">"</span>

  <span class="c"># Start fake-ipa</span>
  docker run <span class="se">\</span>
    <span class="nt">-d</span> <span class="nt">--net</span> host <span class="nt">--name</span> fake-ipa-<span class="k">${</span><span class="nv">i</span><span class="k">}</span> <span class="se">\</span>
    <span class="nt">-v</span> <span class="s2">"</span><span class="k">${</span><span class="nv">container_conf_dir</span><span class="k">}</span><span class="s2">"</span>:/app <span class="se">\</span>
    <span class="nt">-v</span> <span class="s2">"</span><span class="si">$(</span><span class="nb">realpath </span>cert<span class="si">)</span><span class="s2">"</span>:/root/cert <span class="se">\</span>
    <span class="s2">"</span><span class="k">${</span><span class="nv">FAKEIPA_IMAGE</span><span class="k">}</span><span class="s2">"</span>
<span class="k">done</span>
</code></pre></div></div>

<p>In this setup, we made it so that all the <strong>sushy-tools</strong> containers will
listen on the port range running from 8001, 8002,…, while the <strong>Fake IPA</strong>
containers have ports 9001, 9002,…</p>

<h3 id="step-4-add-the-bmh-objects">Step 4: Add the BMH objects</h3>

<p>Now that we have <strong>sushy-tools</strong> and <strong>Fake IPA</strong> containers running, we can
already generate the manifest for BMH objects, and apply them to the cluster. A
BMH object will look like this</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Secret</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">name-bmc-secret</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">environment.metal3.io</span><span class="pi">:</span> <span class="s">baremetal</span>
<span class="na">type</span><span class="pi">:</span> <span class="s">Opaque</span>
<span class="na">data</span><span class="pi">:</span>
  <span class="na">username</span><span class="pi">:</span> <span class="s">YWRtaW4=</span>
  <span class="na">password</span><span class="pi">:</span> <span class="s">cGFzc3dvcmQ=</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">metal3.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">BareMetalHost</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">name</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">online</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">bmc</span><span class="pi">:</span>
    <span class="na">address</span><span class="pi">:</span> <span class="s">redfish+http://192.168.222.1:{port}/redfish/v1/Systems/{uuid}</span>
    <span class="na">credentialsName</span><span class="pi">:</span> <span class="s">name-bmc-secret</span>
  <span class="na">bootMACAddress</span><span class="pi">:</span> <span class="s">random_mac</span>
  <span class="na">bootMode</span><span class="pi">:</span> <span class="s">legacy</span>
</code></pre></div></div>

<p>In this manifest:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">name</code> is the node name we generated in the previous step.</li>
  <li><code class="language-plaintext highlighter-rouge">uuid</code> is the random uuid we generated for the same node.</li>
  <li><code class="language-plaintext highlighter-rouge">random_mac</code> is a random mac address for the boot. It’s NOT the same as the
NIC mac address we generated for the node.</li>
  <li><code class="language-plaintext highlighter-rouge">port</code> is the listening port on one of the <strong>sushy-tools</strong> containers we
created in the previous step. Since every <strong>sushy-tools</strong> and <strong>Fake IPA</strong>
container has information about ALL the nodes, we can decide what container to
locate the “node”. In general, it’s a good idea to spread them out, so all
containers are loaded equally.</li>
</ul>

<p>We can now run <code class="language-plaintext highlighter-rouge">kubectl apply -f</code> on one (or all of) the BMH manifests. What you
expect to see is that a BMH object is created, and its state will change from
<code class="language-plaintext highlighter-rouge">registering</code> to <code class="language-plaintext highlighter-rouge">available</code> after a while. It means <strong>ironic</strong> acknowledged
that the node is valid, in good state and ready to be provisioned.</p>

<h3 id="step-5-deploy-the-fake-nodes-to-kubernetes-clusters">Step 5: Deploy the fake nodes to kubernetes clusters</h3>

<p>Before provisioning our clusters, let’s init the process, so that we have CAPI
and CAPM3 installed</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="syntax"><code>clusterctl init <span class="nt">--infrastructure</span><span class="o">=</span>metal3
</code></pre></div></div>

<p>After a while, we should see that CAPI, CAPM3, and IPAM pods become available.</p>

<p>In a standard Metal3 workflow, after having the BMH objects in an <code class="language-plaintext highlighter-rouge">available</code>
state, we can provision new Kubernetes clusters with <code class="language-plaintext highlighter-rouge">clusterctl</code>. However, with
fake nodes, things get a tiny bit more complex. At the end of the provisioning
process, <strong>Cluster API</strong> expects that there is a new kubernetes API server
created for the new cluster, from which it will check if all nodes are up, all
the control planes have <code class="language-plaintext highlighter-rouge">apiserver</code>, <code class="language-plaintext highlighter-rouge">etcd</code>, etc. pods up and running, and so
on. It is where the <a href="https://github.com/metal3-io/cluster-api-provider-metal3/blob/main/hack/fake-apiserver/README.md"><strong>Fake Kubernetes API Server</strong>
(FKAS)</a>
comes in.</p>

<p>As the <strong>FKAS README</strong> linked above already described how it works, we won’t go
into details. We simply need to send <strong>FKAS</strong> a <code class="language-plaintext highlighter-rouge">register</code> POST request (with
the new cluster’s namespace and cluster name), and it will give us an IP and a
port, which we can plug into our cluster template and then run <code class="language-plaintext highlighter-rouge">clusterctl
generate cluster</code>.</p>

<p>Under the hood, <strong>FKAS</strong> generates unique API servers for different clusters.
Each of the fake API servers does the following jobs:</p>

<ul>
  <li>Mimicking API Calls: The Fake Kubernetes API server was set up to respond to
the essential Kubernetes API calls made during provisioning.</li>
  <li>Node Registration: When CAPM3 registered nodes, the Fake API server returned
success responses, making Metal3 believe the nodes had joined a real Kubernetes
cluster.</li>
  <li>Cluster Health and Status: The Fake API responded with “healthy” statuses,
allowing CAPI/CAPM3 to continue its workflow without interruption.</li>
  <li>Node Creation and Deletion: When CAPI queried for node status or attempted to
add/remove nodes, the Fake API server responded realistically, ensuring the
provisioning process continued smoothly.</li>
  <li>Pretending to Host Kubelet: The Fake API server also simulated kubelet
responses, which allowed CAPI/CAPM3 to interact with the fake clusters as though
they were managing actual nodes.</li>
</ul>

<p>Note that in this experiment, we provisioned every one of the 1000 fake nodes to
a single-node cluster, but it’s possible to increase the number of control
planes and worker nodes by changing the <code class="language-plaintext highlighter-rouge">--control-plane-machine-count</code> and
<code class="language-plaintext highlighter-rouge">worker-machine-count</code> parameters in the <code class="language-plaintext highlighter-rouge">clusterctl generate cluster</code> command.
However, you will need to ensure that all clusters’ total nodes do not exceed
the number of BMHs.</p>

<p>As a glance, the whole simulation looks like this:</p>

<p><img src="/assets/2024-10-24-Scaling-Kubernetes-with-Metal3-on-Fake-Node/simulation-layout.jpg" alt="Simulation
layout" /></p>

<p>It will likely take some time, but once the BMHs are all provisioned, we should
be able to verify that all, or at least, most of the clusters are in good shape:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># This will list the clusters.</span>
kubectl get clusters <span class="nt">-A</span>

<span class="c"># This will determine the clusters' readiness.</span>
kubectl get kcp <span class="nt">-A</span>
</code></pre></div></div>

<ul>
  <li>For each cluster, it’s also a good idea to perform a <a href="https://cluster-api.sigs.k8s.io/clusterctl/commands/describe-cluster.html?highlight=describe%20cluster#clusterctl-describe-cluster">clusterctl
check</a>.</li>
</ul>

<h3 id="accessing-the-fake-cluster">Accessing the fake cluster</h3>

<p>A rather interesting (but not essential for our goal) check we can perform on
the fake clusters is to try accessing them. Let’s start with fetching a
cluster’s kubeconfig:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="syntax"><code>clusterctl <span class="nt">-n</span> &lt;cluster-namespace&gt; get kubeconfig &lt;cluster-name&gt; <span class="o">&gt;</span> kubeconfig-&lt;cluster-name&gt;.yaml
</code></pre></div></div>

<p>As usual, <code class="language-plaintext highlighter-rouge">clusterctl</code> will generate a kubeconfig file, but we cannot use it
just yet. Recall that we generated the API endpoint using FKAS; the address we
have now will be a combination of a port with FKAS’s IP address, which isn’t
accessible from outside the cluster. What we should do now is:</p>

<ul>
  <li>Edit the <code class="language-plaintext highlighter-rouge">kubeconfig-&lt;cluster-name&gt;.yaml</code> so that the endpoint is in the form
<code class="language-plaintext highlighter-rouge">localhost:&lt;port&gt;</code>.</li>
  <li>Port-forward the FKAS Pod to the same port the kubeconfig has shown.</li>
</ul>

<p>And voila, now we can access the fake cluster with <code class="language-plaintext highlighter-rouge">kubectl --kubeconfig
kubeconfig-&lt;cluster-name&gt;.yaml</code>. You can inspect its state and check the
resources (nodes, pods, etc.), but we won’t be able to run any workload on it as
it’s fake.</p>

<h2 id="results">Results</h2>

<p>In this post, we have demonstrated how it is possible to “generate”
bare-metal-based Kubernetes clusters from thin air (or rather, a bunch of nodes
that do not exist). Of course, these “clusters” are not very useful. Still,
successfully provisioning them without letting any of our main components
(<strong>CAPI</strong>, <strong>CAPM3</strong>, <strong>BMO</strong>, and <strong>Ironic</strong>) know they are working with fake
hardware proves that <strong>Metal3</strong> is capable of handling a heavy workload and
provision multiple nodes/clusters.</p>

<p>If interested, you could also check (and try out) the experiment by yourself
<a href="https://github.com/Nordix/metal3-clusterapi-docs/blob/main/Support/Multitenancy/Scalability-with-Fake-Nodes/README.md">here</a>.</p>]]></content><author><name>Huy Mai</name></author><category term="metal3" /><category term="cluster API" /><category term="ironic" /><category term="baremetal" /><category term="scaling" /><summary type="html"><![CDATA[If you’ve ever tried scaling out Kubernetes clusters in a bare-metal environment, you’ll know that large-scale testing comes with serious challenges. Most of us don’t have access to enough physical servers—or even virtual machines—to simulate the kinds of large-scale environments we need for stress testing, especially when deploying hundreds or thousands of clusters.]]></summary></entry><entry><title type="html">Scaling to 1000 clusters - Part 3</title><link href="https://metal3.io/blog/2024/05/30/Scaling_part_3.html" rel="alternate" type="text/html" title="Scaling to 1000 clusters - Part 3" /><published>2024-05-30T00:00:00-05:00</published><updated>2024-05-30T00:00:00-05:00</updated><id>https://metal3.io/blog/2024/05/30/Scaling_part_3</id><content type="html" xml:base="https://metal3.io/blog/2024/05/30/Scaling_part_3.html"><![CDATA[<!-- markdownlint-disable no-space-in-emphasis -->
<p>In <a href="/blog/2023/05/05/Scaling_part_1.html">part 1</a>, we introduced the
Bare Metal Operator test mode and saw how it can be used to play with
BareMetalHosts without Ironic and without any actual hosts. We continued in
<a href="/blog/2023/05/17/Scaling_part_2.html">part 2</a> with how to fake
workload clusters enough for convincing Cluster API’s controllers that they are
healthy. These two pieces together allowed us to run scaling tests and reach our
target of 1000 single node clusters. In this final part of the blog post series,
we will take a look at the results, the issues that we encountered and the
improvements that have been made.
<!-- markdownlint-enable no-space-in-emphasis --></p>

<h2 id="issues-encountered-and-lessons-learned">Issues encountered and lessons learned</h2>

<p>As part of this work we have learned a lot. We found genuine bugs and
performance issues, but we also learned about relevant configuration options for
Cluster API and controllers in general.</p>

<p>One of the first things we hit was <a href="https://github.com/metal3-io/baremetal-operator/issues/1190">this bug in Bare Metal
Operator</a> that
caused endless requeues for some deleted objects. It was not a big deal, barely
noticeable, at small scale. However, at larger scales things like this become a
problem. The logs become unreadable as they are filled with “spam” from
requeuing deleted objects and the controller is wasting resources trying to
reconcile them.</p>

<p>As mentioned, we also learned a lot from this experiment. For example, that all
the controllers have flags for setting their concurrency, i.e. how many objects
they reconcile in parallel. The default is 10, which works well in most cases,
but for larger scales it may be necessary to tune this in order to speed up the
reconciliation process.</p>

<p>The next thing we hit was rate limits! Both
<a href="https://github.com/kubernetes/client-go/blob/02d652e007235a5b46b9972bf136f274983853e6/util/workqueue/default_rate_limiters.go#L39">client-go</a>
and
<a href="https://github.com/kubernetes-sigs/controller-runtime/blob/v0.14.5/pkg/client/config/config.go#L96">controller-runtime</a>
have default rate limits of 10 and 20 QPS (Queries Per Second) respectively that
the controllers inherit unless overridden. In general, this is a good thing, as
it prevents controllers from overloading the API server. They obviously become
an issue once you scale far enough though. For us that happened when we got to
600 clusters.</p>

<p>Why 600? The number was actually a good clue, and the reason we managed figure
out what was wrong! Let’s break it down. By default, the Cluster API controller
will reconcile objects every 10 minutes (=600 seconds) in addition to reacting
to events. Each reconciliation will normally involve one or more API calls, so
at 600 clusters, we would have at least one API call per second just from the
periodic sync. In other words, the controllers would at this point use up a
large part of their budget on periodic reconciliation and quickly reach their
limit when adding reactions to events, such as the creation of a new cluster.</p>

<p>At the time, these rate limits were not configurable in the Cluster API
controllers, so we had to patch the controllers to increase the limits. We have
since then added flags to the controllers to make this configurable. If you
found this interesting, you can read more about it in <a href="https://github.com/kubernetes-sigs/cluster-api/issues/8052">this
issue</a>.</p>

<p>With concurrency and rate limits taken care of, we managed to reach our target
of 1000 clusters in reasonable time. However, there was still a problem with
resource usage. The Kubeadm control plane controller was <a href="https://github.com/kubernetes-sigs/cluster-api/issues/8602">unreasonably CPU
hungry</a>!</p>

<p>Luckily, Cluster API has excellent <a href="https://cluster-api.sigs.k8s.io/developer/core/tilt">debugging and monitoring tools
available</a> so it was easy
to collect data and profile the controllers. A quick look at the dashboard
confirmed that the Kubeadm control plane controller was indeed the culprit, with
a CPU usage far higher than the other controllers.</p>

<p><img src="/assets/2024-05-30-Scaling_part_3/CAPI-dashboard.png" alt="CAPI monitoring
dashboard" /></p>

<p>We then collected some profiling data and found the cause of the CPU usage. It
was generating new private keys for accessing the workload cluster API server
<em>every time</em> it needed to access it. This is a CPU intensive operation, and it
happened four times per reconciliation! The flame graph seen below clearly shows
the four key generation operations, and makes it obvious that this is what takes
up most of the time spent on the CPU for the controller.</p>

<p><img src="/assets/2024-05-30-Scaling_part_3/KCP-profiling.png" alt="KCP profiling graph" /></p>

<h2 id="improvements">Improvements</h2>

<p>All issues mentioned in the previous section have been addressed. The Bare Metal
Operator is no longer re-queuing deleted objects. All controllers have flags for
setting their concurrency and rate limits, and the Kubeadm control plane
controller is now caching and reusing the private keys instead of generating new
ones every time.</p>

<p>The impact of all of this is that</p>

<ul>
  <li>the Bare Metal Operator has more readable logs and lower CPU usage,</li>
  <li>users can configure rate limits for all Cluster API and Metal3 controllers if
necessary, and</li>
  <li>the Kubeadm control plane controller has a much lower CPU usage and faster
reconciliation times.</li>
</ul>

<h2 id="results">Results</h2>

<p>When we set out, it was simply not possible to reach a scale of 1000 clusters in
a reasonable time. With the collaboration, help from maintainers and other
community members, we managed to reach our target. It is now possible to manage
thousands of workload clusters through a single Cluster API management cluster.</p>

<p>The discussions and efforts also resulted in a <a href="https://kccncna2023.sched.com/event/1R2py/cluster-api-deep-dive-improving-performance-up-to-2k-clusters-fabrizio-pandini-stefan-buringer-vmware">deep dive presentation at
KubeCon NA
2023</a>
from the Cluster API maintainers.</p>

<p>Cluster API itself now also has an <a href="https://github.com/kubernetes-sigs/cluster-api/tree/main/test/infrastructure/inmemory">in-memory
provider</a>
which makes it almost trivial to test large scale scenarios. However, it must be
noted that it can only be used to test the core, bootstrap and control plane
providers. If you want to try it out, you can use the following script. Please
note that this will still be CPU intensive, despite the improvements mentioned
above. Creating 1000 clusters is no small task!</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>kind create cluster
<span class="nb">export </span><span class="nv">CLUSTER_TOPOLOGY</span><span class="o">=</span><span class="nb">true
</span>clusterctl init <span class="nt">--core</span><span class="o">=</span>cluster-api:v1.7.2 <span class="nt">--bootstrap</span><span class="o">=</span>kubeadm:v1.7.2 <span class="nt">--control-plane</span><span class="o">=</span>kubeadm:v1.7.2 <span class="nt">--infrastructure</span><span class="o">=</span><span class="k">in</span><span class="nt">-memory</span>:v1.7.2

<span class="c"># Patch the controllers to increase the rate limits and concurrency</span>
kubectl <span class="nt">-n</span> capi-system patch deployment capi-controller-manager <span class="se">\</span>
  <span class="nt">--type</span><span class="o">=</span>json <span class="nt">-p</span><span class="o">=</span><span class="s1">'[
    {"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kube-api-qps=100"},
    {"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kube-api-burst=200"},
    {"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--cluster-concurrency=100"},
    {"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--machine-concurrency=100"}
  ]'</span>

kubectl <span class="nt">-n</span> capi-kubeadm-control-plane-system patch deployment capi-kubeadm-control-plane-controller-manager <span class="se">\</span>
  <span class="nt">--type</span><span class="o">=</span>json <span class="nt">-p</span><span class="o">=</span><span class="s1">'[
    {"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kube-api-qps=100"},
    {"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kube-api-burst=200"},
    {"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kubeadmcontrolplane-concurrency=100"}
  ]'</span>

kubectl <span class="nt">-n</span> capi-kubeadm-bootstrap-system patch deployment capi-kubeadm-bootstrap-controller-manager <span class="se">\</span>
  <span class="nt">--type</span><span class="o">=</span>json <span class="nt">-p</span><span class="o">=</span><span class="s1">'[
    {"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kube-api-qps=100"},
    {"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kube-api-burst=200"},
    {"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kubeadmconfig-concurrency=100"},
    {"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--cluster-concurrency=100"}
  ]'</span>

<span class="c"># Create a ClusterClass and save a Cluster manifest</span>
kubectl apply <span class="nt">-f</span> https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.7.2/clusterclass-in-memory-quick-start.yaml
clusterctl generate cluster <span class="k">in</span><span class="nt">-memory-test</span> <span class="nt">--flavor</span><span class="o">=</span><span class="k">in</span><span class="nt">-memory-development</span> <span class="nt">--kubernetes-version</span><span class="o">=</span>v1.30.0 <span class="o">&gt;</span> <span class="k">in</span><span class="nt">-memory-cluster</span>.yaml

<span class="c"># Create 1000 clusters</span>
<span class="nv">START</span><span class="o">=</span>0
<span class="nv">NUM</span><span class="o">=</span>1000
<span class="k">for</span> <span class="o">((</span><span class="nv">i</span><span class="o">=</span>START<span class="p">;</span> i&lt;NUM<span class="p">;</span> i++<span class="o">))</span>
<span class="k">do
  </span><span class="nv">name</span><span class="o">=</span><span class="s2">"test-</span><span class="si">$(</span><span class="nb">printf</span> <span class="s2">"%03d</span><span class="se">\n</span><span class="s2">"</span> <span class="s2">"</span><span class="nv">$i</span><span class="s2">"</span><span class="si">)</span><span class="s2">"</span>
  <span class="nb">sed</span> <span class="s2">"s/in-memory-test/</span><span class="k">${</span><span class="nv">name</span><span class="k">}</span><span class="s2">/g"</span> <span class="k">in</span><span class="nt">-memory-cluster</span>.yaml | kubectl apply <span class="nt">-f</span> -
<span class="k">done</span>
</code></pre></div></div>

<p>This should result in 1000 ready in-memory clusters (and a pretty hot laptop if
you run it locally). On a laptop with an i9-12900H CPU, it took about 15 minutes
until all clusters were ready.</p>

<h2 id="conclusion-and-next-steps">Conclusion and next steps</h2>

<p>We are very happy with the results we achieved. The community has been very
helpful and responsive, and we are very grateful for all the help we received.
Going forward, we will hopefully be able to run scale tests periodically to
ensure that we are not regressing. Even small scale tests can be enough to
detect performance regressions as long as we keep track of the performance
metrics. This is something we hope to incorporate into the CI system in the
future.</p>]]></content><author><name>Lennart Jern</name></author><category term="metal3" /><category term="cluster API" /><category term="provider" /><category term="edge" /><summary type="html"><![CDATA[In part 1, we introduced the Bare Metal Operator test mode and saw how it can be used to play with BareMetalHosts without Ironic and without any actual hosts. We continued in part 2 with how to fake workload clusters enough for convincing Cluster API’s controllers that they are healthy. These two pieces together allowed us to run scaling tests and reach our target of 1000 single node clusters. In this final part of the blog post series, we will take a look at the results, the issues that we encountered and the improvements that have been made.]]></summary></entry><entry><title type="html">Metal3 at KubeCon EU 2024</title><link href="https://metal3.io/blog/2024/04/10/Metal3_at_KubeCon_EU_2024.html" rel="alternate" type="text/html" title="Metal3 at KubeCon EU 2024" /><published>2024-04-10T00:00:00-05:00</published><updated>2024-04-10T00:00:00-05:00</updated><id>https://metal3.io/blog/2024/04/10/Metal3_at_KubeCon_EU_2024</id><content type="html" xml:base="https://metal3.io/blog/2024/04/10/Metal3_at_KubeCon_EU_2024.html"><![CDATA[<p>The Metal3 project was present at KubeCon EU 2024 with multiple maintainers,
contributors and users! For many of us, this was the first time we met in the
physical world, despite working together for years already. This was very
valuable and appreciated by many of us, I am sure. We had time to casually
discuss ideas and proposals, hack together on the
<a href="https://github.com/metal3-io/ironic-standalone-operator">ironic-standalone-operator</a>
and simply get to know each other.</p>

<p><img src="/assets/2024-04-10-Metal3_at_KubeCon_EU_2024/lightningtalk.jpg" alt="Lightning
talk" /></p>

<p><em>Photo by Michael Captain.</em></p>

<p>As a project, we had the opportunity to give an update through a <a href="https://www.youtube.com/watch?v=6QsOQsQZQS8">lightning
talk</a> on Tuesday!</p>

<!-- markdownlint-disable no-inline-html -->
<iframe width="560" height="315" src="https://www.youtube.com/embed/6QsOQsQZQS8?si=bH5w9svPxM1NE8Le" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
<!-- markdownlint-enable no-inline-html -->

<p>On Wednesday we continued with a <a href="https://kccnceu2024.sched.com/event/1YheY">contribfest session</a>
where we gave an introduction to the project for potential new contributors. We
had prepared a number of good-first-issue’s that people could choose from if
they wanted. Perhaps more important though, was that we had time to answer
questions, discuss use-cases, issues and features with the attendees. The new
<a href="https://book.metal3.io/quick-start">quick-start</a> page was also launched just in
time for the contribfest. It should hopefully make it easier to get started with
the project and we encourage everyone to run through it and report or fix any
issues found.</p>

<p><img src="/assets/2024-04-10-Metal3_at_KubeCon_EU_2024/contribfest.jpg" alt="Contribfest" /></p>

<p><em>Photo from the official CNCF Flickr. More photos
<a href="https://www.flickr.com/photos/143247548@N03/53609847541/in/album-72177720315561784/">here</a>.</em></p>

<p>Finally, just like previous, we had a table in the Project Pavilion. There was a
lot of interest in Metal3, more than last year I would say. Even with five
maintainers working in parallel, we still had a hard time keeping up with the
amount of people stopping by to ask questions! My takeaway from this event is
that we still have work to do on explaining what Metal3 is and how it works. It
is quite uncommon that people know about baseboard management controllers (BMCs)
and this of course makes it harder to grasp what Metal3 is all about. However,
the interest is there, so we just need to get the information out there so that
people can learn! Another takeaway is that Cluster API in general seems to
really take off. Many people that came by our kiosk knew about Cluster API and
were interested in Metal3 because of the integration with have with it.</p>

<p>For those of you who couldn’t attend, I hope this post gives an idea about what
happened at KubeCon related to Metal3. Did you miss the contribfest? Maybe you
would like to contribute but don’t know where to start? Check out the
<a href="https://github.com/issues?page=1&amp;q=archived%3Afalse+user%3Ametal3-io+label%3A%22good+first+issue%22+is%3Aissue+sort%3Acreated-asc+is%3Aopen">good-first-issue’s</a>!
There are still plenty to choose from, and we will keep adding more.</p>]]></content><author><name>Lennart Jern</name></author><category term="metal3" /><category term="talk" /><category term="conference" /><category term="kubecon" /><summary type="html"><![CDATA[The Metal3 project was present at KubeCon EU 2024 with multiple maintainers, contributors and users! For many of us, this was the first time we met in the physical world, despite working together for years already. This was very valuable and appreciated by many of us, I am sure. We had time to casually discuss ideas and proposals, hack together on the ironic-standalone-operator and simply get to know each other.]]></summary></entry><entry><title type="html">How to run Metal3 website locally with Jekyll</title><link href="https://metal3.io/blog/2024/01/18/How_to_run_Metal3_website_locally_with_Jekyll.html" rel="alternate" type="text/html" title="How to run Metal3 website locally with Jekyll" /><published>2024-01-18T00:00:00-06:00</published><updated>2024-01-18T00:00:00-06:00</updated><id>https://metal3.io/blog/2024/01/18/How_to_run_Metal3_website_locally_with_Jekyll</id><content type="html" xml:base="https://metal3.io/blog/2024/01/18/How_to_run_Metal3_website_locally_with_Jekyll.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>If you’re a developer or contributor to the Metal3 project, you may need
to run the Metal3 website locally to test changes and ensure everything
looks as expected before deploying them. In this guide, we’ll walk you
through the process of setting up and running Metal3’s website locally
on your machine using Jekyll.</p>

<h2 id="prerequisites">Prerequisites</h2>

<p>Before we begin, make sure you have the following prerequisites
installed on your system:</p>

<ul>
  <li>
    <p>Ruby: Jekyll, the static site generator used by Metal3, is built with
Ruby. Install Ruby and its development tools by running the following
command in your terminal:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nb">sudo </span>apt <span class="nb">install </span>ruby-full
</code></pre></div>    </div>
  </li>
</ul>

<h2 id="setting-up-metal3s-website">Setting up Metal3’s Website</h2>

<p>Once Ruby is installed, we can proceed to set up Metal3’s website and
its dependencies. Follow these steps:</p>

<ul>
  <li>
    <p>Clone the Metal3 website repository from GitHub. Open your terminal
and navigate to the directory where you want to clone the repository,
then run the following command:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>git clone https://github.com/metal3-io/metal3-io.github.io.git
</code></pre></div>    </div>
  </li>
  <li>
    <p>Change to the cloned directory:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nb">cd </span>metal3-io.github.io
</code></pre></div>    </div>
  </li>
  <li>
    <p>Install the required gems and dependencies using Bundler. Run the
following command:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>bundle <span class="nb">install</span>
</code></pre></div>    </div>
  </li>
</ul>

<h2 id="running-the-metal3-website-locally">Running the Metal3 Website Locally</h2>

<p>With Metal3’s website and its dependencies installed, you can now start the local
development server to view and test the website. In the terminal, navigate to the
project’s root directory (<code class="language-plaintext highlighter-rouge">metal3-io.github.io</code>) and run the following command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>bundle <span class="nb">exec </span>jekyll serve
</code></pre></div></div>

<p>This command tells Jekyll to build the website and start a local server.
Once the server is running, you’ll see output indicating the local
address where the Metal3 website is being served, typically
<a href="http://localhost:4000">http://localhost:4000</a>.</p>

<p>Open your web browser and enter the provided address. Congratulations!
You should now see the Metal3 website running locally, allowing you to
preview your changes and ensure everything is working as expected.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Running Metal3’s website locally using Jekyll is a great way to test
changes and ensure the site functions properly before deploying them. By
following the steps outlined in this guide, you’ve successfully set up
and run Metal3’s website locally. Feel free to explore the Metal3
documentation and contribute to the project further.</p>]]></content><author><name>Salima Rabiu</name></author><category term="metal3" /><category term="baremetal" /><category term="metal3-dev-env" /><category term="documentation" /><category term="development" /><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Scaling to 1000 clusters - Part 2</title><link href="https://metal3.io/blog/2023/05/17/Scaling_part_2.html" rel="alternate" type="text/html" title="Scaling to 1000 clusters - Part 2" /><published>2023-05-17T00:00:00-05:00</published><updated>2023-05-17T00:00:00-05:00</updated><id>https://metal3.io/blog/2023/05/17/Scaling_part_2</id><content type="html" xml:base="https://metal3.io/blog/2023/05/17/Scaling_part_2.html"><![CDATA[<p>In <a href="/blog/2023/05/05/Scaling_part_1.html">part 1</a>, we introduced the Bare Metal Operator test mode and saw how it can be used to play with BareMetalHosts without Ironic and without any actual hosts.
Now we will take a look at the other end of the stack and how we can fake the workload cluster API’s.</p>

<h2 id="test-setup">Test setup</h2>

<p>The end goal is to have one management cluster where the Cluster API and Metal3 controllers run.
In this cluster we would generate BareMetalHosts and create Clusters, Metal3Clusters, etc to benchmark the controllers.
To give them a realistic test, we also need to fake the workload cluster API’s.
These will run separately in “backing” clusters to avoid interfering with the test (e.g. by using up all the resources in the management cluster).
Here is a diagram that describes the setup:</p>

<p><img src="/assets/2023-05-17-Scaling_part_2/scaling-fake-clusters.drawio.png" alt="diagram of test setup" /></p>

<p>How are we going to fake the workload cluster API’s then?
The most obvious solution is to just run the real deal, i.e. the <code class="language-plaintext highlighter-rouge">kube-apiserver</code>.
This is what would be run in a real workload cluster, together with the other components that make up the Kubernetes control plane.</p>

<p>If you want to follow along and try to set this up yourself, you will need at least the following tools installed:</p>

<ul>
  <li><a href="https://kind.sigs.k8s.io/docs/user/quick-start">kind</a></li>
  <li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl">kubectl</a></li>
  <li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">kubeadm</a></li>
  <li><a href="https://cluster-api.sigs.k8s.io/user/quick-start.html#install-clusterctl">clusterctl</a></li>
  <li><a href="https://github.com/openssl/openssl">openssl</a></li>
  <li><a href="https://curl.se/">curl</a></li>
  <li><a href="https://www.gnu.org/software/wget/">wget</a></li>
</ul>

<p>This has been tested with Kubernetes v1.25, kind v0.19 and clusterctl v1.4.2.
All script snippets are assumed to be for the <code class="language-plaintext highlighter-rouge">bash</code> shell.</p>

<h2 id="running-the-kubernetes-api-server">Running the Kubernetes API server</h2>

<p>There are many misconceptions, maybe even superstitions, about the Kubernetes control plane.
The fact is that it is in no way special.
It consists of a few programs that can be run in any way you want: in a container, as a systemd unit or directly executed at the command line.
They can run on a Node or outside of the cluster.
You can even run multiple instances on the same host as long as you avoid port collisions.</p>

<p>For our purposes we basically want to run as little as possible of the control plane components.
We just need the API to be available and possible for us to populate with data that the controllers expect to be there.
In other words, we need the API server and etcd.
The scheduler is not necessary since we won’t run any actual workload (we are just pretending the Nodes are there anyway) and the controller manager would just get in the way when we want to fake resources.
It would, for example, try to update the status of the (fake) Nodes that we want to create.</p>

<p>The API server will need an etcd instance to connect to.
It will also need some TLS configuration, both for connecting to etcd and for handling service accounts.
One simple way to generate the needed certificates is to use kubeadm.
But before we get there we need to think about how the configuration should look like.</p>

<p>For simplicity, we will simply run the API server and etcd in a kind cluster for now.
It would then be easy to run them in some other Kubernetes cluster later if needed.
Let’s create it right away:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>kind create cluster
<span class="c"># Note: This has been tested with node image</span>
<span class="c"># kindest/node:v1.26.3@sha256:61b92f38dff6ccc29969e7aa154d34e38b89443af1a2c14e6cfbd2df6419c66f</span>
</code></pre></div></div>

<p>To try to cut down on the resources required, we will also use a single multi-tenant etcd instance instead of one per API server.
We can rely on the internal service discovery so the API server can find etcd via an address like <code class="language-plaintext highlighter-rouge">etcd-server.etd-system.svc.cluster.local</code>, instead of using IP addresses.
Finally, we will need an endpoint where the API is exposed to the cluster where the controllers are running, but for now we can focus on just getting it up and running with <code class="language-plaintext highlighter-rouge">127.0.0.1:6443</code> as the endpoint.</p>

<p>Based on the above, we can create a <code class="language-plaintext highlighter-rouge">kubeadm-config.yaml</code> file like this:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubeadm.k8s.io/v1beta3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ClusterConfiguration</span>
<span class="na">apiServer</span><span class="pi">:</span>
  <span class="na">certSANs</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">127.0.0.1</span>
<span class="na">clusterName</span><span class="pi">:</span> <span class="s">test</span>
<span class="na">controlPlaneEndpoint</span><span class="pi">:</span> <span class="s">127.0.0.1:6443</span>
<span class="na">etcd</span><span class="pi">:</span>
  <span class="na">local</span><span class="pi">:</span>
    <span class="na">serverCertSANs</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">etcd-server.etcd-system.svc.cluster.local</span>
    <span class="na">peerCertSANs</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">etcd-0.etcd.etcd-system.svc.cluster.local</span>
<span class="na">kubernetesVersion</span><span class="pi">:</span> <span class="s">v1.25.3</span>
<span class="na">certificatesDir</span><span class="pi">:</span> <span class="s">/tmp/test/pki</span>
</code></pre></div></div>

<p>We can now use this to generate some certificates and upload them to the cluster:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Generate CA certificates</span>
kubeadm init phase certs etcd-ca <span class="nt">--config</span> kubeadm-config.yaml
kubeadm init phase certs ca <span class="nt">--config</span> kubeadm-config.yaml
<span class="c"># Generate etcd peer and server certificates</span>
kubeadm init phase certs etcd-peer <span class="nt">--config</span> kubeadm-config.yaml
kubeadm init phase certs etcd-server <span class="nt">--config</span> kubeadm-config.yaml

<span class="c"># Upload certificates</span>
kubectl create namespace etcd-system
kubectl <span class="nt">-n</span> etcd-system create secret tls test-etcd <span class="nt">--cert</span> /tmp/test/pki/etcd/ca.crt <span class="nt">--key</span> /tmp/test/pki/etcd/ca.key
kubectl <span class="nt">-n</span> etcd-system create secret tls etcd-peer <span class="nt">--cert</span> /tmp/test/pki/etcd/peer.crt <span class="nt">--key</span> /tmp/test/pki/etcd/peer.key
kubectl <span class="nt">-n</span> etcd-system create secret tls etcd-server <span class="nt">--cert</span> /tmp/test/pki/etcd/server.crt <span class="nt">--key</span> /tmp/test/pki/etcd/server.key
</code></pre></div></div>

<h3 id="deploying-a-multi-tenant-etcd-instance">Deploying a multi-tenant etcd instance</h3>

<p>Now it is time to deploy etcd!</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/manifests/v2/etcd.yaml <span class="se">\</span>
  | <span class="nb">sed</span> <span class="s2">"s/CLUSTER/test/g"</span> | kubectl <span class="nt">-n</span> etcd-system apply <span class="nt">-f</span> -
kubectl <span class="nt">-n</span> etcd-system <span class="nb">wait </span>sts/etcd <span class="nt">--for</span><span class="o">=</span><span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.status.availableReplicas}"</span><span class="o">=</span>1
</code></pre></div></div>

<p>As mentioned before, we want to create a <a href="https://etcd.io/docs/v3.5/op-guide/authentication/rbac/">multi-tenant etcd</a> that many API servers can share.
For this reason, we will need to create a root user and enable authentication for etcd:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Create root role</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  role add root
<span class="c"># Create root user</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  user add root <span class="nt">--new-user-password</span><span class="o">=</span><span class="s2">"rootpw"</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  user grant-role root root
<span class="c"># Enable authentication</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  auth <span class="nb">enable</span>
</code></pre></div></div>

<p>At this point we have a working etcd instance with authentication and TLS enabled.
Each client will need to have an etcd user to interact with this instance so we need to create an etcd user for the API server.
We already created a root user before so this should look familiar.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c">## Create etcd tenant</span>
<span class="c"># Create user</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="nt">--user</span> root:rootpw <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  user add <span class="nb">test</span> <span class="nt">--new-user-password</span><span class="o">=</span><span class="nb">test</span>
<span class="c"># Create role</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="nt">--user</span> root:rootpw <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  role add <span class="nb">test</span>
<span class="c"># Add read/write permissions for prefix to the role</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="nt">--user</span> root:rootpw <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  role grant-permission <span class="nb">test</span> <span class="nt">--prefix</span><span class="o">=</span><span class="nb">true </span>readwrite <span class="s2">"/test/"</span>
<span class="c"># Give the user permissions from the role</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="nt">--user</span> root:rootpw <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  user grant-role <span class="nb">test test</span>
</code></pre></div></div>

<p>From etcd’s point of view, everything is now ready.
The API server could theoretically use <code class="language-plaintext highlighter-rouge">etcdctl</code> and authenticate with the username and password that we created for it.
However, that is not how the API server works.
It expects to be able to authenticate using client certificates.
Luckily, etcd supports this so we just have to generate the certificates and sign them so that etcd trusts them.
The key thing is to set the common name in the certificate to the name of the user we want to authenticate as.</p>

<p>Since <code class="language-plaintext highlighter-rouge">kubeadm</code> always sets the same common name, we will here use <code class="language-plaintext highlighter-rouge">openssl</code> to generate the client certificates so that we get control over it.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Generate etcd client certificate</span>
openssl req <span class="nt">-newkey</span> rsa:2048 <span class="nt">-nodes</span> <span class="nt">-subj</span> <span class="s2">"/CN=test"</span> <span class="se">\</span>
 <span class="nt">-keyout</span> <span class="s2">"/tmp/test/pki/apiserver-etcd-client.key"</span> <span class="nt">-out</span> <span class="s2">"/tmp/test/pki/apiserver-etcd-client.csr"</span>
openssl x509 <span class="nt">-req</span> <span class="nt">-in</span> <span class="s2">"/tmp/test/pki/apiserver-etcd-client.csr"</span> <span class="se">\</span>
  <span class="nt">-CA</span> /tmp/test/pki/etcd/ca.crt <span class="nt">-CAkey</span> /tmp/test/pki/etcd/ca.key <span class="nt">-CAcreateserial</span> <span class="se">\</span>
  <span class="nt">-out</span> <span class="s2">"/tmp/test/pki/apiserver-etcd-client.crt"</span> <span class="nt">-days</span> 365
</code></pre></div></div>

<h3 id="deploying-the-api-server">Deploying the API server</h3>

<p>In order to deploy the API server, we will first need to generate some more certificates.
The client certificates for connecting to etcd are already ready, but it also needs certificates to secure the exposed API itself, and a few other things.
Then we will also need to create secrets from all of these certificates:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>kubeadm init phase certs ca <span class="nt">--config</span> kubeadm-config.yaml
kubeadm init phase certs apiserver <span class="nt">--config</span> kubeadm-config.yaml
kubeadm init phase certs sa <span class="nt">--cert-dir</span> /tmp/test/pki

kubectl create ns workload-api
kubectl <span class="nt">-n</span> workload-api create secret tls test-ca <span class="nt">--cert</span> /tmp/test/pki/ca.crt <span class="nt">--key</span> /tmp/test/pki/ca.key
kubectl <span class="nt">-n</span> workload-api create secret tls test-etcd <span class="nt">--cert</span> /tmp/test/pki/etcd/ca.crt <span class="nt">--key</span> /tmp/test/pki/etcd/ca.key
kubectl <span class="nt">-n</span> workload-api create secret tls <span class="s2">"test-apiserver-etcd-client"</span> <span class="se">\</span>
  <span class="nt">--cert</span> <span class="s2">"/tmp/test/pki/apiserver-etcd-client.crt"</span> <span class="se">\</span>
  <span class="nt">--key</span> <span class="s2">"/tmp/test/pki/apiserver-etcd-client.key"</span>
kubectl <span class="nt">-n</span> workload-api create secret tls apiserver <span class="se">\</span>
  <span class="nt">--cert</span> <span class="s2">"/tmp/test/pki/apiserver.crt"</span> <span class="se">\</span>
  <span class="nt">--key</span> <span class="s2">"/tmp/test/pki/apiserver.key"</span>
kubectl <span class="nt">-n</span> workload-api create secret generic test-sa <span class="se">\</span>
  <span class="nt">--from-file</span><span class="o">=</span>tls.crt<span class="o">=</span><span class="s2">"/tmp/test/pki/sa.pub"</span> <span class="se">\</span>
  <span class="nt">--from-file</span><span class="o">=</span>tls.key<span class="o">=</span><span class="s2">"/tmp/test/pki/sa.key"</span>
</code></pre></div></div>

<p>With all that out of the way, we can finally deploy the API server!
For this we will use a normal Deployment.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Deploy API server</span>
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/manifests/v2/kube-apiserver-deployment.yaml |
  <span class="nb">sed</span> <span class="s2">"s/CLUSTER/test/g"</span> | kubectl <span class="nt">-n</span> workload-api apply <span class="nt">-f</span> -
kubectl <span class="nt">-n</span> workload-api <span class="nb">wait</span> <span class="nt">--for</span><span class="o">=</span><span class="nv">condition</span><span class="o">=</span>Available deploy/test-kube-apiserver
</code></pre></div></div>

<p>Time to check if it worked!
We can use port-forwarding to access the API, but of course we will need some authentication method for it to be useful.
With kubeadm we can generate a kubeconfig based on the certificates we already have.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>kubeadm kubeconfig user <span class="nt">--client-name</span> kubernetes-admin <span class="nt">--org</span> system:masters <span class="se">\</span>
  <span class="nt">--config</span> kubeadm-config.yaml <span class="o">&gt;</span> kubeconfig.yaml
</code></pre></div></div>

<p>Now open another terminal and set up port-forwarding to the API server:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>kubectl <span class="nt">-n</span> workload-api port-forward svc/test-kube-apiserver 6443
</code></pre></div></div>

<p>Back in the original terminal, you should now be able to reach the workload API server:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>kubectl <span class="nt">--kubeconfig</span> kubeconfig.yaml cluster-info
</code></pre></div></div>

<p>Note that it won’t have any Nodes or Pods running.
It is completely empty since it is running on its own.
There is no kubelet that registered as a Node or applied static manifests, there is no scheduler or controller manager.
Exactly like we want it.</p>

<h2 id="faking-nodes-and-other-resources">Faking Nodes and other resources</h2>

<p>Let’s take a step back and think about what we have done so far.
We have deployed a Kubernetes API server and a multi-tenant etcd instance.
More API servers can be added in the same way, so it is straight forward to scale.
All of it runs in a kind cluster, which means that it is easy to set up and we can switch to any other Kubernetes cluster if needed later.
Through Kubernetes we also get an easy way to access the API servers by using port-forwarding, without exposing all of them separately.</p>

<p>The time has now come to think about what we need to put in the workload cluster API to convince the Cluster API and Metal3 controllers that it is healthy.
First of all they will expect to see Nodes that match the Machines and that they have a provider ID set.
Secondly, they will expect to see healthy control plane Pods.
Finally, they will try to check on the etcd cluster.</p>

<p>The final point is a problem, but we can work around it for now by configuring <a href="https://cluster-api.sigs.k8s.io/tasks/external-etcd.html">external etcd</a>.
It will lead to a different code path for the bootstrap and control plane controllers, but until we have something better it will be a good enough test.</p>

<p>Creating the Nodes and control plane Pods is really easy though.
We are just adding resources and there are no controllers or validating web hooks that can interfere.
Try it out!</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Create a Node</span>
kubectl <span class="nt">--kubeconfig</span><span class="o">=</span>kubeconfig.yaml create <span class="nt">-f</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/fake-node.yaml
<span class="c"># Check that it worked</span>
kubectl <span class="nt">--kubeconfig</span><span class="o">=</span>kubeconfig.yaml get nodes
<span class="c"># Maybe label it as part of the control plane?</span>
kubectl <span class="nt">--kubeconfig</span><span class="o">=</span>kubeconfig.yaml label node fake-node node-role.kubernetes.io/control-plane<span class="o">=</span><span class="s2">""</span>
</code></pre></div></div>

<p>Now add a Pod:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>kubectl <span class="nt">--kubeconfig</span><span class="o">=</span>kubeconfig.yaml create <span class="nt">-f</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-apiserver-pod.yaml
<span class="c"># Set status on the pods (it is not added when using create/apply).</span>
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-apiserver-pod-status.yaml |
  kubectl <span class="nt">--kubeconfig</span><span class="o">=</span>kubeconfig.yaml <span class="nt">-n</span> kube-system patch pod kube-apiserver-node-name <span class="se">\</span>
    <span class="nt">--subresource</span><span class="o">=</span>status <span class="nt">--patch-file</span><span class="o">=</span>/dev/stdin
</code></pre></div></div>

<p>You should be able to see something like this:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">$</span><span class="w"> </span>kubectl <span class="nt">--kubeconfig</span> kubeconfig.yaml get pods <span class="nt">-A</span>
<span class="go">NAMESPACE     NAME                       READY   STATUS    RESTARTS   AGE
kube-system   kube-apiserver-node-name   1/1     Running   0          16h
</span><span class="gp">$</span><span class="w"> </span>kubectl <span class="nt">--kubeconfig</span> kubeconfig.yaml get nodes
<span class="go">NAME        STATUS   ROLES    AGE   VERSION
</span><span class="gp">fake-node   Ready    &lt;none&gt;</span><span class="w">   </span>16h   v1.25.3
</code></pre></div></div>

<p>Now all we have to do is to ensure that the API returns information that the controllers expect.</p>

<h2 id="hooking-up-the-api-server-to-a-cluster-api-cluster">Hooking up the API server to a Cluster API cluster</h2>

<p>We will now set up a fresh cluster where we can run the Cluster API and Metal3 controllers.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Delete the previous cluster</span>
kind delete cluster
<span class="c"># Create a fresh new cluster</span>
kind create cluster
<span class="c"># Initialize Cluster API with Metal3</span>
clusterctl init <span class="nt">--infrastructure</span> metal3
<span class="c">## Deploy the Bare Metal Opearator</span>
<span class="c"># Create the namespace where it will run</span>
kubectl create ns baremetal-operator-system
<span class="c"># Deploy it in normal mode</span>
kubectl apply <span class="nt">-k</span> https://github.com/metal3-io/baremetal-operator/config/default
<span class="c"># Patch it to run in test mode</span>
kubectl patch <span class="nt">-n</span> baremetal-operator-system deploy baremetal-operator-controller-manager <span class="nt">--type</span><span class="o">=</span>json <span class="se">\</span>
  <span class="nt">-p</span><span class="o">=</span><span class="s1">'[{"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--test-mode"}]'</span>
</code></pre></div></div>

<p>You should now have a cluster with the Cluster API, Metal3 provider and Bare Metal Operator running.
Next, we will prepare some files that will come in handy later, namely a cluster template, BareMetalHost manifest and Kubeadm configuration file.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Download cluster-template</span>
<span class="nv">CLUSTER_TEMPLATE</span><span class="o">=</span>/tmp/cluster-template.yaml
<span class="c"># https://github.com/metal3-io/cluster-api-provider-metal3/blob/main/examples/clusterctl-templates/clusterctl-cluster.yaml</span>
<span class="nv">CLUSTER_TEMPLATE_URL</span><span class="o">=</span><span class="s2">"https://raw.githubusercontent.com/metal3-io/cluster-api-provider-metal3/main/examples/clusterctl-templates/clusterctl-cluster.yaml"</span>
wget <span class="nt">-O</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_TEMPLATE</span><span class="k">}</span><span class="s2">"</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_TEMPLATE_URL</span><span class="k">}</span><span class="s2">"</span>

<span class="c"># Save a manifest of a BareMetalHost</span>
<span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh"> &gt; /tmp/test-hosts.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: worker-1-bmc-secret
type: Opaque
data:
  username: YWRtaW4=
  password: cGFzc3dvcmQ=
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: worker-1
spec:
  online: true
  bmc:
    address: libvirt://192.168.122.1:6233/
    credentialsName: worker-1-bmc-secret
  bootMACAddress: "00:60:2F:10:E9:A7"
</span><span class="no">EOF

</span><span class="c"># Save a kubeadm config template</span>
<span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh"> &gt; /tmp/kubeadm-config-template.yaml
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs:
    - localhost
    - 127.0.0.1
    - 0.0.0.0
    - HOST
clusterName: test
controlPlaneEndpoint: HOST:6443
etcd:
  local:
    serverCertSANs:
      - etcd-server.etcd-system.svc.cluster.local
    peerCertSANs:
      - etcd-0.etcd.etcd-system.svc.cluster.local
kubernetesVersion: v1.25.3
certificatesDir: /tmp/CLUSTER/pki
</span><span class="no">EOF
</span></code></pre></div></div>

<p>With this we have enough to start creating the workload cluster.
First, we need to set up some certificates.
This should look very familiar from earlier when we created certificates for the Kubernetes API server and etcd.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nb">mkdir</span> <span class="nt">-p</span> /tmp/pki/etcd
<span class="nv">CLUSTER</span><span class="o">=</span><span class="s2">"test"</span>
<span class="nv">NAMESPACE</span><span class="o">=</span>etcd-system
<span class="nv">CLUSTER_APIENDPOINT_HOST</span><span class="o">=</span><span class="s2">"test-kube-apiserver.</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">.svc.cluster.local"</span>

<span class="nb">sed</span> <span class="nt">-e</span> <span class="s2">"s/NAMESPACE/</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">/g"</span> <span class="nt">-e</span> <span class="s2">"s/</span><span class="se">\/</span><span class="s2">CLUSTER//g"</span> <span class="nt">-e</span> <span class="s2">"s/HOST/</span><span class="k">${</span><span class="nv">CLUSTER_APIENDPOINT_HOST</span><span class="k">}</span><span class="s2">/g"</span> <span class="se">\</span>
  /tmp/kubeadm-config-template.yaml <span class="o">&gt;</span> <span class="s2">"/tmp/kubeadm-config-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span>

<span class="c"># Generate CA certificates</span>
kubeadm init phase certs etcd-ca <span class="nt">--config</span> <span class="s2">"/tmp/kubeadm-config-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span>
kubeadm init phase certs ca <span class="nt">--config</span> <span class="s2">"/tmp/kubeadm-config-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span>
<span class="c"># Generate etcd peer and server certificates</span>
kubeadm init phase certs etcd-peer <span class="nt">--config</span> <span class="s2">"/tmp/kubeadm-config-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span>
kubeadm init phase certs etcd-server <span class="nt">--config</span> <span class="s2">"/tmp/kubeadm-config-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span>
</code></pre></div></div>

<p>Next, we create the namespace, the BareMetalHost and secrets from the certificates:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nv">CLUSTER</span><span class="o">=</span>test-1
<span class="nv">NAMESPACE</span><span class="o">=</span>test-1
kubectl create namespace <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span>
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> apply <span class="nt">-f</span> /tmp/test-hosts.yaml
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">-etcd"</span> <span class="nt">--cert</span> /tmp/pki/etcd/ca.crt <span class="nt">--key</span> /tmp/pki/etcd/ca.key
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">-ca"</span> <span class="nt">--cert</span> /tmp/pki/ca.crt <span class="nt">--key</span> /tmp/pki/ca.key
</code></pre></div></div>

<p>We are now ready to create the cluster!
We just need a few variables for the template.
The important part here is the <code class="language-plaintext highlighter-rouge">CLUSTER_APIENDPOINT_HOST</code> and <code class="language-plaintext highlighter-rouge">CLUSTER_APIENDPOINT_PORT</code>, since this will be used by the controllers to connect to the workload cluster API.
You should set the IP to the private IP of the test machine or similar.
This way we can use port-forwarding to expose the API on this IP, which the controllers can then reach.
The port just have to be one not in use, and preferably something that is easy to remember and associate with the correct cluster.
For example, cluster 1 gets port 10001, cluster 2 gets 10002, etc.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nb">export </span><span class="nv">IMAGE_CHECKSUM</span><span class="o">=</span><span class="s2">"97830b21ed272a3d854615beb54cf004"</span>
<span class="nb">export </span><span class="nv">IMAGE_CHECKSUM_TYPE</span><span class="o">=</span><span class="s2">"md5"</span>
<span class="nb">export </span><span class="nv">IMAGE_FORMAT</span><span class="o">=</span><span class="s2">"raw"</span>
<span class="nb">export </span><span class="nv">IMAGE_URL</span><span class="o">=</span><span class="s2">"http://172.22.0.1/images/rhcos-ootpa-latest.qcow2"</span>
<span class="nb">export </span><span class="nv">KUBERNETES_VERSION</span><span class="o">=</span><span class="s2">"v1.25.3"</span>
<span class="nb">export </span><span class="nv">WORKERS_KUBEADM_EXTRA_CONFIG</span><span class="o">=</span><span class="s2">""</span>
<span class="nb">export </span><span class="nv">CLUSTER_APIENDPOINT_HOST</span><span class="o">=</span><span class="s2">"172.17.0.2"</span>
<span class="nb">export </span><span class="nv">CLUSTER_APIENDPOINT_PORT</span><span class="o">=</span><span class="s2">"10001"</span>
<span class="nb">export </span><span class="nv">CTLPLANE_KUBEADM_EXTRA_CONFIG</span><span class="o">=</span><span class="s2">"
    clusterConfiguration:
      controlPlaneEndpoint: </span><span class="k">${</span><span class="nv">CLUSTER_APIENDPOINT_HOST</span><span class="k">}</span><span class="s2">:</span><span class="k">${</span><span class="nv">CLUSTER_APIENDPOINT_PORT</span><span class="k">}</span><span class="s2">
      apiServer:
        certSANs:
        - localhost
        - 127.0.0.1
        - 0.0.0.0
        - </span><span class="k">${</span><span class="nv">CLUSTER_APIENDPOINT_HOST</span><span class="k">}</span><span class="s2">
      etcd:
        external:
          endpoints:
            - https://etcd-server:2379
          caFile: /etc/kubernetes/pki/etcd/ca.crt
          certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt
          keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key"</span>
</code></pre></div></div>

<p>Create the cluster!</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>clusterctl generate cluster <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="nt">--from</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_TEMPLATE</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="nt">--target-namespace</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> | kubectl apply <span class="nt">-f</span> -
</code></pre></div></div>

<p>This will give you a cluster and all the templates and other resources that are needed.
However, we will need to fill in for the non-existent hardware and create the workload cluster API server, like we practiced before.
This time it is slightly different, because some of the steps are handled by the Cluster API.
We just need to take care of what would happen on the node, plus the etcd part since we are using external etcd configuration.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nb">mkdir</span> <span class="nt">-p</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/etcd"</span>

<span class="c"># Generate etcd client certificate</span>
openssl req <span class="nt">-newkey</span> rsa:2048 <span class="nt">-nodes</span> <span class="nt">-subj</span> <span class="s2">"/CN=</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
 <span class="nt">-keyout</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver-etcd-client.key"</span> <span class="nt">-out</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver-etcd-client.csr"</span>
openssl x509 <span class="nt">-req</span> <span class="nt">-in</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver-etcd-client.csr"</span> <span class="se">\</span>
  <span class="nt">-CA</span> /tmp/pki/etcd/ca.crt <span class="nt">-CAkey</span> /tmp/pki/etcd/ca.key <span class="nt">-CAcreateserial</span> <span class="se">\</span>
  <span class="nt">-out</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver-etcd-client.crt"</span> <span class="nt">-days</span> 365

<span class="c"># Get the k8s ca certificate and key.</span>
<span class="c"># This is used by kubeadm to generate the api server certificates</span>
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> get secrets <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">-ca"</span> <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.data.tls</span><span class="se">\.</span><span class="s2">crt}"</span> | <span class="nb">base64</span> <span class="nt">-d</span> <span class="o">&gt;</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/ca.crt"</span>
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> get secrets <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">-ca"</span> <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.data.tls</span><span class="se">\.</span><span class="s2">key}"</span> | <span class="nb">base64</span> <span class="nt">-d</span> <span class="o">&gt;</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/ca.key"</span>

<span class="c"># Generate certificates</span>
<span class="nb">sed</span> <span class="nt">-e</span> <span class="s2">"s/NAMESPACE/</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">/g"</span> <span class="nt">-e</span> <span class="s2">"s/CLUSTER/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/g"</span> <span class="nt">-e</span> <span class="s2">"s/HOST/</span><span class="k">${</span><span class="nv">CLUSTER_APIENDPOINT_HOST</span><span class="k">}</span><span class="s2">/g"</span> <span class="se">\</span>
  /tmp/kubeadm-config-template.yaml <span class="o">&gt;</span> <span class="s2">"/tmp/kubeadm-config-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span>
kubeadm init phase certs apiserver <span class="nt">--config</span> <span class="s2">"/tmp/kubeadm-config-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span>

<span class="c"># Create secrets</span>
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">-apiserver-etcd-client"</span> <span class="nt">--cert</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver-etcd-client.crt"</span> <span class="nt">--key</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver-etcd-client.key"</span>
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls apiserver <span class="nt">--cert</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver.crt"</span> <span class="nt">--key</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver.key"</span>
</code></pre></div></div>

<p>Now we will need to set up the fake cluster resources.
For this we will create a second kind cluster and set up etcd, just like we did before.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Note: This will create a kubeconfig context named kind-backing-cluster-1,</span>
<span class="c"># i.e. "kind-" is prefixed to the name.</span>
kind create cluster <span class="nt">--name</span> backing-cluster-1

<span class="c"># Setup central etcd</span>
<span class="nv">CLUSTER</span><span class="o">=</span><span class="s2">"test"</span>
<span class="nv">NAMESPACE</span><span class="o">=</span>etcd-system
kubectl create namespace <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span>

<span class="c"># Upload certificates</span>
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">-etcd"</span> <span class="nt">--cert</span> /tmp/pki/etcd/ca.crt <span class="nt">--key</span> /tmp/pki/etcd/ca.key
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls etcd-peer <span class="nt">--cert</span> /tmp/pki/etcd/peer.crt <span class="nt">--key</span> /tmp/pki/etcd/peer.key
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls etcd-server <span class="nt">--cert</span> /tmp/pki/etcd/server.crt <span class="nt">--key</span> /tmp/pki/etcd/server.key

<span class="c"># Deploy ETCD</span>
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/manifests/v2/etcd.yaml <span class="se">\</span>
  | <span class="nb">sed</span> <span class="s2">"s/CLUSTER/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/g"</span> | kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> apply <span class="nt">-f</span> -
kubectl <span class="nt">-n</span> etcd-system <span class="nb">wait </span>sts/etcd <span class="nt">--for</span><span class="o">=</span><span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.status.availableReplicas}"</span><span class="o">=</span>1

<span class="c"># Create root role</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  role add root
<span class="c"># Create root user</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  user add root <span class="nt">--new-user-password</span><span class="o">=</span><span class="s2">"rootpw"</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  user grant-role root root
<span class="c"># Enable authentication</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  auth <span class="nb">enable</span>
</code></pre></div></div>

<p>Switch the context back to the first cluster with <code class="language-plaintext highlighter-rouge">kubectl config use-context kind-kind</code> so we don’t get confused about which is the main cluster.
We will now need to put all the expected certificates for the fake cluster in the <code class="language-plaintext highlighter-rouge">kind-backing-cluster-1</code> so that they can be used by the API server that we will deploy there.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nv">CLUSTER</span><span class="o">=</span>test-1
<span class="nv">NAMESPACE</span><span class="o">=</span>test-1
<span class="c"># Setup fake resources for cluster test-1</span>
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 create namespace <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span>
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">-etcd"</span> <span class="nt">--cert</span> /tmp/pki/etcd/ca.crt <span class="nt">--key</span> /tmp/pki/etcd/ca.key
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">-ca"</span> <span class="nt">--cert</span> /tmp/pki/ca.crt <span class="nt">--key</span> /tmp/pki/ca.key
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">-apiserver-etcd-client"</span> <span class="nt">--cert</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver-etcd-client.crt"</span> <span class="nt">--key</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver-etcd-client.key"</span>
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls apiserver <span class="nt">--cert</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver.crt"</span> <span class="nt">--key</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver.key"</span>

kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> get secrets <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">-sa"</span> <span class="nt">-o</span> yaml | kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 create <span class="nt">-f</span> -

<span class="c">## Create etcd tenant</span>
<span class="c"># Create user</span>
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="nt">--user</span> root:rootpw <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  user add <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--new-user-password</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">"</span>
<span class="c"># Create role</span>
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="nt">--user</span> root:rootpw <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  role add <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">"</span>
<span class="c"># Add read/write permissions for prefix to the role</span>
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="nt">--user</span> root:rootpw <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  role grant-permission <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--prefix</span><span class="o">=</span><span class="nb">true </span>readwrite <span class="s2">"/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/"</span>
<span class="c"># Give the user permissions from the role</span>
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="nt">--user</span> root:rootpw <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  user grant-role <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">"</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">"</span>
</code></pre></div></div>

<p>Check that the Metal3Machine is associated with a BareMetalHost.
Deploy the API server.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Deploy API server</span>
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/manifests/v2/kube-apiserver-deployment.yaml |
  <span class="nb">sed</span> <span class="nt">-e</span> <span class="s2">"s/CLUSTER/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/g"</span> | kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> apply <span class="nt">-f</span> -
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> <span class="nb">wait</span> <span class="nt">--for</span><span class="o">=</span><span class="nv">condition</span><span class="o">=</span>Available deploy/test-kube-apiserver

<span class="c"># Get kubeconfig</span>
clusterctl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> get kubeconfig <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">"</span> <span class="o">&gt;</span> <span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span>
<span class="c"># Edit kubeconfig to point to 127.0.0.1:${CLUSTER_APIENDPOINT_PORT}</span>
<span class="nb">sed</span> <span class="nt">-i</span> <span class="nt">-e</span> <span class="s2">"s/</span><span class="k">${</span><span class="nv">CLUSTER_APIENDPOINT_HOST</span><span class="k">}</span><span class="s2">/127.0.0.1/"</span> <span class="nt">-e</span> <span class="s2">"s/:6443/:</span><span class="k">${</span><span class="nv">CLUSTER_APIENDPOINT_PORT</span><span class="k">}</span><span class="s2">/"</span> <span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span>
<span class="c"># Port forward for accessing the API</span>
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> port-forward <span class="se">\</span>
      <span class="nt">--address</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_APIENDPOINT_HOST</span><span class="k">}</span><span class="s2">,127.0.0.1"</span> svc/test-kube-apiserver <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_APIENDPOINT_PORT</span><span class="k">}</span><span class="s2">"</span>:6443 &amp;
<span class="c"># Check that it is working</span>
kubectl <span class="nt">--kubeconfig</span><span class="o">=</span><span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span> cluster-info
</code></pre></div></div>

<p>Now that we have a working API for the workload cluster, the only remaining thing is to put everything that the controllers expect in it.
This includes adding a Node to match the Machine as well as static pods that Cluster API expects to be there.
Let’s start with the Node!
The Node must have the correct name and a label with the BareMetalHost UID so that the controllers can put the correct provider ID on it.
We have only created 1 BareMetalHost so it is easy to pick the correct one.
The name of the Node should be the same as the Machine, which is also only a single one.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nv">machine</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span>kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> get machine <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[0].metadata.name}"</span><span class="si">)</span><span class="s2">"</span>
<span class="nv">bmh_uid</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span>kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> get bmh <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[0].metadata.uid}"</span><span class="si">)</span><span class="s2">"</span>
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/fake-node.yaml |
  <span class="nb">sed</span> <span class="nt">-e</span> <span class="s2">"s/fake-node/</span><span class="k">${</span><span class="nv">machine</span><span class="k">}</span><span class="s2">/g"</span> <span class="nt">-e</span> <span class="s2">"s/fake-uuid/</span><span class="k">${</span><span class="nv">bmh_uid</span><span class="k">}</span><span class="s2">/g"</span> | <span class="se">\</span>
  kubectl <span class="nt">--kubeconfig</span><span class="o">=</span><span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span> create <span class="nt">-f</span> -
<span class="c"># Label it as control-plane since this is a control-plane node.</span>
kubectl <span class="nt">--kubeconfig</span><span class="o">=</span><span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span> label node <span class="s2">"</span><span class="k">${</span><span class="nv">machine</span><span class="k">}</span><span class="s2">"</span> node-role.kubernetes.io/control-plane<span class="o">=</span><span class="s2">""</span>
<span class="c"># Upload kubeadm config to configmap. This will mark the KCP as initialized.</span>
kubectl <span class="nt">--kubeconfig</span><span class="o">=</span><span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span> <span class="nt">-n</span> kube-system create cm kubeadm-config <span class="se">\</span>
  <span class="nt">--from-file</span><span class="o">=</span><span class="nv">ClusterConfiguration</span><span class="o">=</span><span class="s2">"/tmp/kubeadm-config-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span>
</code></pre></div></div>

<p>This should be enough to make the Machines healthy!
You should be able to see something similar to this:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">$</span><span class="w"> </span>clusterctl <span class="nt">-n</span> test-1 describe cluster test-1
<span class="go">NAME                                            READY  SEVERITY  REASON  SINCE  MESSAGE
Cluster/test-1                                  True                     46s
├─ClusterInfrastructure - Metal3Cluster/test-1  True                     114m
└─ControlPlane - KubeadmControlPlane/test-1     True                     46s
  └─Machine/test-1-f2nw2                        True                     47s
</span></code></pre></div></div>

<p>However, if you check the KubeadmControlPlane more carefully, you will notice that it is still complaining about control plane components.
This is because we have not created the static pods yet, and it is also unable to check the certificate expiration date for the Machine.
Let’s fix it:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Add static pods to make kubeadm control plane manager happy</span>
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-apiserver-pod.yaml |
  <span class="nb">sed</span> <span class="s2">"s/node-name/</span><span class="k">${</span><span class="nv">machine</span><span class="k">}</span><span class="s2">/g"</span> |
  kubectl <span class="nt">--kubeconfig</span><span class="o">=</span><span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span> create <span class="nt">-f</span> -
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-controller-manager-pod.yaml |
  <span class="nb">sed</span> <span class="s2">"s/node-name/</span><span class="k">${</span><span class="nv">machine</span><span class="k">}</span><span class="s2">/g"</span> |
  kubectl <span class="nt">--kubeconfig</span><span class="o">=</span><span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span> create <span class="nt">-f</span> -
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-scheduler-pod.yaml |
  <span class="nb">sed</span> <span class="s2">"s/node-name/</span><span class="k">${</span><span class="nv">machine</span><span class="k">}</span><span class="s2">/g"</span> |
  kubectl <span class="nt">--kubeconfig</span><span class="o">=</span><span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span> create <span class="nt">-f</span> -
<span class="c"># Set status on the pods (it is not added when using create/apply).</span>
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-apiserver-pod-status.yaml |
  kubectl <span class="nt">--kubeconfig</span><span class="o">=</span><span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span> <span class="nt">-n</span> kube-system patch pod <span class="s2">"kube-apiserver-</span><span class="k">${</span><span class="nv">machine</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="nt">--subresource</span><span class="o">=</span>status <span class="nt">--patch-file</span><span class="o">=</span>/dev/stdin
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-controller-manager-pod-status.yaml |
  kubectl <span class="nt">--kubeconfig</span><span class="o">=</span><span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span> <span class="nt">-n</span> kube-system patch pod <span class="s2">"kube-controller-manager-</span><span class="k">${</span><span class="nv">machine</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="nt">--subresource</span><span class="o">=</span>status <span class="nt">--patch-file</span><span class="o">=</span>/dev/stdin
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-scheduler-pod-status.yaml |
  kubectl <span class="nt">--kubeconfig</span><span class="o">=</span><span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span> <span class="nt">-n</span> kube-system patch pod <span class="s2">"kube-scheduler-</span><span class="k">${</span><span class="nv">machine</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="nt">--subresource</span><span class="o">=</span>status <span class="nt">--patch-file</span><span class="o">=</span>/dev/stdin

<span class="c"># Add certificate expiry annotations to make kubeadm control plane manager happy</span>
<span class="nv">CERT_EXPIRY_ANNOTATION</span><span class="o">=</span><span class="s2">"machine.cluster.x-k8s.io/certificates-expiry"</span>
<span class="nv">EXPIRY_TEXT</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span>kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> get secret apiserver <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.data.tls</span><span class="se">\.</span><span class="s2">crt}"</span> | <span class="nb">base64</span> <span class="nt">-d</span> | openssl x509 <span class="nt">-enddate</span> <span class="nt">-noout</span> | <span class="nb">cut</span> <span class="nt">-d</span><span class="o">=</span> <span class="nt">-f</span> 2<span class="si">)</span><span class="s2">"</span>
<span class="nv">EXPIRY</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span><span class="nb">date</span> <span class="nt">--date</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">EXPIRY_TEXT</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--iso-8601</span><span class="o">=</span>seconds<span class="si">)</span><span class="s2">"</span>
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> annotate machine <span class="s2">"</span><span class="k">${</span><span class="nv">machine</span><span class="k">}</span><span class="s2">"</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CERT_EXPIRY_ANNOTATION</span><span class="k">}</span><span class="s2">=</span><span class="k">${</span><span class="nv">EXPIRY</span><span class="k">}</span><span class="s2">"</span>
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> annotate kubeadmconfig <span class="nt">--all</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CERT_EXPIRY_ANNOTATION</span><span class="k">}</span><span class="s2">=</span><span class="k">${</span><span class="nv">EXPIRY</span><span class="k">}</span><span class="s2">"</span>
</code></pre></div></div>

<p>Now we finally have a completely healthy cluster as far as the controllers are concerned.</p>

<h2 id="conclusions-and-summary">Conclusions and summary</h2>

<p>We now have all the tools necessary to start experimenting.</p>

<ul>
  <li>With the BareMetal Operator running in test mode, we can skip Ironic and still work with BareMetalHosts that act like normal.</li>
  <li>We can set up separate “backing” clusters where we run etcd and multiple API servers to fake the workload cluster API’s.</li>
  <li>Fake Nodes and Pods can be easily added to the workload cluster API’s, and configured as we want.</li>
  <li>The workload cluster API’s can be exposed to the controllers in the test cluster using port-forwarding.</li>
</ul>

<p>In this post we have not automated all of this, but if you want to see a scripted setup, take a look at <a href="https://github.com/Nordix/metal3-clusterapi-docs/tree/main/metal3-scaling-experiments">this</a>.
It is what we used to scale to 1000 clusters.
Just remember that it may need some tweaking for your specific environment if you want to try it out!</p>

<p>Specifically we used 10 “backing” clusters, i.e. 10 separate cloud VMs with kind clusters where we run etcd and the workload cluster API’s.
Each one would hold 100 API servers.
The test cluster was on its own separate VM also running a kind cluster with all the controllers and all the Cluster objects, etc.</p>

<p>In the next and final blog post of this series we will take a look at the results of all this.
What issues did we run into along the way?
How did we fix or work around them?
We will also take a look at what is going on in the community related to this and discuss potential future work in the area.</p>]]></content><author><name>Lennart Jern</name></author><category term="metal3" /><category term="cluster API" /><category term="provider" /><category term="edge" /><summary type="html"><![CDATA[In part 1, we introduced the Bare Metal Operator test mode and saw how it can be used to play with BareMetalHosts without Ironic and without any actual hosts. Now we will take a look at the other end of the stack and how we can fake the workload cluster API’s.]]></summary></entry><entry><title type="html">Scaling to 1000 clusters - Part 1</title><link href="https://metal3.io/blog/2023/05/05/Scaling_part_1.html" rel="alternate" type="text/html" title="Scaling to 1000 clusters - Part 1" /><published>2023-05-05T00:00:00-05:00</published><updated>2023-05-05T00:00:00-05:00</updated><id>https://metal3.io/blog/2023/05/05/Scaling_part_1</id><content type="html" xml:base="https://metal3.io/blog/2023/05/05/Scaling_part_1.html"><![CDATA[<p>We want to ensure that Metal3 can scale to thousands of nodes and clusters.
However, running tests with thousands of real servers is expensive and we don’t have access to any such large environment in the project.
So instead we have been focusing on faking the hardware while trying to keep things as realistic as possible for the controllers.
In this first part we will take a look at the Bare Metal Operator and the <a href="https://github.com/metal3-io/baremetal-operator/blob/b76dde223937009cebb9da85e6f1793a544675e6/docs/dev-setup.md?plain=1#L62">test mode</a> it offers.
The next part will be about how to fake the Kubernetes API of the workload clusters.
In the final post we will take a look at the issues we ran into and what is being done in the community to address them so that we can keep scaling!</p>

<h2 id="some-background-on-how-to-fool-the-controllers">Some background on how to fool the controllers</h2>

<p>With the full Metal3 stack, from Ironic to Cluster API, we have the following controllers that operate on Kubernetes APIs:</p>

<ul>
  <li>Cluster API Kubeadm control plane controller</li>
  <li>Cluster API Kubeadm bootstrap controller</li>
  <li>Cluster API controller</li>
  <li>Cluster API provider for Metal3 controller</li>
  <li>IP address manager controller</li>
  <li>Bare Metal Operator controller</li>
</ul>

<p>We will first focus on the controllers that interact with Nodes, Machines, Metal3Machines and BareMetalHosts, i.e. objects related to actual physical machines that we need to fake.
In other words, we are skipping the IP address manager for now.</p>

<p>What do these controllers care about really?
What do we need to do to fool them?
At the Cluster API level, the controllers just care about the Kubernetes resources in the management cluster (e.g. Clusters and Machines) and some resources in the workload cluster (e.g. Nodes and the etcd Pods).
The controllers will try to connect to the workload clusters in order to check the status of the resources there, so if there is no real workload cluster, this is something we will need to fake if we want to fool the controllers.
When it comes to Cluster API provider for Metal3, it connects the abstract high level objects with the BareMetalHosts, so here we will need to make the BareMetalHosts to behave realistically in order to provide a good test.</p>

<p>This is where the Bare Metal Operator test mode comes in.
If we can fake the workload cluster API and the BareMetalHosts, then all the Cluster API controllers and the Metal3 provider will get a realistic test that we can use when working on scalability.</p>

<h2 id="bare-metal-operator-test-mode">Bare Metal Operator test mode</h2>

<p>The Bare Metal Operator has a test mode, in which it doesn’t talk to Ironic.
Instead it just pretends that everything is fine and all actions succeed.
In this mode the BareMetalHosts will move through the state diagram just like they normally would (but quite a bit faster).
To enable it, all you have to do is add the <code class="language-plaintext highlighter-rouge">-test-mode</code> flag when running the Bare Metal Operator controller.
For convenience there is also a make target (<code class="language-plaintext highlighter-rouge">make run-test-mode</code>) that will run the Bare Metal Operator directly on the host in test mode.</p>

<p>Here is an example of how to use it.
You will need kind and kubectl installed for this to work, but you don’t need the Bare Metal Operator repository cloned.</p>

<ol>
  <li>
    <p>Create a kind cluster and deploy cert-manager (needed for web hook certificates):</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>kind create cluster
<span class="c"># Install cert-manager</span>
kubectl apply <span class="nt">-f</span> https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/cert-manager.yaml
</code></pre></div>    </div>
  </li>
  <li>
    <p>Deploy the Bare Metal Operator in test mode:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Create the namespace where it will run</span>
kubectl create ns baremetal-operator-system
<span class="c"># Deploy it in normal mode</span>
kubectl apply <span class="nt">-k</span> https://github.com/metal3-io/baremetal-operator/config/default
<span class="c"># Patch it to run in test mode</span>
kubectl patch <span class="nt">-n</span> baremetal-operator-system deploy baremetal-operator-controller-manager <span class="nt">--type</span><span class="o">=</span>json <span class="se">\</span>
  <span class="nt">-p</span><span class="o">=</span><span class="s1">'[{"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--test-mode"}]'</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>In a separate terminal, create a BareMetalHost from the example manifests:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>kubectl apply <span class="nt">-f</span> https://github.com/metal3-io/baremetal-operator/raw/main/examples/example-host.yaml
</code></pre></div>    </div>
  </li>
</ol>

<p>After applying the BareMetalHost, it will quickly go through <code class="language-plaintext highlighter-rouge">registering</code> and become <code class="language-plaintext highlighter-rouge">available</code>.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">$</span><span class="w"> </span>kubectl get bmh
<span class="go">NAME                    STATE         CONSUMER   ONLINE   ERROR   AGE
example-baremetalhost   registering              true             2s
</span><span class="gp">$</span><span class="w"> </span>kubectl get bmh
<span class="go">NAME                    STATE       CONSUMER   ONLINE   ERROR   AGE
example-baremetalhost   available              true             6s
</span></code></pre></div></div>

<p>We can now provision the BareMetalHost, turn it off, deprovision, etc.
Just like normal, except that the machine doesn’t exist.
Let’s try provisioning it!</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>kubectl patch bmh example-baremetalhost <span class="nt">--type</span><span class="o">=</span>merge <span class="nt">--patch-file</span><span class="o">=</span>/dev/stdin <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
spec:
  image:
    url: "http://example.com/totally-fake-image.vmdk"
    checksum: "made-up-checksum"
    format: vmdk
</span><span class="no">EOF
</span></code></pre></div></div>

<p>You will see it go through <code class="language-plaintext highlighter-rouge">provisioning</code> and end up in <code class="language-plaintext highlighter-rouge">provisioned</code> state:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">$</span><span class="w"> </span>kubectl get bmh
<span class="go">NAME                    STATE          CONSUMER   ONLINE   ERROR   AGE
example-baremetalhost   provisioning              true             7m20s

</span><span class="gp">$</span><span class="w"> </span>kubectl get bmh
<span class="go">NAME                    STATE         CONSUMER   ONLINE   ERROR   AGE
example-baremetalhost   provisioned              true             7m22s
</span></code></pre></div></div>

<h2 id="wrapping-up">Wrapping up</h2>

<p>With Bare Metal Operator in test mode, we have the foundation for starting our scalability journey.
We can easily create BareMetalHost objects and they behave similar to what they would in a real scenario.
A simple bash script will at this point allow us to create as many BareMetalHosts as we would like.
To wrap things up, we will now do just that: put together a script and try generating a few BareMetalHosts.</p>

<p>The script will do the same thing we did before when creating the example BareMetalHost, but it will also give them different names so we don’t get naming collisions.
Here it is:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c">#!/usr/bin/env bash</span>

<span class="nb">set</span> <span class="nt">-eu</span>

create_bmhs<span class="o">()</span> <span class="o">{</span>
  <span class="nv">n</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">1</span><span class="k">}</span><span class="s2">"</span>
  <span class="k">for</span> <span class="o">((</span> i <span class="o">=</span> 1<span class="p">;</span> i &lt;<span class="o">=</span> n<span class="p">;</span> ++i <span class="o">))</span><span class="p">;</span> <span class="k">do
    </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh">
---
apiVersion: v1
kind: Secret
metadata:
  name: worker-</span><span class="nv">$i</span><span class="sh">-bmc-secret
type: Opaque
data:
  username: YWRtaW4=
  password: cGFzc3dvcmQ=
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: worker-</span><span class="nv">$i</span><span class="sh">
spec:
  online: true
  bmc:
    address: libvirt://192.168.122.</span><span class="nv">$i</span><span class="sh">:6233/
    credentialsName: worker-</span><span class="nv">$i</span><span class="sh">-bmc-secret
  bootMACAddress: "</span><span class="si">$(</span><span class="nb">printf</span> <span class="s1">'00:60:2F:%02X:%02X:%02X\n'</span> <span class="k">$((</span>RANDOM%256<span class="k">))</span> <span class="k">$((</span>RANDOM%256<span class="k">))</span> <span class="k">$((</span>RANDOM%256<span class="k">))</span><span class="si">)</span><span class="sh">"
</span><span class="no">EOF
</span>  <span class="k">done</span>
<span class="o">}</span>

<span class="nv">NUM</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">1</span><span class="k">:-</span><span class="nv">10</span><span class="k">}</span><span class="s2">"</span>

create_bmhs <span class="s2">"</span><span class="k">${</span><span class="nv">NUM</span><span class="k">}</span><span class="s2">"</span>
</code></pre></div></div>

<p>Save it as <code class="language-plaintext highlighter-rouge">produce-available-hosts.sh</code> and try it out:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">$</span><span class="w"> </span>./produce-available-hosts.sh 10 | kubectl apply <span class="nt">-f</span> -
<span class="go">secret/worker-1-bmc-secret created
baremetalhost.metal3.io/worker-1 created
secret/worker-2-bmc-secret created
baremetalhost.metal3.io/worker-2 created
secret/worker-3-bmc-secret created
baremetalhost.metal3.io/worker-3 created
secret/worker-4-bmc-secret created
baremetalhost.metal3.io/worker-4 created
secret/worker-5-bmc-secret created
baremetalhost.metal3.io/worker-5 created
secret/worker-6-bmc-secret created
baremetalhost.metal3.io/worker-6 created
secret/worker-7-bmc-secret created
baremetalhost.metal3.io/worker-7 created
secret/worker-8-bmc-secret created
baremetalhost.metal3.io/worker-8 created
secret/worker-9-bmc-secret created
baremetalhost.metal3.io/worker-9 created
secret/worker-10-bmc-secret created
baremetalhost.metal3.io/worker-10 created
</span><span class="gp">$</span><span class="w"> </span>kubectl get bmh
<span class="go">NAME        STATE         CONSUMER   ONLINE   ERROR   AGE
worker-1    registering              true             2s
worker-10   available                true             2s
worker-2    available                true             2s
worker-3    available                true             2s
worker-4    available                true             2s
worker-5    available                true             2s
worker-6    registering              true             2s
worker-7    available                true             2s
worker-8    available                true             2s
worker-9    available                true             2s
</span></code></pre></div></div>

<p>With this we conclude the first part of the scaling series.
In the next post, we will take a look at how to fake the other end of the stack: the workload cluster API.</p>]]></content><author><name>Lennart Jern</name></author><category term="metal3" /><category term="cluster API" /><category term="provider" /><category term="edge" /><summary type="html"><![CDATA[We want to ensure that Metal3 can scale to thousands of nodes and clusters. However, running tests with thousands of real servers is expensive and we don’t have access to any such large environment in the project. So instead we have been focusing on faking the hardware while trying to keep things as realistic as possible for the controllers. In this first part we will take a look at the Bare Metal Operator and the test mode it offers. The next part will be about how to fake the Kubernetes API of the workload clusters. In the final post we will take a look at the issues we ran into and what is being done in the community to address them so that we can keep scaling!]]></summary></entry><entry><title type="html">One cluster - multiple providers</title><link href="https://metal3.io/blog/2022/07/08/One_cluster_multiple_providers.html" rel="alternate" type="text/html" title="One cluster - multiple providers" /><published>2022-07-08T00:00:00-05:00</published><updated>2022-07-08T00:00:00-05:00</updated><id>https://metal3.io/blog/2022/07/08/One_cluster_multiple_providers</id><content type="html" xml:base="https://metal3.io/blog/2022/07/08/One_cluster_multiple_providers.html"><![CDATA[<p>Running on bare metal has both benefits and drawbacks. You can get the
best performance possible out of the hardware, but it can also be quite
expensive and maybe not necessary for <em>all</em> workloads. Perhaps a hybrid
cluster could give you the best of both? Raw power for the workload that
needs it, and cheap virtualized commodity for the rest. This blog post
will show how to set up a cluster like this using the Cluster API backed
by the Metal3 and BYOH providers.</p>

<h2 id="the-problem">The problem</h2>

<p>Imagine that you have some bare metal servers that you want to use for
some specific workload. Maybe the workload benefits from the specific
hardware or there are some requirements that make it necessary to run it
there. The rest of the organization already uses Kubernetes and the
cluster API everywhere so of course you want the same for this as well.
Perfect, grab Metal³ and start working!</p>

<p>But hold on, this would mean that you use some of the servers for
running the Kubernetes control plane and possibly all the cluster API
controllers. If there are enough servers this is probably not an issue,
but do you really want to “waste” these servers on such generic
workloads that could be running anywhere? This can become especially
painful if you need multiple control plane nodes. Each server is
probably powerful enough to run all the control planes and controllers,
but it would be a single point of failure…</p>

<p>What if there was a way to use a different cluster API infrastructure
provider for some nodes? For example, use the Openstack infrastructure
provider for the control plane and Metal³ for the workers. Let’s do an
experiment!</p>

<h2 id="setting-up-the-experiment-environment">Setting up the experiment environment</h2>

<p>This blog post will use the <a href="https://github.com/vmware-tanzu/cluster-api-provider-bringyourownhost">Bring your own
host</a>
(BYOH) provider together with Metal³ as a proof of concept to show what
is currently possible.</p>

<p>The BYOH provider was chosen as the second provider for two reasons:</p>

<ol>
  <li>Due to its design (you provision the host yourself), it is very easy
to adapt it to the test (e.g. use a VM in the same network that the
metal3-dev-env uses).</li>
  <li>It is one of the providers that is known to work when combining
multiple providers for a single cluster.</li>
</ol>

<p>We will be using the
<a href="https://github.com/metal3-io/metal3-dev-env">metal3-dev-env</a> on Ubuntu
as a starting point for this experiment. Note that it makes substantial
changes to the machine where it is running, so you may want to use a
dedicated lab machine instead of your laptop for this. If you have not
done so already, clone it and run <code class="language-plaintext highlighter-rouge">make</code>. This should give you a
management cluster with the Metal³ provider installed and two
BareMetalHosts ready for provisioning.</p>

<p>The next step is to add the BYOH provider and a ByoHost.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>clusterctl init <span class="nt">--infrastructure</span> byoh
</code></pre></div></div>

<p>For the ByoHost we will use Vagrant.
You can install it with <code class="language-plaintext highlighter-rouge">sudo apt install vagrant</code>.
Then copy the Vagrantfile below to a new folder and run <code class="language-plaintext highlighter-rouge">vagrant up</code>.</p>

<pre><code class="language-Vagrantfile"># -*- mode: ruby -*-
hosts = {
    "control-plane1" =&gt; { "memory" =&gt; 2048, "ip" =&gt; "192.168.10.10"},
    # "control-plane2" =&gt; { "memory" =&gt; 2048, "ip" =&gt; "192.168.10.11"},
    # "control-plane3" =&gt; { "memory" =&gt; 2048, "ip" =&gt; "192.168.10.12"},
}


Vagrant.configure("2") do |config|
    # Choose which box you want below
    config.vm.box = "generic/ubuntu2004"
    config.vm.synced_folder ".", "/vagrant", disabled: true
    config.vm.provider :libvirt do |libvirt|
      # QEMU system connection is required for private network configuration
      libvirt.qemu_use_session = false
    end


    # Loop over all machine names
    hosts.each_key do |host|
        config.vm.define host, primary: host == hosts.keys.first do |node|
            node.vm.hostname = host
            node.vm.network :private_network, ip: hosts[host]["ip"],
              libvirt__forward_mode: "route"
            node.vm.provider :libvirt do |lv|
                lv.memory = hosts[host]["memory"]
                lv.cpus = 2
            end
        end
    end
end
</code></pre>

<p>Vagrant should now have created a new VM to use as a ByoHost. Now we
just need to run the BYOH agent in the VM to make it register as a
ByoHost in the management cluster. The BYOH agent needs a kubeconfig
file to do this, so we start by copying it to the VM:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nb">cp</span> ~/.kube/config ~/.kube/management-cluster.conf
<span class="c"># Ensure that the correct IP is used (not localhost)</span>
<span class="nb">export </span><span class="nv">KIND_IP</span><span class="o">=</span><span class="si">$(</span>docker inspect <span class="nt">-f</span> <span class="s1">'{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'</span> kind-control-plane<span class="si">)</span>
<span class="nb">sed</span> <span class="nt">-i</span> <span class="s1">'s/    server\:.*/    server\: https\:\/\/'</span><span class="s2">"</span><span class="nv">$KIND_IP</span><span class="s2">"</span><span class="s1">'\:6443/g'</span> ~/.kube/management-cluster.conf
scp <span class="nt">-i</span> .vagrant/machines/control-plane1/libvirt/private_key <span class="se">\</span>
  /home/ubuntu/.kube/management-cluster.conf vagrant@192.168.10.10:management-cluster.conf

</code></pre></div></div>

<p>Next, install the prerequisites and host agent in the VM and run it.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>vagrant ssh
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> socat ebtables ethtool conntrack
wget https://github.com/vmware-tanzu/cluster-api-provider-bringyourownhost/releases/download/v0.2.0/byoh-hostagent-linux-amd64
<span class="nb">mv </span>byoh-hostagent-linux-amd64 byoh-hostagent
<span class="nb">chmod</span> +x byoh-hostagent
<span class="nb">sudo</span> ./byoh-hostagent <span class="nt">--namespace</span> metal3 <span class="nt">--kubeconfig</span> management-cluster.conf
</code></pre></div></div>

<p>You should now have a management cluster with both the Metal³ and BYOH
providers installed, as well as two BareMetalHosts and one ByoHost.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">$</span><span class="w"> </span>kubectl <span class="nt">-n</span> metal3 get baremetalhosts,byohosts
<span class="go">NAME                             STATE       CONSUMER   ONLINE   ERROR   AGE
baremetalhost.metal3.io/node-0   available              true             18m
baremetalhost.metal3.io/node-1   available              true             18m


NAME                                                     AGE
byohost.infrastructure.cluster.x-k8s.io/control-plane1   73s
</span></code></pre></div></div>

<h2 id="creating-a-multi-provider-cluster">Creating a multi-provider cluster</h2>

<p>The trick is to create both a Metal3Cluster and a ByoCluster that are
owned by one common Cluster. We will use the ByoCluster for the control
plane in this case. First the Cluster:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">cluster.x-k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Cluster</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">cni</span><span class="pi">:</span> <span class="s">mixed-cluster-crs-0</span>
    <span class="na">crs</span><span class="pi">:</span> <span class="s2">"</span><span class="s">true"</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">clusterNetwork</span><span class="pi">:</span>
    <span class="na">pods</span><span class="pi">:</span>
      <span class="na">cidrBlocks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">192.168.0.0/16</span>
    <span class="na">serviceDomain</span><span class="pi">:</span> <span class="s">cluster.local</span>
    <span class="na">services</span><span class="pi">:</span>
      <span class="na">cidrBlocks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">10.128.0.0/12</span>
  <span class="na">controlPlaneRef</span><span class="pi">:</span>
    <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">controlplane.cluster.x-k8s.io/v1beta1</span>
    <span class="na">kind</span><span class="pi">:</span> <span class="s">KubeadmControlPlane</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">mixed-cluster-control-plane</span>
  <span class="na">infrastructureRef</span><span class="pi">:</span>
    <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1beta1</span>
    <span class="na">kind</span><span class="pi">:</span> <span class="s">ByoCluster</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
</code></pre></div></div>

<p>Add the rest of the BYOH manifests to get a control plane.
The code is collapsed here for easier reading.
Please click on the line below to expand it.</p>

<!-- markdownlint-disable MD033 -->

<details>
  <summary>KubeadmControlPlane, ByoCluster and ByoMachineTemplate</summary>
  <!-- Enable markdown parsing of the content. -->
  <div>

    <!-- markdownlint-enable MD033 -->

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c1">#{% raw %}</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">controlplane.cluster.x-k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">KubeadmControlPlane</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">nodepool</span><span class="pi">:</span> <span class="s">pool0</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mixed-cluster-control-plane</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">kubeadmConfigSpec</span><span class="pi">:</span>
    <span class="na">clusterConfiguration</span><span class="pi">:</span>
      <span class="na">apiServer</span><span class="pi">:</span>
        <span class="na">certSANs</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">localhost</span>
        <span class="pi">-</span> <span class="s">127.0.0.1</span>
        <span class="pi">-</span> <span class="s">0.0.0.0</span>
        <span class="pi">-</span> <span class="s">host.docker.internal</span>
      <span class="na">controllerManager</span><span class="pi">:</span>
        <span class="na">extraArgs</span><span class="pi">:</span>
          <span class="na">enable-hostpath-provisioner</span><span class="pi">:</span> <span class="s2">"</span><span class="s">true"</span>
    <span class="na">files</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">content</span><span class="pi">:</span> <span class="pi">|</span>
        <span class="s">apiVersion: v1</span>
        <span class="s">kind: Pod</span>
        <span class="s">metadata:</span>
          <span class="s">creationTimestamp: null</span>
          <span class="s">name: kube-vip</span>
          <span class="s">namespace: kube-system</span>
        <span class="s">spec:</span>
          <span class="s">containers:</span>
          <span class="s">- args:</span>
            <span class="s">- start</span>
            <span class="s">env:</span>
            <span class="s">- name: vip_arp</span>
              <span class="s">value: "true"</span>
            <span class="s">- name: vip_leaderelection</span>
              <span class="s">value: "true"</span>
            <span class="s">- name: vip_address</span>
              <span class="s">value: 192.168.10.20</span>
            <span class="s">- name: vip_interface</span>
              <span class="s">value: {{ .DefaultNetworkInterfaceName }}</span>
            <span class="s">- name: vip_leaseduration</span>
              <span class="s">value: "15"</span>
            <span class="s">- name: vip_renewdeadline</span>
              <span class="s">value: "10"</span>
            <span class="s">- name: vip_retryperiod</span>
              <span class="s">value: "2"</span>
            <span class="s">image: ghcr.io/kube-vip/kube-vip:v0.3.5</span>
            <span class="s">imagePullPolicy: IfNotPresent</span>
            <span class="s">name: kube-vip</span>
            <span class="s">resources: {}</span>
            <span class="s">securityContext:</span>
              <span class="s">capabilities:</span>
                <span class="s">add:</span>
                <span class="s">- NET_ADMIN</span>
                <span class="s">- SYS_TIME</span>
            <span class="s">volumeMounts:</span>
            <span class="s">- mountPath: /etc/kubernetes/admin.conf</span>
              <span class="s">name: kubeconfig</span>
          <span class="s">hostNetwork: true</span>
          <span class="s">volumes:</span>
          <span class="s">- hostPath:</span>
              <span class="s">path: /etc/kubernetes/admin.conf</span>
              <span class="s">type: FileOrCreate</span>
            <span class="s">name: kubeconfig</span>
        <span class="s">status: {}</span>
        <span class="s">owner: root:root</span>
        <span class="s">path: /etc/kubernetes/manifests/kube-vip.yaml</span>
    <span class="na">initConfiguration</span><span class="pi">:</span>
      <span class="na">nodeRegistration</span><span class="pi">:</span>
        <span class="na">criSocket</span><span class="pi">:</span> <span class="s">/var/run/containerd/containerd.sock</span>
        <span class="na">ignorePreflightErrors</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">Swap</span>
        <span class="pi">-</span> <span class="s">DirAvailable--etc-kubernetes-manifests</span>
        <span class="pi">-</span> <span class="s">FileAvailable--etc-kubernetes-kubelet.conf</span>
    <span class="na">joinConfiguration</span><span class="pi">:</span>
      <span class="na">nodeRegistration</span><span class="pi">:</span>
        <span class="na">criSocket</span><span class="pi">:</span> <span class="s">/var/run/containerd/containerd.sock</span>
        <span class="na">ignorePreflightErrors</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">Swap</span>
        <span class="pi">-</span> <span class="s">DirAvailable--etc-kubernetes-manifests</span>
        <span class="pi">-</span> <span class="s">FileAvailable--etc-kubernetes-kubelet.conf</span>
  <span class="na">machineTemplate</span><span class="pi">:</span>
    <span class="na">infrastructureRef</span><span class="pi">:</span>
      <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1beta1</span>
      <span class="na">kind</span><span class="pi">:</span> <span class="s">ByoMachineTemplate</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">mixed-cluster-control-plane</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">version</span><span class="pi">:</span> <span class="s">v1.23.5</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ByoCluster</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">bundleLookupBaseRegistry</span><span class="pi">:</span> <span class="s">projects.registry.vmware.com/cluster_api_provider_bringyourownhost</span>
  <span class="na">bundleLookupTag</span><span class="pi">:</span> <span class="s">v1.23.5</span>
  <span class="na">controlPlaneEndpoint</span><span class="pi">:</span>
    <span class="na">host</span><span class="pi">:</span> <span class="s">192.168.10.20</span>
    <span class="na">port</span><span class="pi">:</span> <span class="m">6443</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ByoMachineTemplate</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mixed-cluster-control-plane</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span> <span class="pi">{}</span>
<span class="c1">#</span>
</code></pre></div>    </div>

  </div>
</details>

<p>So far this is a “normal” Cluster backed by the BYOH provider. But now
it is time to do something different. Instead of adding more ByoHosts as
workers, we will add a Metal3Cluster and MachineDeployment backed by
BareMetalHosts! Note that the <code class="language-plaintext highlighter-rouge">controlPlaneEndpoint</code> of the
Metal3Cluster must point to the same endpoint that the ByoCluster is
using.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Metal3Cluster</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">controlPlaneEndpoint</span><span class="pi">:</span>
    <span class="na">host</span><span class="pi">:</span> <span class="s">192.168.10.20</span>
    <span class="na">port</span><span class="pi">:</span> <span class="m">6443</span>
  <span class="na">noCloudProvider</span><span class="pi">:</span> <span class="no">true</span>
</code></pre></div></div>

<!-- markdownlint-disable MD033 -->

<details>
  <summary>IPPools</summary>
  <div>

    <!-- markdownlint-enable MD033 -->

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">ipam.metal3.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">IPPool</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">provisioning-pool</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">clusterName</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
  <span class="na">namePrefix</span><span class="pi">:</span> <span class="s">test1-prov</span>
  <span class="na">pools</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">end</span><span class="pi">:</span> <span class="s">172.22.0.200</span>
    <span class="na">start</span><span class="pi">:</span> <span class="s">172.22.0.100</span>
  <span class="na">prefix</span><span class="pi">:</span> <span class="m">24</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">ipam.metal3.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">IPPool</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">baremetalv4-pool</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">clusterName</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
  <span class="na">gateway</span><span class="pi">:</span> <span class="s">192.168.111.1</span>
  <span class="na">namePrefix</span><span class="pi">:</span> <span class="s">test1-bmv4</span>
  <span class="na">pools</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">end</span><span class="pi">:</span> <span class="s">192.168.111.200</span>
    <span class="na">start</span><span class="pi">:</span> <span class="s">192.168.111.100</span>
  <span class="na">prefix</span><span class="pi">:</span> <span class="m">24</span>
</code></pre></div>    </div>

  </div>
</details>

<p>These manifests are quite large but they are just the same as would be
used by the metal3-dev-env with some name changes here and there. The
key thing to note is that all references to a Cluster are to the one we
defined above. Here is the MachineDeployment:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">cluster.x-k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">MachineDeployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">cluster.x-k8s.io/cluster-name</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
    <span class="na">nodepool</span><span class="pi">:</span> <span class="s">nodepool-0</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">test1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">clusterName</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">cluster.x-k8s.io/cluster-name</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
      <span class="na">nodepool</span><span class="pi">:</span> <span class="s">nodepool-0</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">cluster.x-k8s.io/cluster-name</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
        <span class="na">nodepool</span><span class="pi">:</span> <span class="s">nodepool-0</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">bootstrap</span><span class="pi">:</span>
        <span class="na">configRef</span><span class="pi">:</span>
          <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">bootstrap.cluster.x-k8s.io/v1beta1</span>
          <span class="na">kind</span><span class="pi">:</span> <span class="s">KubeadmConfigTemplate</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">test1-workers</span>
      <span class="na">clusterName</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
      <span class="na">infrastructureRef</span><span class="pi">:</span>
        <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1beta1</span>
        <span class="na">kind</span><span class="pi">:</span> <span class="s">Metal3MachineTemplate</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">test1-workers</span>
      <span class="na">nodeDrainTimeout</span><span class="pi">:</span> <span class="s">0s</span>
      <span class="na">version</span><span class="pi">:</span> <span class="s">v1.23.5</span>
</code></pre></div></div>

<p>Finally, we add the Metal3MachineTemplate, Metal3DataTemplate and
KubeadmConfigTemplate. Here you may want to add your public ssh key in
the KubeadmConfigTemplate (the last few lines).</p>

<!-- markdownlint-disable MD033 -->

<details>
  <summary>Metal3MachineTemplate, Metal3DataTemplate and KubeadmConfigTemplate</summary>
  <!-- Enable markdown parsing of the content. -->
  <div>

    <!-- markdownlint-enable MD033 -->

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c1">#</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Metal3MachineTemplate</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">test1-workers</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">dataTemplate</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">test1-workers-template</span>
      <span class="na">image</span><span class="pi">:</span>
        <span class="na">checksum</span><span class="pi">:</span> <span class="s">http://172.22.0.1/images/UBUNTU_22.04_NODE_IMAGE_K8S_v1.23.5-raw.img.md5sum</span>
        <span class="na">checksumType</span><span class="pi">:</span> <span class="s">md5</span>
        <span class="na">format</span><span class="pi">:</span> <span class="s">raw</span>
        <span class="na">url</span><span class="pi">:</span> <span class="s">http://172.22.0.1/images/UBUNTU_22.04_NODE_IMAGE_K8S_v1.23.5-raw.img</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Metal3DataTemplate</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">test1-workers-template</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">metal3</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">clusterName</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
  <span class="na">metaData</span><span class="pi">:</span>
    <span class="na">ipAddressesFromIPPool</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">provisioningIP</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">provisioning-pool</span>
    <span class="na">objectNames</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">name</span>
      <span class="na">object</span><span class="pi">:</span> <span class="s">machine</span>
    <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">local-hostname</span>
      <span class="na">object</span><span class="pi">:</span> <span class="s">machine</span>
    <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">local_hostname</span>
      <span class="na">object</span><span class="pi">:</span> <span class="s">machine</span>
    <span class="na">prefixesFromIPPool</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">provisioningCIDR</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">provisioning-pool</span>
  <span class="na">networkData</span><span class="pi">:</span>
    <span class="na">links</span><span class="pi">:</span>
      <span class="na">ethernets</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">id</span><span class="pi">:</span> <span class="s">enp1s0</span>
        <span class="na">macAddress</span><span class="pi">:</span>
          <span class="na">fromHostInterface</span><span class="pi">:</span> <span class="s">enp1s0</span>
        <span class="na">type</span><span class="pi">:</span> <span class="s">phy</span>
      <span class="pi">-</span> <span class="na">id</span><span class="pi">:</span> <span class="s">enp2s0</span>
        <span class="na">macAddress</span><span class="pi">:</span>
          <span class="na">fromHostInterface</span><span class="pi">:</span> <span class="s">enp2s0</span>
        <span class="na">type</span><span class="pi">:</span> <span class="s">phy</span>
    <span class="na">networks</span><span class="pi">:</span>
      <span class="na">ipv4</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">id</span><span class="pi">:</span> <span class="s">baremetalv4</span>
        <span class="na">ipAddressFromIPPool</span><span class="pi">:</span> <span class="s">baremetalv4-pool</span>
        <span class="na">link</span><span class="pi">:</span> <span class="s">enp2s0</span>
        <span class="na">routes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">gateway</span><span class="pi">:</span>
            <span class="na">fromIPPool</span><span class="pi">:</span> <span class="s">baremetalv4-pool</span>
          <span class="na">network</span><span class="pi">:</span> <span class="s">0.0.0.0</span>
          <span class="na">prefix</span><span class="pi">:</span> <span class="m">0</span>
    <span class="na">services</span><span class="pi">:</span>
      <span class="na">dns</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">8.8.8.8</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">bootstrap.cluster.x-k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">KubeadmConfigTemplate</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">test1-workers</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">files</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">content</span><span class="pi">:</span> <span class="pi">|</span>
          <span class="s">network:</span>
            <span class="s">version: 2</span>
            <span class="s">renderer: networkd</span>
            <span class="s">bridges:</span>
              <span class="s">ironicendpoint:</span>
                <span class="s">interfaces: [enp1s0]</span>
                <span class="s">addresses:</span>
                <span class="s">- {{ ds.meta_data.provisioningIP }}/{{ ds.meta_data.provisioningCIDR }}</span>
        <span class="na">owner</span><span class="pi">:</span> <span class="s">root:root</span>
        <span class="na">path</span><span class="pi">:</span> <span class="s">/etc/netplan/52-ironicendpoint.yaml</span>
        <span class="na">permissions</span><span class="pi">:</span> <span class="s2">"</span><span class="s">0644"</span>
      <span class="pi">-</span> <span class="na">content</span><span class="pi">:</span> <span class="pi">|</span>
          <span class="s">[registries.search]</span>
          <span class="s">registries = ['docker.io']</span>
          <span class="s">[registries.insecure]</span>
          <span class="s">registries = ['192.168.111.1:5000']</span>
        <span class="na">path</span><span class="pi">:</span> <span class="s">/etc/containers/registries.conf</span>
      <span class="na">joinConfiguration</span><span class="pi">:</span>
        <span class="na">nodeRegistration</span><span class="pi">:</span>
          <span class="na">kubeletExtraArgs</span><span class="pi">:</span>
            <span class="na">cgroup-driver</span><span class="pi">:</span> <span class="s">systemd</span>
            <span class="na">container-runtime</span><span class="pi">:</span> <span class="s">remote</span>
            <span class="na">container-runtime-endpoint</span><span class="pi">:</span> <span class="s">unix:///var/run/crio/crio.sock</span>
            <span class="na">feature-gates</span><span class="pi">:</span> <span class="s">AllAlpha=false</span>
            <span class="na">node-labels</span><span class="pi">:</span> <span class="s">metal3.io/uuid={{ ds.meta_data.uuid }}</span>
            <span class="na">provider-id</span><span class="pi">:</span> <span class="s">metal3://{{ ds.meta_data.uuid }}</span>
            <span class="na">runtime-request-timeout</span><span class="pi">:</span> <span class="s">5m</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s2">"</span><span class="s">{{</span><span class="nv"> </span><span class="s">ds.meta_data.name</span><span class="nv"> </span><span class="s">}}"</span>
      <span class="na">preKubeadmCommands</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">netplan apply</span>
      <span class="pi">-</span> <span class="s">systemctl enable --now crio kubelet</span>
      <span class="na">users</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">metal3</span>
        <span class="c1"># sshAuthorizedKeys:</span>
        <span class="c1"># - add your public key here for debugging</span>
        <span class="na">sudo</span><span class="pi">:</span> <span class="s">ALL=(ALL) NOPASSWD:ALL</span>
<span class="c1">#</span>
</code></pre></div>    </div>

  </div>
</details>

<p>The result of all this is a Cluster with two Machines, one from the
Metal³ provider and one from the BYOH provider.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">$</span><span class="w"> </span>k <span class="nt">-n</span> metal3 get machine
<span class="go">NAME                                CLUSTER         NODENAME                PROVIDERID                                      PHASE     AGE     VERSION
mixed-cluster-control-plane-48qmm   mixed-cluster   control-plane1          byoh://control-plane1/jf5uye                    Running   7m41s   v1.23.5
test1-8767dbccd-24cl5               mixed-cluster   test1-8767dbccd-24cl5   metal3://0642d832-3a7c-4ce9-833e-a629a60a455c   Running   7m18s   v1.23.5
</span></code></pre></div></div>

<p>Let’s also check that the workload cluster is functioning as expected.
Get the kubeconfig and add Calico as CNI.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>clusterctl get kubeconfig <span class="nt">-n</span> metal3 mixed-cluster <span class="o">&gt;</span> kubeconfig.yaml
<span class="nb">export </span><span class="nv">KUBECONFIG</span><span class="o">=</span>kubeconfig.yaml
kubectl apply <span class="nt">-f</span> https://docs.projectcalico.org/v3.20/manifests/calico.yaml
</code></pre></div></div>

<p>Now check the nodes.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">$</span><span class="w"> </span>kubectl get nodes
<span class="go">NAME                    STATUS   ROLES                  AGE   VERSION
control-plane1          Ready    control-plane,master   88m   v1.23.5
</span><span class="gp">test1-8767dbccd-24cl5   Ready    &lt;none&gt;</span><span class="w">                 </span>82m   v1.23.5
</code></pre></div></div>

<p>Going back to the management cluster, we can inspect the state of the
cluster API resources.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">$</span><span class="w"> </span>clusterctl <span class="nt">-n</span> metal3 describe cluster mixed-cluster
<span class="go">NAME                                                                        READY  SEVERITY  REASON  SINCE  MESSAGE
Cluster/mixed-cluster                                                       True                     13m
├─ClusterInfrastructure - ByoCluster/mixed-cluster
├─ControlPlane - KubeadmControlPlane/mixed-cluster-control-plane            True                     13m
│ └─Machine/mixed-cluster-control-plane-hp2fp                               True                     13m
│   └─MachineInfrastructure - ByoMachine/mixed-cluster-control-plane-vxft5
└─Workers
  └─MachineDeployment/test1                                                 True                     3m57s
    └─Machine/test1-7f77dfb7c8-j7x4q                                        True                     9m32s
</span></code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>

<p>As we have seen in this post, it is possible to combine at least some
infrastructure providers when creating a single cluster. This can be
useful for example if a provider has a high cost or limited resources.
Furthermore, the use case is not addressed by MachineDeployments since
they would all be from the same provider (even though they can have
different properties).</p>

<p>There is some room for development and improvement though. The most
obvious thing is perhaps that Clusters only have one
<code class="language-plaintext highlighter-rouge">infrastructureRef</code>. This means that the cluster API controllers are not
aware of the “secondary” infrastructure provider(s).</p>

<p>Another thing that may be less obvious is the reliance on Nodes and
Machines in the Kubeadm control plane provider. It is not an issue in
the example we have seen here since both Metal³ and BYOH creates Nodes.
However, there are some projects where Nodes are unnecessary. See for
example <a href="https://github.com/clastix/kamaji">Kamaji</a>, which aims to
integrate with the cluster API. The idea here is to run the control
plane components in the management cluster as Pods. Naturally, there
would not be any control plane Nodes or Machines in this case. (A second
provider would be used to add workers.) But the Kubeadm control plane
provider expects there to be both Machines and Nodes for the control
plane, so a new provider is likely needed to make this work as desired.</p>

<p>This issue can already be seen in the
<a href="https://github.com/loft-sh/cluster-api-provider-vcluster">vcluster</a>
provider, where the Cluster stays in <code class="language-plaintext highlighter-rouge">Provisioning</code> state because it is
“Waiting for the first control plane machine to have its
<code class="language-plaintext highlighter-rouge">status.nodeRef</code> set”. The idea with vcluster is to reuse the Nodes of
the management cluster but provide a separate control plane. This gives
users better isolation than just namespaces without the need for another
“real” cluster. It is for example possible to have different custom
resource definitions in each vcluster. But since vcluster runs all the
pods (including the control plane) in the management cluster, there will
never be a control plane Machine or <code class="language-plaintext highlighter-rouge">nodeRef</code>.</p>

<p>There is already one implementation of a control plane provider without
Nodes, i.e. the EKS provider. Perhaps this is the way forward. One
implementation for each specific case. It would be nice if it was
possible to do it in a more generic way though, similar to how the
Kubeadm control plane provider is used by almost all infrastructure
providers.</p>

<p>To summarize, there is already some support for mixed clusters with
multiple providers. However, there are some issues that make it
unnecessarily awkward. Two things that could be improved in the cluster
API would be the following:</p>

<ol>
  <li>Make the <code class="language-plaintext highlighter-rouge">cluster.infrastructureRef</code> into a list to allow multiple
infrastructure providers to be registered.</li>
  <li>Drop the assumption that there will always be control plane Machines
and Nodes (e.g. by implementing a new control plane provider).</li>
</ol>]]></content><author><name>Lennart Jern</name></author><category term="metal3" /><category term="cluster API" /><category term="provider" /><category term="hybrid" /><category term="edge" /><summary type="html"><![CDATA[Running on bare metal has both benefits and drawbacks. You can get the best performance possible out of the hardware, but it can also be quite expensive and maybe not necessary for all workloads. Perhaps a hybrid cluster could give you the best of both? Raw power for the workload that needs it, and cheap virtualized commodity for the rest. This blog post will show how to set up a cluster like this using the Cluster API backed by the Metal3 and BYOH providers.]]></summary></entry></feed>