<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="https://metal3.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://metal3.io/" rel="alternate" type="text/html" /><updated>2025-10-31T19:27:28-05:00</updated><id>https://metal3.io/feed.xml</id><title type="html">Metal³ - Metal Kubed</title><subtitle>Metal3.io aims to build on baremetal host provisioning technologies to provide a Kubernetes native API for managing bare metal hosts via a provisioning stack that is also running on Kubernetes.</subtitle><entry><title type="html">Metal3.io Becomes a CNCF Incubating Project</title><link href="https://metal3.io/blog/2025/08/27/metal3-becomes-cncf-incubating-project.html" rel="alternate" type="text/html" title="Metal3.io Becomes a CNCF Incubating Project" /><published>2025-08-27T00:00:00-05:00</published><updated>2025-08-27T00:00:00-05:00</updated><id>https://metal3.io/blog/2025/08/27/metal3-becomes-cncf-incubating-project</id><content type="html" xml:base="https://metal3.io/blog/2025/08/27/metal3-becomes-cncf-incubating-project.html"><![CDATA[<p>We are pleased to share some incredible news with our community!  The CNCF
<a href="https://www.cncf.io/people/technical-oversight-committee/">Technical Oversight Committee</a> has officially voted to accept Metal3 as an
incubating project. This milestone represents years of hard work, collaboration,
and innovation, and we couldn’t be more excited about what lies ahead!</p>

<h2 id="our-journey-from-sandbox-to-incubation">Our Journey from Sandbox to Incubation</h2>

<p>What started as a collaboration between Red Hat and Ericsson in 2019 has
blossomed into something truly special. When we joined the CNCF sandbox in
September 2020, we knew we had something powerful: a way to make bare metal
infrastructure as Kubernetes-native as any cloud platform. Today, that vision
has grown far beyond our initial dreams.</p>

<h2 id="the-numbers-tell-our-story">The Numbers Tell Our Story</h2>

<p>We’re incredibly proud of what our community has accomplished together:</p>

<ul>
  <li><strong>57 active contributing organizations</strong> from around the globe</li>
  <li><strong>186 amazing contributors</strong> who’ve shaped our project</li>
  <li><strong>8,368 merged pull requests</strong> representing countless hours of collaboration</li>
  <li><strong>1,523 GitHub stars</strong> from supporters worldwide</li>
  <li><strong>187 releases</strong> of continuous improvement</li>
</ul>

<p>But beyond the numbers, what makes us truly happy is seeing organizations like
Fujitsu, Ikea, SUSE, Ericsson, and Red Hat successfully deploying Metal3 in
production environments.</p>

<h2 id="what-makes-us-proud">What Makes Us Proud</h2>

<p>Metal3 has evolved into so much more than a bare metal provisioning tool. We’ve
built a comprehensive platform that:</p>

<ul>
  <li>Seamlessly integrates with Cluster API for Kubernetes lifecycle management</li>
  <li>Provides robust IP address management through our IPAM component</li>
  <li>Offers enterprise-grade security with automated vulnerability scanning</li>
  <li>Supports firmware management and day-2 operations</li>
  <li>Runs entirely on Kubernetes using native APIs</li>
</ul>

<p>Our new Ironic Standalone Operator has revolutionized deployment simplicity,
making it easier than ever for teams to get started with Metal3.</p>

<h2 id="looking-forward-with-excitement">Looking Forward with Excitement</h2>

<p>The roadmap ahead fills us with anticipation! In 2025, we’re planning:</p>

<ul>
  <li>Enhanced multi-tenancy support</li>
  <li>ARM architecture support beyond x86_64</li>
  <li>Improved DHCP-less provisioning capabilities</li>
  <li>New API revisions across our components</li>
  <li>Continued simplification of the user experience</li>
</ul>

<h2 id="the-adventure-continues">The Adventure Continues</h2>

<p>Joining CNCF incubation isn’t the end of our journey – it’s an exciting new
chapter! With the foundation’s support and our amazing community behind us,
we’re more energized than ever to push the boundaries of what’s possible with
bare metal Kubernetes infrastructure.</p>

<p><strong>Thank you</strong> for being part of this incredible adventure. Here’s to making bare
metal as cloud-native as the clouds themselves!</p>]]></content><author><name>Honza Pokorný</name></author><category term="metal3" /><category term="cncf" /><category term="community" /><category term="announcement" /><summary type="html"><![CDATA[We are pleased to share some incredible news with our community! The CNCF Technical Oversight Committee has officially voted to accept Metal3 as an incubating project. This milestone represents years of hard work, collaboration, and innovation, and we couldn’t be more excited about what lies ahead!]]></summary></entry><entry><title type="html">Introducing Baremetal Operator end-to-end test suite</title><link href="https://metal3.io/blog/2024/12/13/Introducing-BMO-E2E.html" rel="alternate" type="text/html" title="Introducing Baremetal Operator end-to-end test suite" /><published>2024-12-13T00:00:00-06:00</published><updated>2024-12-13T00:00:00-06:00</updated><id>https://metal3.io/blog/2024/12/13/Introducing-BMO-E2E</id><content type="html" xml:base="https://metal3.io/blog/2024/12/13/Introducing-BMO-E2E.html"><![CDATA[<p>In the beginning, there was
<a href="https://github.com/metal3-io/metal3-dev-env">metal3-dev-env</a>. It could set up a
virtualized “baremetal” lab and test all the components together. As Metal3
matured, it grew in complexity and capabilities, with release branches, API
versions, etc. Metal3-dev-env did everything from cloning the repositories and
building the container images, to deploying the controllers and running tests,
on top of setting up the virtual machines and the networks, of course. Needless
to say, it became hard to understand and easy to misuse.</p>

<p>We tried reducing the scope a bit by introducing end to end tests <a href="https://github.com/metal3-io/cluster-api-provider-metal3/tree/main/test">directly in
the Cluster API provider
Metal3</a>
(CAPM3). However, metal3-dev-env was still very much entangled with CAPM3. It
was at this point that I got tired of trying to gradually fix it and took the
initiative to start from scratch with end to end tests in <a href="https://github.com/metal3-io/baremetal-operator">Baremetal Operator
(BMO)</a> instead.</p>

<p>Up until that point, we had been testing BMO through CAPM3 and the cluster API
flow. It worked, but it was very inefficient. From the perspective on the
Baremetal Operator, a test could look something like this:</p>

<ol>
  <li>Register 5 BareMetalHosts</li>
  <li>Inspect the 5 BareMetalHosts</li>
  <li>Provision the 5 BareMetalHosts all with the same image</li>
  <li>Deprovision 1 BareMetalHost</li>
  <li>Provision it again with another image</li>
  <li>Deprovision another BareMetalHost</li>
  <li>Provision it again with the other image</li>
  <li>Continue in the same way with the rest of the BareMetalHosts…</li>
  <li>Deprovision all BareMetalHosts</li>
</ol>

<p>As you can see, it is very repetitive, constantly doing the same thing again and
again. As a consequence of this and the complexity of metal3-dev-env, it was
quite an effort to thoroughly test something related to BMO code. I was
constantly questioning myself and the test environment. “Is it testing the code
I wrote?” “Is it doing the relevant scenario?” “Is the configuration correct?”</p>

<h2 id="baremetal-operator-end-to-end-tests-are-born">Baremetal Operator end to end tests are born</h2>

<p>Sometimes it is easier to start from scratch, so <a href="https://github.com/metal3-io/baremetal-operator/pull/1303">this is what we
did</a>. The Baremetal
Operator end to end tests started out as a small script that only set up
minikube, some VMs and a baseboard management controller (BMC) emulator. The
goal was simple: do the minimum required to simulate a baremetal lab. From this,
it was quite easy to build a test module that was responsible for deploying the
necessary controllers and running some tests.</p>

<p>Notice the separation of concerns here! The test module expects a baremetal lab
environment to be already existing and the script that sets up the environment
is not involved in anyway with the tests or deployment of the controllers. This
design is deliberate, with a clear goal that the test module should be useful
across multiple environments. It should be possible to run the test suite
against real baremetal labs with multiple different configurations. I am hoping
that we will get a chance next year to try it for real in a baremetal lab.</p>

<h2 id="how-does-it-work">How does it work?</h2>

<p>The flexibility of the end to end module is possible through a configuration
file. It can be used to configure everything from the image URL and checksum to
the timeout limits. Since Ironic can be deployed in many different ways, it was
also necessary to make this flexible. The user can optionally set up Ironic
before the test, or provide a kustomization that will be applied automatically.
A separate configuration file declares the BMCs that should be used in the
tests.</p>

<p>The <a href="https://github.com/metal3-io/baremetal-operator/tree/main/test/e2e/config">configuration that we use in
CI</a>
shows how these files look like. As a proof of concept for the flexibility of
the tests, it can be noted that we already have two different configurations.
One for running the tests with Ironic and one for running them with BMO in
fixture mode. The first is the “normal” mode, the latter means that BMO does not
communicate with Ironic at all, it just pretends. While that obviously isn’t
useful for any thorough tests, it still provides a quick and light weight test
suite, and ensures that we do not get too attached to one particular
configuration.</p>

<p>The test suite itself is made with Ginkgo and Gomega. Instead of building a long
chain of checks and scenarios we have attempted to do small, isolated tests.
This makes it possible to run multiple in parallel and shorten the test suite
duration, as well as easily identify where exactly errors occur. In order to
accomplish this, we make heavy use of the <a href="https://book.metal3.io/bmo/status_annotation">status
annotation</a> so that we can skip
inspection when possible.</p>

<h2 id="where-are-we-today">Where are we today?</h2>

<p>It is already several months since we switched over to the BMO e2e test suite as
the primary, and only required tests for pull requests in the BMO repository. We
run the <a href="https://github.com/metal3-io/baremetal-operator/blob/main/.github/workflows/e2e-test.yml">end to end test suite as GitHub
workflows</a>
and it covers more than the metal3-dev-env and CAPM3 based tests from BMO
perspective. That does not mean that we are done though. At the time of writing,
there are <a href="https://github.com/orgs/metal3-io/projects/5/views/2">several GitHub
issues</a> for improving and
extending the tests. The progress has significantly slowed though, as can
perhaps be expected, since the most essentials parts were implemented.</p>

<h2 id="the-future">The future</h2>

<p>In the future we hope to make the BMO end to end module and tooling more useful
for local development and testing. It should be easy to spin up a minimal
environment and test specific scenarios, also using Tilt. Additionally, we want
to “rebase” the CAPM3 end to end tests on this work. It should be possible to
reuse the code and tooling for simulating a baremetal lab so that we can get rid
of the entanglement with metal3-dev-env.</p>]]></content><author><name>Lennart Jern</name></author><category term="metal3" /><category term="cluster API" /><category term="provider" /><category term="edge" /><summary type="html"><![CDATA[In the beginning, there was metal3-dev-env. It could set up a virtualized “baremetal” lab and test all the components together. As Metal3 matured, it grew in complexity and capabilities, with release branches, API versions, etc. Metal3-dev-env did everything from cloning the repositories and building the container images, to deploying the controllers and running tests, on top of setting up the virtual machines and the networks, of course. Needless to say, it became hard to understand and easy to misuse.]]></summary></entry><entry><title type="html">Scaling Kubernetes with Metal3: Simulating 1000 Clusters with Fake Ironic Agents</title><link href="https://metal3.io/blog/2024/10/24/Scaling-Kubernetes-with-Metal3-on-Fake-Node.html" rel="alternate" type="text/html" title="Scaling Kubernetes with Metal3: Simulating 1000 Clusters with Fake Ironic Agents" /><published>2024-10-24T00:00:00-05:00</published><updated>2024-10-24T00:00:00-05:00</updated><id>https://metal3.io/blog/2024/10/24/Scaling-Kubernetes-with-Metal3-on-Fake-Node</id><content type="html" xml:base="https://metal3.io/blog/2024/10/24/Scaling-Kubernetes-with-Metal3-on-Fake-Node.html"><![CDATA[<p>If you’ve ever tried scaling out Kubernetes clusters in a bare-metal
environment, you’ll know that large-scale testing comes with serious challenges.
Most of us don’t have access to enough physical servers—or even virtual
machines—to simulate the kinds of large-scale environments we need for stress
testing, especially when deploying hundreds or thousands of clusters.</p>

<p>That’s where this experiment comes in.</p>

<p>Using Metal3, we simulated a massive environment—provisioning 1000 single-node
Kubernetes clusters—without any actual hardware. The trick? A combination of
Fake Ironic Python Agents (IPA) and Fake Kubernetes API servers. These tools
allowed us to run an entirely realistic Metal3 provisioning workflow while
simulating thousands of nodes and clusters, all without needing a single real
machine.</p>

<p>The motivation behind this was simple: to create a scalable testing environment
that lets us validate Metal3’s performance, workflow, and reliability without
needing an expensive hardware lab or virtual machine fleet. By simulating nodes
and clusters, we could push the limits of Metal3’s provisioning process
cost-effectively and time-efficiently.</p>

<p>In this post, I’ll explain exactly how it all works, from setting up multiple
Ironic services to faking hardware nodes and clusters and sharing the lessons
learned. Whether you’re a Metal3 user or just curious about how to test
large-scale Kubernetes environments, it’ll surely be a good read. Let’s get
started!</p>

<h2 id="prerequisites--setup">Prerequisites &amp; Setup</h2>

<p>Before diving into the fun stuff, let’s ensure we’re on the same page. You don’t
need to be a Metal3 expert to follow along, but having a bit of background will
help!</p>

<h3 id="what-youll-need-to-know">What You’ll Need to Know</h3>

<p>Let’s start by ensuring you’re familiar with some essential tools and concepts
that power Metal3 workflow. If you’re confident in your Metal3 skills, please
feel free to skip this part.</p>

<h4 id="a-typical-metal3-workflow">A typical Metal3 Workflow</h4>

<p>The following diagram explains a typical Metal3 workflow. We will, then, go into
details of every component.</p>

<p><img src="/assets/2024-10-24-Scaling-Kubernetes-with-Metal3-on-Fake-Node/metal3-typical-workflow.jpg" alt="Metal3 Typical
Workflow" /></p>

<h4 id="cluster-api-capi">Cluster API (CAPI)</h4>

<p>CAPI is a project that simplifies the deployment and management of Kubernetes
clusters. It provides a consistent way to create, update, and scale clusters
through Kubernetes-native APIs. The magic of CAPI is that it abstracts away many
of the underlying details so that you can manage clusters on different platforms
(cloud, bare metal, etc.) in a unified way.</p>

<h4 id="cluster-api-provider-metal3-capm3">Cluster API Provider Metal3 (CAPM3)</h4>

<p>CAPM3 extends CAPI to work specifically with Metal3 environments. It connects
the dots between CAPI, BMO, and Ironic, allowing Kubernetes clusters to be
deployed on bare-metal infrastructure. It handles tasks like provisioning new
nodes, registering them with Kubernetes, and scaling clusters.</p>

<h4 id="bare-metal-operator-bmo">Bare Metal Operator (BMO)</h4>

<p>BMO is a controller that runs inside a Kubernetes cluster and works alongside
Ironic to manage bare-metal infrastructure. It automates the lifecycle of
bare-metal hosts, managing things like registering new hosts, powering them on
or off, and monitoring their status.</p>

<h5 id="bare-metal-host-bmh">Bare Metal Host (BMH)</h5>

<p>A BMH is the Kubernetes representation of a bare-metal node. It contains
information about how to reach the node it represents, and BMO monitors its
desired state closely. When BMO notices that a BMH object state is requested to
change (either by a human user or CAPM3), it will decide what needs to be done
and tell Ironic.</p>

<h4 id="ironic--ironic-python-agent-ipa">Ironic &amp; Ironic Python Agent (IPA)</h4>

<ul>
  <li>Ironic is a bare-metal provisioning tool that handles tasks like booting
servers, deploying bootable media (e.g., operating systems) to disk, and
configuring hardware. Think of Ironic as the piece of software that manages
actual physical servers. In a Metal3 workflow, Ironic receives orders from BMO
and translates them into actionable steps. Ironic has multiple ways to interact
with the machines, and one of them is the so-called “ agent-based direct deploy”
method, which is commonly used by BMO. The agent mentioned is called <strong>Ironic
Python Agent</strong> (IPA), which is a piece of software that runs on each bare-metal
node and carries out Ironic’s instructions. It interacts with the hardware
directly, like wiping disks, configuring networks, and handling boot processes.</li>
</ul>

<p>In a typical Metal3 workflow, BMO reads the desired state of the node from the
BMH object, translates the Kubernetes reconciling logic to concrete actions, and
forwards them to Ironic, which, as part of the provisioning process, tells IPA
the exact steps it needs to perform to get the nodes to desired states. During
the first boot after node image installation, Kubernetes components will be
installed on the nodes by cloud-init, and once the process succeeds, Ironic
and IPA finish the provisioning process, and CAPI and CAPM3 will verify the
health of the newly provisioned Kubernetes cluster(s).</p>

<h2 id="the-experiment-simulating-1000-kubernetes-clusters">The Experiment: Simulating 1000 Kubernetes Clusters</h2>

<p>This experiment aimed to push Metal3 to simulate 1000 single-node Kubernetes
clusters on fake hardware. Instead of provisioning real machines, we used Fake
Ironic Python Agents (Fake IP) and Fake Kubernetes API Servers (FKAS) to
simulate nodes and control planes, respectively. This setup allowed us to test a
massive environment without the need for physical infrastructure.</p>

<p>Since our goal is to verify the Metal3 limit, our setup will let all the Metal3
components (except for IPA, which runs inside and will be scaled with the nodes)
to keep working as they do in a typical workflow. In fact, none of the
components should be aware that they are running with fake hardware.</p>

<p>Take the figure we had earlier as a base, here is the revised workflow with fake
nodes.</p>

<p><img src="/assets/2024-10-24-Scaling-Kubernetes-with-Metal3-on-Fake-Node/metal3-simulation-workflow.jpg" alt="Metal3 Simulation
Workflow" /></p>

<h3 id="step-1-setting-up-the-environment">Step 1: Setting Up the environment</h3>

<p>As you may have known, a typical Metal3 workflow requires several components:
bootstrap Kubernetes cluster, possible external networks, bare-metal nodes, etc.
As we are working on simulating the environment, we will start with a newly
spawned Ubuntu VM, create a cluster with minikube, add networks with libvirt,
and so on (If you’re familiar with Metal3’s dev-env, this step is similar to
what script
<a href="https://github.com/metal3-io/metal3-dev-env/blob/main/01_prepare_host.sh">01</a>,
<a href="https://github.com/metal3-io/metal3-dev-env/blob/main/02_configure_host.sh">02</a>
and a part of
<a href="https://github.com/metal3-io/metal3-dev-env/blob/main/03_launch_mgmt_cluster.sh">03</a>
do). We will not discuss this part, but you can find the related setup from
<a href="https://github.com/Nordix/metal3-clusterapi-docs/blob/main/Support/Multitenancy/Scalability-with-Fake-Nodes/vm-setup.sh">this
script</a>
if interested.</p>

<p><strong>Note</strong>: If you intend to follow along, note that going to 1000 nodes requires
a large environment and will take a long time. In our setup, we had a VM with 24
cores and 32GB of RAM, of which we assigned 14 cores and 20GB of RAM to the
minikube VM, and the process took roughly 48 hours. If your environment is less
powerful, consider reducing the nodes you want to provision. Something like 100
nodes will require minimal resources and time while still being impressive.</p>

<h3 id="step-2-install-bmo-and-ironic">Step 2: Install BMO and Ironic</h3>

<p>In Metal3’s typical workflow, we usually rely on Kustomize to install Ironic and
BMO. Kustomize helps us define configurations for Kubernetes resources, making
it easier to customize and deploy services. However, our current Kustomize
overlay for Metal3 configures only a single Ironic instance. This setup works
well for smaller environments, but it becomes a bottleneck when scaling up and
handling thousands of nodes.</p>

<p>That’s where Ironic’s <strong>special mode</strong> comes into play. Ironic has the ability
to run <strong>multiple Ironic conductors</strong> while sharing the same database. The best
part? Workload balancing between conductors happens automatically, which means
that no matter which Ironic conductor receives a request, the load is evenly
distributed across all conductors, ensuring efficient provisioning. Achieving
this requires separating <strong>ironic conductor</strong> from the database, which allows us
to scale up the conductor part. Each <strong>conductor</strong> will have its own
<code class="language-plaintext highlighter-rouge">PROVISIONING_IP</code>, hence the need to have a specialized <code class="language-plaintext highlighter-rouge">configMap.</code></p>

<p>We used <a href="https://helm.sh/">Helm</a> for this purpose. In our Helm chart, the
<strong>Ironic conductor</strong> container and <strong>HTTP server (httpd)</strong> container are
separated into a new pod, and the rest of the ironic package (mostly
MariaDB-ironic database) stays in another pod. A list of PROVISIONING_IPs is
provided by the chart’s <code class="language-plaintext highlighter-rouge">values.yaml</code>, and for each IP, an  <strong>ironic conductor</strong>
pod is created, along with a config map whose values are rendered with the IP’s
value. This way, we can dynamically scale up/down ironic (or, more specifically,
<strong>ironic conductors</strong>) by simply adding/removing ips.</p>

<p>Another piece of information that we need to keep in mind is the ipa-downloader
container. In our current metal3-dev-env, the IPA-downloader container runs as
an init Container for ironic, and its job is to download the IPA image to a
Persistent Volume. This image contains the <strong>Ironic Python Agent</strong>, and it is
assumed to exist by Ironic. For the multiple-conductor scenario, running the
same init-container for all the conductors, at the same time, could be slow
and/or fail due to network issue. To make it work, we made a small “hack” in the
chart: the ipa image will exist in a specific location inside the minikube host,
and all the conductor pods will mount to that same location. In production, a
more throughout solution might be to keep the IPA-downloader as an
init-container, but points the image to the local image server, which we set up
in the previous step.</p>

<p>BMO, on the other hand, still works well with kustomize, as we do not need to
scale it. As with typical metal3 workflow, BMO and Ironic must share some
authentication to work with TLS.</p>

<p>You can check out the full Ironic helm chart
<a href="https://github.com/Nordix/metal3-clusterapi-docs/tree/main/Support/Multitenancy/Scalability-with-Fake-Nodes/ironic">here</a>.</p>

<h3 id="step-3-creating-fake-nodes-with-fake-ironic-python-agents">Step 3: Creating Fake Nodes with Fake Ironic Python Agents</h3>

<p>As we mentioned at the beginning, instead of using real hardware, we will use a
new tool called <strong>Fake Ironic Python Agent</strong>, or <strong>Fake IPA</strong> to simulate the
nodes.</p>

<p>Setting up <strong>Fake IPA</strong> is relatively straightforward, as <strong>Fake IPA</strong> runs as
containers on the host machine, but first, we need to create the list of “nodes”
that we will use (Fake IPA requires to have that list ready when it starts). A
“node” typically looks like this</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="o">{</span>
      <span class="s2">"uuid"</span>: <span class="nv">$uuid</span>,
      <span class="s2">"name"</span>: <span class="nv">$node_name</span>,
      <span class="s2">"power_state"</span>: <span class="s2">"Off"</span>,
      <span class="s2">"external_notifier"</span>: <span class="s2">"True"</span>,
      <span class="s2">"nics"</span>: <span class="o">[</span>
        <span class="o">{</span><span class="s2">"mac"</span>: <span class="nv">$macaddr</span>, <span class="s2">"ip"</span>: <span class="s2">"192.168.0.100"</span><span class="o">}</span>
      <span class="o">]</span>,
<span class="o">}</span>
</code></pre></div></div>

<p>All of the variables (<code class="language-plaintext highlighter-rouge">uuid</code>, <code class="language-plaintext highlighter-rouge">node_name</code>, <code class="language-plaintext highlighter-rouge">macaddress</code>) can be dynamically
generated in any way you want (check <a href="https://github.com/Nordix/metal3-clusterapi-docs/blob/main/Support/Multitenancy/Scalability-with-Fake-Nodes/generate_unique_nodes.sh">this
script</a>
out if you need an idea). Still, we must store this information to generate the
BMH objects that match those “nodes.” The <code class="language-plaintext highlighter-rouge">ip</code> is, on the other hand, not
essential. It could be anything.</p>

<p>We must also start up the <strong>sushy-tools</strong> container in this step. It is a tool
that simulates the <a href="https://www.techtarget.com/searchnetworking/definition/baseboard-management-controller">Baseboard Management
Controller</a>
for non-bare-metal hardware, and we have been using it extensively inside Metal3
dev-env and CI to control and provision VMs as if they are bare-metal nodes. In
a bare-metal setup, Ironic will ask the BMC to install IPA on the node, and in
our setup, <strong>sushy-tools</strong> will get the same request, but it will simply fake
the installation and, in the end, forward <strong>Ironic</strong> traffic to the <strong>Fake IPA</strong>
container.</p>

<p>Another piece of information we will need is the cert that <strong>Ironic</strong> will use
in its communication with <strong>IPA</strong>. IPA is supposed to get it from Ironic, but as
<strong>Fake IPA</strong> cannot do that (at least not yet), we must get the cert and provide
it in <strong>Fake IPA</strong> config.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nb">mkdir </span>cert
kubectl get secret <span class="nt">-n</span> baremetal-operator-system ironic-cert <span class="nt">-o</span> json <span class="se">\</span>
  <span class="nt">-o</span><span class="o">=</span><span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.data.ca</span><span class="se">\.</span><span class="s2">crt}"</span> | <span class="nb">base64</span> <span class="nt">-d</span> <span class="o">&gt;</span>cert/ironic-ca.crt
</code></pre></div></div>

<p>Also note that one set of <strong>sushy-tools</strong> and <strong>Fake IPA</strong> containers won’t be
enough to provision 1000 nodes. Just like <strong>Ironic</strong>, they need to be scaled up
extensively (about 20-30 pairs will be sufficient for 1000 nodes), but
fortunately, the scaling is straightforward: We just need to give them different
ports. Both of these components also require a Python-based config file. For
convenience, in this setup, we create a big file and provide it to both of them,
using the following shell script:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="k">for </span>i <span class="k">in</span> <span class="si">$(</span><span class="nb">seq </span>1 <span class="s2">"</span><span class="nv">$N_SUSHY</span><span class="s2">"</span><span class="si">)</span><span class="p">;</span> <span class="k">do
  </span><span class="nv">container_conf_dir</span><span class="o">=</span><span class="s2">"</span><span class="nv">$SUSHY_CONF_DIR</span><span class="s2">/sushy-</span><span class="nv">$i</span><span class="s2">"</span>

  <span class="c"># Use round-robin to choose fake-ipa and sushy-tools containers for the node</span>
  <span class="nv">fake_ipa_port</span><span class="o">=</span><span class="k">$((</span><span class="m">9901</span> <span class="o">+</span> <span class="o">((</span><span class="nv">$i</span> <span class="o">%</span> <span class="k">${</span><span class="nv">N_FAKE_IPA</span><span class="k">:-</span><span class="nv">1</span><span class="k">}))</span><span class="o">))</span>
  <span class="nv">sushy_tools_port</span><span class="o">=</span><span class="k">$((</span><span class="m">8000</span> <span class="o">+</span> i<span class="k">))</span>
  ports+<span class="o">=(</span><span class="k">${</span><span class="nv">sushy_tools_port</span><span class="k">}</span><span class="o">)</span>

  <span class="c"># This is only so that we have the list of the needed ports for other</span>
  <span class="c"># purposes, like configuring the firewalls.</span>
  ports+<span class="o">=(</span><span class="k">${</span><span class="nv">fake_ipa_port</span><span class="k">}</span><span class="o">)</span>

  <span class="nb">mkdir</span> <span class="nt">-p</span> <span class="s2">"</span><span class="k">${</span><span class="nv">container_conf_dir</span><span class="k">}</span><span class="s2">"</span>

  <span class="c"># Generate the htpasswd file, which is required by sushy-tools</span>
  <span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="sh">'</span><span class="no">EOF</span><span class="sh">' &gt;"</span><span class="k">${</span><span class="nv">container_conf_dir</span><span class="k">}</span><span class="sh">"/htpasswd
admin:</span><span class="nv">$2b$12$/</span><span class="sh">dVOBNatORwKpF.ss99KB.vESjfyONOxyH.UgRwNyZi1Xs/W2pGVS
</span><span class="no">EOF

</span>  <span class="c"># Set configuration options</span>
  <span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> &gt;"</span><span class="k">${</span><span class="nv">container_conf_dir</span><span class="k">}</span><span class="sh">"/conf.py
import collections

SUSHY_EMULATOR_LIBVIRT_URI = "</span><span class="k">${</span><span class="nv">LIBVIRT_URI</span><span class="k">}</span><span class="sh">"
SUSHY_EMULATOR_IGNORE_BOOT_DEVICE = False
SUSHY_EMULATOR_VMEDIA_VERIFY_SSL = False
SUSHY_EMULATOR_AUTH_FILE = "/root/sushy/htpasswd"
SUSHY_EMULATOR_FAKE_DRIVER = True
SUSHY_EMULATOR_LISTEN_PORT = "</span><span class="k">${</span><span class="nv">sushy_tools_port</span><span class="k">}</span><span class="sh">"
EXTERNAL_NOTIFICATION_URL = "http://</span><span class="k">${</span><span class="nv">ADVERTISE_HOST</span><span class="k">}</span><span class="sh">:</span><span class="k">${</span><span class="nv">fake_ipa_port</span><span class="k">}</span><span class="sh">"
FAKE_IPA_API_URL = "</span><span class="k">${</span><span class="nv">API_URL</span><span class="k">}</span><span class="sh">"
FAKE_IPA_URL = "http://</span><span class="k">${</span><span class="nv">ADVERTISE_HOST</span><span class="k">}</span><span class="sh">:</span><span class="k">${</span><span class="nv">fake_ipa_port</span><span class="k">}</span><span class="sh">"
FAKE_IPA_INSPECTION_CALLBACK_URL = "</span><span class="k">${</span><span class="nv">CALLBACK_URL</span><span class="k">}</span><span class="sh">"
FAKE_IPA_ADVERTISE_ADDRESS_IP = "</span><span class="k">${</span><span class="nv">ADVERTISE_HOST</span><span class="k">}</span><span class="sh">"
FAKE_IPA_ADVERTISE_ADDRESS_PORT = "</span><span class="k">${</span><span class="nv">fake_ipa_port</span><span class="k">}</span><span class="sh">"
FAKE_IPA_CAFILE = "/root/cert/ironic-ca.crt"
SUSHY_FAKE_IPA_LISTEN_IP = "</span><span class="k">${</span><span class="nv">ADVERTISE_HOST</span><span class="k">}</span><span class="sh">"
SUSHY_FAKE_IPA_LISTEN_PORT = "</span><span class="k">${</span><span class="nv">fake_ipa_port</span><span class="k">}</span><span class="sh">"
SUSHY_EMULATOR_FAKE_IPA = True
SUSHY_EMULATOR_FAKE_SYSTEMS = </span><span class="si">$(</span><span class="nb">cat </span>nodes.json<span class="si">)</span><span class="sh">
</span><span class="no">EOF

</span>  <span class="c"># Start sushy-tools</span>
  docker run <span class="nt">-d</span> <span class="nt">--net</span> host <span class="nt">--name</span> <span class="s2">"sushy-tools-</span><span class="k">${</span><span class="nv">i</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="nt">-v</span> <span class="s2">"</span><span class="k">${</span><span class="nv">container_conf_dir</span><span class="k">}</span><span class="s2">"</span>:/root/sushy <span class="se">\</span>
    <span class="s2">"</span><span class="k">${</span><span class="nv">SUSHY_TOOLS_IMAGE</span><span class="k">}</span><span class="s2">"</span>

  <span class="c"># Start fake-ipa</span>
  docker run <span class="se">\</span>
    <span class="nt">-d</span> <span class="nt">--net</span> host <span class="nt">--name</span> fake-ipa-<span class="k">${</span><span class="nv">i</span><span class="k">}</span> <span class="se">\</span>
    <span class="nt">-v</span> <span class="s2">"</span><span class="k">${</span><span class="nv">container_conf_dir</span><span class="k">}</span><span class="s2">"</span>:/app <span class="se">\</span>
    <span class="nt">-v</span> <span class="s2">"</span><span class="si">$(</span><span class="nb">realpath </span>cert<span class="si">)</span><span class="s2">"</span>:/root/cert <span class="se">\</span>
    <span class="s2">"</span><span class="k">${</span><span class="nv">FAKEIPA_IMAGE</span><span class="k">}</span><span class="s2">"</span>
<span class="k">done</span>
</code></pre></div></div>

<p>In this setup, we made it so that all the <strong>sushy-tools</strong> containers will
listen on the port range running from 8001, 8002,…, while the <strong>Fake IPA</strong>
containers have ports 9001, 9002,…</p>

<h3 id="step-4-add-the-bmh-objects">Step 4: Add the BMH objects</h3>

<p>Now that we have <strong>sushy-tools</strong> and <strong>Fake IPA</strong> containers running, we can
already generate the manifest for BMH objects, and apply them to the cluster. A
BMH object will look like this</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Secret</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="pi">{</span><span class="nv">name</span><span class="pi">}</span><span class="s">-bmc-secret</span>
  <span class="na">labels</span><span class="pi">:</span>
      <span class="na">environment.metal3.io</span><span class="pi">:</span> <span class="s">baremetal</span>
<span class="na">type</span><span class="pi">:</span> <span class="s">Opaque</span>
<span class="na">data</span><span class="pi">:</span>
  <span class="na">username</span><span class="pi">:</span> <span class="s">YWRtaW4=</span>
  <span class="na">password</span><span class="pi">:</span> <span class="s">cGFzc3dvcmQ=</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">metal3.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">BareMetalHost</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="pi">{</span><span class="nv">name</span><span class="pi">}</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">online</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">bmc</span><span class="pi">:</span>
    <span class="na">address</span><span class="pi">:</span> <span class="s">redfish+http://192.168.222.1:{port}/redfish/v1/Systems/{uuid}</span>
    <span class="na">credentialsName</span><span class="pi">:</span> <span class="pi">{</span><span class="nv">name</span><span class="pi">}</span><span class="s">-bmc-secret</span>
  <span class="na">bootMACAddress</span><span class="pi">:</span> <span class="pi">{</span><span class="nv">random_mac</span><span class="pi">}</span>
  <span class="na">bootMode</span><span class="pi">:</span> <span class="s">legacy</span>
</code></pre></div></div>

<p>In this manifest:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">name</code> is the node name we generated in the previous step.</li>
  <li><code class="language-plaintext highlighter-rouge">uuid</code> is the random uuid we generated for the same node.</li>
  <li><code class="language-plaintext highlighter-rouge">random_mac</code> is a random mac address for the boot. It’s NOT the same as the
NIC mac address we generated for the node.</li>
  <li><code class="language-plaintext highlighter-rouge">port</code> is the listening port on one of the <strong>sushy-tools</strong> containers we
created in the previous step. Since every <strong>sushy-tools</strong> and <strong>Fake IPA</strong>
container has information about ALL the nodes, we can decide what container to
locate the “node”. In general, it’s a good idea to spread them out, so all
containers are loaded equally.</li>
</ul>

<p>We can now run <code class="language-plaintext highlighter-rouge">kubectl apply -f</code> on one (or all of) the BMH manifests. What you
expect to see is that a BMH object is created, and its state will change from
<code class="language-plaintext highlighter-rouge">registering</code> to <code class="language-plaintext highlighter-rouge">available</code> after a while. It means <strong>ironic</strong> acknowledged
that the node is valid, in good state and ready to be provisioned.</p>

<h3 id="step-5-deploy-the-fake-nodes-to-kubernetes-clusters">Step 5: Deploy the fake nodes to kubernetes clusters</h3>

<p>Before provisioning our clusters, let’s init the process, so that we have CAPI
and CAPM3 installed</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="syntax"><code>clusterctl init <span class="nt">--infrastructure</span><span class="o">=</span>metal3
</code></pre></div></div>

<p>After a while, we should see that CAPI, CAPM3, and IPAM pods become available.</p>

<p>In a standard Metal3 workflow, after having the BMH objects in an <code class="language-plaintext highlighter-rouge">available</code>
state, we can provision new Kubernetes clusters with <code class="language-plaintext highlighter-rouge">clusterctl</code>. However, with
fake nodes, things get a tiny bit more complex. At the end of the provisioning
process, <strong>Cluster API</strong> expects that there is a new kubernetes API server
created for the new cluster, from which it will check if all nodes are up, all
the control planes have <code class="language-plaintext highlighter-rouge">apiserver</code>, <code class="language-plaintext highlighter-rouge">etcd</code>, etc. pods up and running, and so
on. It is where the <a href="https://github.com/metal3-io/cluster-api-provider-metal3/blob/main/hack/fake-apiserver/README.md"><strong>Fake Kubernetes API Server</strong>
(FKAS)</a>
comes in.</p>

<p>As the <strong>FKAS README</strong> linked above already described how it works, we won’t go
into details. We simply need to send <strong>FKAS</strong> a <code class="language-plaintext highlighter-rouge">register</code> POST request (with
the new cluster’s namespace and cluster name), and it will give us an IP and a
port, which we can plug into our cluster template and then run <code class="language-plaintext highlighter-rouge">clusterctl
generate cluster</code>.</p>

<p>Under the hood, <strong>FKAS</strong> generates unique API servers for different clusters.
Each of the fake API servers does the following jobs:</p>

<ul>
  <li>Mimicking API Calls: The Fake Kubernetes API server was set up to respond to
the essential Kubernetes API calls made during provisioning.</li>
  <li>Node Registration: When CAPM3 registered nodes, the Fake API server returned
success responses, making Metal3 believe the nodes had joined a real Kubernetes
cluster.</li>
  <li>Cluster Health and Status: The Fake API responded with “healthy” statuses,
allowing CAPI/CAPM3 to continue its workflow without interruption.</li>
  <li>Node Creation and Deletion: When CAPI queried for node status or attempted to
add/remove nodes, the Fake API server responded realistically, ensuring the
provisioning process continued smoothly.</li>
  <li>Pretending to Host Kubelet: The Fake API server also simulated kubelet
responses, which allowed CAPI/CAPM3 to interact with the fake clusters as though
they were managing actual nodes.</li>
</ul>

<p>Note that in this experiment, we provisioned every one of the 1000 fake nodes to
a single-node cluster, but it’s possible to increase the number of control
planes and worker nodes by changing the <code class="language-plaintext highlighter-rouge">--control-plane-machine-count</code> and
<code class="language-plaintext highlighter-rouge">worker-machine-count</code> parameters in the <code class="language-plaintext highlighter-rouge">clusterctl generate cluster</code> command.
However, you will need to ensure that all clusters’ total nodes do not exceed
the number of BMHs.</p>

<p>As a glance, the whole simulation looks like this:</p>

<p><img src="/assets/2024-10-24-Scaling-Kubernetes-with-Metal3-on-Fake-Node/simulation-layout.jpg" alt="Simulation
layout" /></p>

<p>It will likely take some time, but once the BMHs are all provisioned, we should
be able to verify that all, or at least, most of the clusters are in good shape:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># This will list the clusters.</span>
kubectl get clusters <span class="nt">-A</span>

<span class="c"># This will determine the clusters' readiness.</span>
kubectl get kcp <span class="nt">-A</span>
</code></pre></div></div>

<ul>
  <li>For each cluster, it’s also a good idea to perform a <a href="https://cluster-api.sigs.k8s.io/clusterctl/commands/describe-cluster.html?highlight=describe%20cluster#clusterctl-describe-cluster">clusterctl
check</a>.</li>
</ul>

<h3 id="accessing-the-fake-cluster">Accessing the fake cluster</h3>

<p>A rather interesting (but not essential for our goal) check we can perform on
the fake clusters is to try accessing them. Let’s start with fetching a
cluster’s kubeconfig:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="syntax"><code>clusterctl <span class="nt">-n</span> &lt;cluster-namespace&gt; get kubeconfig &lt;cluster-name&gt; <span class="o">&gt;</span> kubeconfig-&lt;cluster-name&gt;.yaml
</code></pre></div></div>

<p>As usual, <code class="language-plaintext highlighter-rouge">clusterctl</code> will generate a kubeconfig file, but we cannot use it
just yet. Recall that we generated the API endpoint using FKAS; the address we
have now will be a combination of a port with FKAS’s IP address, which isn’t
accessible from outside the cluster. What we should do now is:</p>

<ul>
  <li>Edit the <code class="language-plaintext highlighter-rouge">kubeconfig-&lt;cluster-name&gt;.yaml</code> so that the endpoint is in the form
<code class="language-plaintext highlighter-rouge">localhost:&lt;port&gt;</code>.</li>
  <li>Port-forward the FKAS Pod to the same port the kubeconfig has shown.</li>
</ul>

<p>And voila, now we can access the fake cluster with <code class="language-plaintext highlighter-rouge">kubectl --kubeconfig
kubeconfig-&lt;cluster-name&gt;.yaml</code>. You can inspect its state and check the
resources (nodes, pods, etc.), but we won’t be able to run any workload on it as
it’s fake.</p>

<h2 id="results">Results</h2>

<p>In this post, we have demonstrated how it is possible to “generate”
bare-metal-based Kubernetes clusters from thin air (or rather, a bunch of nodes
that do not exist). Of course, these “clusters” are not very useful. Still,
successfully provisioning them without letting any of our main components
(<strong>CAPI</strong>, <strong>CAPM3</strong>, <strong>BMO</strong>, and <strong>Ironic</strong>) know they are working with fake
hardware proves that <strong>Metal3</strong> is capable of handling a heavy workload and
provision multiple nodes/clusters.</p>

<p>If interested, you could also check (and try out) the experiment by yourself
<a href="https://github.com/Nordix/metal3-clusterapi-docs/blob/main/Support/Multitenancy/Scalability-with-Fake-Nodes/README.md">here</a>.</p>]]></content><author><name>Huy Mai</name></author><category term="metal3" /><category term="cluster API" /><category term="ironic" /><category term="baremetal" /><category term="scaling" /><summary type="html"><![CDATA[If you’ve ever tried scaling out Kubernetes clusters in a bare-metal environment, you’ll know that large-scale testing comes with serious challenges. Most of us don’t have access to enough physical servers—or even virtual machines—to simulate the kinds of large-scale environments we need for stress testing, especially when deploying hundreds or thousands of clusters.]]></summary></entry><entry><title type="html">Scaling to 1000 clusters - Part 3</title><link href="https://metal3.io/blog/2024/05/30/Scaling_part_3.html" rel="alternate" type="text/html" title="Scaling to 1000 clusters - Part 3" /><published>2024-05-30T00:00:00-05:00</published><updated>2024-05-30T00:00:00-05:00</updated><id>https://metal3.io/blog/2024/05/30/Scaling_part_3</id><content type="html" xml:base="https://metal3.io/blog/2024/05/30/Scaling_part_3.html"><![CDATA[<!-- markdownlint-disable no-space-in-emphasis -->
<p>In <a href="/blog/2023/05/05/Scaling_part_1.html">part 1</a>, we introduced the
Bare Metal Operator test mode and saw how it can be used to play with
BareMetalHosts without Ironic and without any actual hosts. We continued in
<a href="/blog/2023/05/17/Scaling_part_2.html">part 2</a> with how to fake
workload clusters enough for convincing Cluster API’s controllers that they are
healthy. These two pieces together allowed us to run scaling tests and reach our
target of 1000 single node clusters. In this final part of the blog post series,
we will take a look at the results, the issues that we encountered and the
improvements that have been made.
<!-- markdownlint-enable no-space-in-emphasis --></p>

<h2 id="issues-encountered-and-lessons-learned">Issues encountered and lessons learned</h2>

<p>As part of this work we have learned a lot. We found genuine bugs and
performance issues, but we also learned about relevant configuration options for
Cluster API and controllers in general.</p>

<p>One of the first things we hit was <a href="https://github.com/metal3-io/baremetal-operator/issues/1190">this bug in Bare Metal
Operator</a> that
caused endless requeues for some deleted objects. It was not a big deal, barely
noticeable, at small scale. However, at larger scales things like this become a
problem. The logs become unreadable as they are filled with “spam” from
requeuing deleted objects and the controller is wasting resources trying to
reconcile them.</p>

<p>As mentioned, we also learned a lot from this experiment. For example, that all
the controllers have flags for setting their concurrency, i.e. how many objects
they reconcile in parallel. The default is 10, which works well in most cases,
but for larger scales it may be necessary to tune this in order to speed up the
reconciliation process.</p>

<p>The next thing we hit was rate limits! Both
<a href="https://github.com/kubernetes/client-go/blob/02d652e007235a5b46b9972bf136f274983853e6/util/workqueue/default_rate_limiters.go#L39">client-go</a>
and
<a href="https://github.com/kubernetes-sigs/controller-runtime/blob/v0.14.5/pkg/client/config/config.go#L96">controller-runtime</a>
have default rate limits of 10 and 20 QPS (Queries Per Second) respectively that
the controllers inherit unless overridden. In general, this is a good thing, as
it prevents controllers from overloading the API server. They obviously become
an issue once you scale far enough though. For us that happened when we got to
600 clusters.</p>

<p>Why 600? The number was actually a good clue, and the reason we managed figure
out what was wrong! Let’s break it down. By default, the Cluster API controller
will reconcile objects every 10 minutes (=600 seconds) in addition to reacting
to events. Each reconciliation will normally involve one or more API calls, so
at 600 clusters, we would have at least one API call per second just from the
periodic sync. In other words, the controllers would at this point use up a
large part of their budget on periodic reconciliation and quickly reach their
limit when adding reactions to events, such as the creation of a new cluster.</p>

<p>At the time, these rate limits were not configurable in the Cluster API
controllers, so we had to patch the controllers to increase the limits. We have
since then added flags to the controllers to make this configurable. If you
found this interesting, you can read more about it in <a href="https://github.com/kubernetes-sigs/cluster-api/issues/8052">this
issue</a>.</p>

<p>With concurrency and rate limits taken care of, we managed to reach our target
of 1000 clusters in reasonable time. However, there was still a problem with
resource usage. The Kubeadm control plane controller was <a href="https://github.com/kubernetes-sigs/cluster-api/issues/8602">unreasonably CPU
hungry</a>!</p>

<p>Luckily, Cluster API has excellent <a href="https://cluster-api.sigs.k8s.io/developer/core/tilt">debugging and monitoring tools
available</a> so it was easy
to collect data and profile the controllers. A quick look at the dashboard
confirmed that the Kubeadm control plane controller was indeed the culprit, with
a CPU usage far higher than the other controllers.</p>

<p><img src="/assets/2024-05-30-Scaling_part_3/CAPI-dashboard.png" alt="CAPI monitoring
dashboard" /></p>

<p>We then collected some profiling data and found the cause of the CPU usage. It
was generating new private keys for accessing the workload cluster API server
<em>every time</em> it needed to access it. This is a CPU intensive operation, and it
happened four times per reconciliation! The flame graph seen below clearly shows
the four key generation operations, and makes it obvious that this is what takes
up most of the time spent on the CPU for the controller.</p>

<p><img src="/assets/2024-05-30-Scaling_part_3/KCP-profiling.png" alt="KCP profiling graph" /></p>

<h2 id="improvements">Improvements</h2>

<p>All issues mentioned in the previous section have been addressed. The Bare Metal
Operator is no longer re-queuing deleted objects. All controllers have flags for
setting their concurrency and rate limits, and the Kubeadm control plane
controller is now caching and reusing the private keys instead of generating new
ones every time.</p>

<p>The impact of all of this is that</p>

<ul>
  <li>the Bare Metal Operator has more readable logs and lower CPU usage,</li>
  <li>users can configure rate limits for all Cluster API and Metal3 controllers if
necessary, and</li>
  <li>the Kubeadm control plane controller has a much lower CPU usage and faster
reconciliation times.</li>
</ul>

<h2 id="results">Results</h2>

<p>When we set out, it was simply not possible to reach a scale of 1000 clusters in
a reasonable time. With the collaboration, help from maintainers and other
community members, we managed to reach our target. It is now possible to manage
thousands of workload clusters through a single Cluster API management cluster.</p>

<p>The discussions and efforts also resulted in a <a href="https://kccncna2023.sched.com/event/1R2py/cluster-api-deep-dive-improving-performance-up-to-2k-clusters-fabrizio-pandini-stefan-buringer-vmware">deep dive presentation at
KubeCon NA
2023</a>
from the Cluster API maintainers.</p>

<p>Cluster API itself now also has an <a href="https://github.com/kubernetes-sigs/cluster-api/tree/main/test/infrastructure/inmemory">in-memory
provider</a>
which makes it almost trivial to test large scale scenarios. However, it must be
noted that it can only be used to test the core, bootstrap and control plane
providers. If you want to try it out, you can use the following script. Please
note that this will still be CPU intensive, despite the improvements mentioned
above. Creating 1000 clusters is no small task!</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>kind create cluster
<span class="nb">export </span><span class="nv">CLUSTER_TOPOLOGY</span><span class="o">=</span><span class="nb">true
</span>clusterctl init <span class="nt">--core</span><span class="o">=</span>cluster-api:v1.7.2 <span class="nt">--bootstrap</span><span class="o">=</span>kubeadm:v1.7.2 <span class="nt">--control-plane</span><span class="o">=</span>kubeadm:v1.7.2 <span class="nt">--infrastructure</span><span class="o">=</span><span class="k">in</span><span class="nt">-memory</span>:v1.7.2

<span class="c"># Patch the controllers to increase the rate limits and concurrency</span>
kubectl <span class="nt">-n</span> capi-system patch deployment capi-controller-manager <span class="se">\</span>
  <span class="nt">--type</span><span class="o">=</span>json <span class="nt">-p</span><span class="o">=</span><span class="s1">'[
    {"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kube-api-qps=100"},
    {"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kube-api-burst=200"},
    {"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--cluster-concurrency=100"},
    {"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--machine-concurrency=100"}
  ]'</span>

kubectl <span class="nt">-n</span> capi-kubeadm-control-plane-system patch deployment capi-kubeadm-control-plane-controller-manager <span class="se">\</span>
  <span class="nt">--type</span><span class="o">=</span>json <span class="nt">-p</span><span class="o">=</span><span class="s1">'[
    {"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kube-api-qps=100"},
    {"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kube-api-burst=200"},
    {"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kubeadmcontrolplane-concurrency=100"}
  ]'</span>

kubectl <span class="nt">-n</span> capi-kubeadm-bootstrap-system patch deployment capi-kubeadm-bootstrap-controller-manager <span class="se">\</span>
  <span class="nt">--type</span><span class="o">=</span>json <span class="nt">-p</span><span class="o">=</span><span class="s1">'[
    {"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kube-api-qps=100"},
    {"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kube-api-burst=200"},
    {"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kubeadmconfig-concurrency=100"},
    {"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--cluster-concurrency=100"}
  ]'</span>

<span class="c"># Create a ClusterClass and save a Cluster manifest</span>
kubectl apply <span class="nt">-f</span> https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.7.2/clusterclass-in-memory-quick-start.yaml
clusterctl generate cluster <span class="k">in</span><span class="nt">-memory-test</span> <span class="nt">--flavor</span><span class="o">=</span><span class="k">in</span><span class="nt">-memory-development</span> <span class="nt">--kubernetes-version</span><span class="o">=</span>v1.30.0 <span class="o">&gt;</span> <span class="k">in</span><span class="nt">-memory-cluster</span>.yaml

<span class="c"># Create 1000 clusters</span>
<span class="nv">START</span><span class="o">=</span>0
<span class="nv">NUM</span><span class="o">=</span>1000
<span class="k">for</span> <span class="o">((</span><span class="nv">i</span><span class="o">=</span>START<span class="p">;</span> i&lt;NUM<span class="p">;</span> i++<span class="o">))</span>
<span class="k">do
  </span><span class="nv">name</span><span class="o">=</span><span class="s2">"test-</span><span class="si">$(</span><span class="nb">printf</span> <span class="s2">"%03d</span><span class="se">\n</span><span class="s2">"</span> <span class="s2">"</span><span class="nv">$i</span><span class="s2">"</span><span class="si">)</span><span class="s2">"</span>
  <span class="nb">sed</span> <span class="s2">"s/in-memory-test/</span><span class="k">${</span><span class="nv">name</span><span class="k">}</span><span class="s2">/g"</span> <span class="k">in</span><span class="nt">-memory-cluster</span>.yaml | kubectl apply <span class="nt">-f</span> -
<span class="k">done</span>
</code></pre></div></div>

<p>This should result in 1000 ready in-memory clusters (and a pretty hot laptop if
you run it locally). On a laptop with an i9-12900H CPU, it took about 15 minutes
until all clusters were ready.</p>

<h2 id="conclusion-and-next-steps">Conclusion and next steps</h2>

<p>We are very happy with the results we achieved. The community has been very
helpful and responsive, and we are very grateful for all the help we received.
Going forward, we will hopefully be able to run scale tests periodically to
ensure that we are not regressing. Even small scale tests can be enough to
detect performance regressions as long as we keep track of the performance
metrics. This is something we hope to incorporate into the CI system in the
future.</p>]]></content><author><name>Lennart Jern</name></author><category term="metal3" /><category term="cluster API" /><category term="provider" /><category term="edge" /><summary type="html"><![CDATA[In part 1, we introduced the Bare Metal Operator test mode and saw how it can be used to play with BareMetalHosts without Ironic and without any actual hosts. We continued in part 2 with how to fake workload clusters enough for convincing Cluster API’s controllers that they are healthy. These two pieces together allowed us to run scaling tests and reach our target of 1000 single node clusters. In this final part of the blog post series, we will take a look at the results, the issues that we encountered and the improvements that have been made.]]></summary></entry><entry><title type="html">Metal3 at KubeCon EU 2024</title><link href="https://metal3.io/blog/2024/04/10/Metal3_at_KubeCon_EU_2024.html" rel="alternate" type="text/html" title="Metal3 at KubeCon EU 2024" /><published>2024-04-10T00:00:00-05:00</published><updated>2024-04-10T00:00:00-05:00</updated><id>https://metal3.io/blog/2024/04/10/Metal3_at_KubeCon_EU_2024</id><content type="html" xml:base="https://metal3.io/blog/2024/04/10/Metal3_at_KubeCon_EU_2024.html"><![CDATA[<p>The Metal3 project was present at KubeCon EU 2024 with multiple maintainers,
contributors and users! For many of us, this was the first time we met in the
physical world, despite working together for years already. This was very
valuable and appreciated by many of us, I am sure. We had time to casually
discuss ideas and proposals, hack together on the
<a href="https://github.com/metal3-io/ironic-standalone-operator">ironic-standalone-operator</a>
and simply get to know each other.</p>

<p><img src="/assets/2024-04-10-Metal3_at_KubeCon_EU_2024/lightningtalk.jpg" alt="Lightning
talk" /></p>

<p><em>Photo by Michael Captain.</em></p>

<p>As a project, we had the opportunity to give an update through a <a href="https://www.youtube.com/watch?v=6QsOQsQZQS8">lightning
talk</a> on Tuesday!</p>

<!-- markdownlint-disable no-inline-html -->
<iframe width="560" height="315" src="https://www.youtube.com/embed/6QsOQsQZQS8?si=bH5w9svPxM1NE8Le" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
<!-- markdownlint-enable no-inline-html -->

<p>On Wednesday we continued with a <a href="https://kccnceu2024.sched.com/event/1YheY">contribfest session</a>
where we gave an introduction to the project for potential new contributors. We
had prepared a number of good-first-issue’s that people could choose from if
they wanted. Perhaps more important though, was that we had time to answer
questions, discuss use-cases, issues and features with the attendees. The new
<a href="https://book.metal3.io/quick-start">quick-start</a> page was also launched just in
time for the contribfest. It should hopefully make it easier to get started with
the project and we encourage everyone to run through it and report or fix any
issues found.</p>

<p><img src="/assets/2024-04-10-Metal3_at_KubeCon_EU_2024/contribfest.jpg" alt="Contribfest" /></p>

<p><em>Photo from the official CNCF Flickr. More photos
<a href="https://www.flickr.com/photos/143247548@N03/53609847541/in/album-72177720315561784/">here</a>.</em></p>

<p>Finally, just like previous, we had a table in the Project Pavilion. There was a
lot of interest in Metal3, more than last year I would say. Even with five
maintainers working in parallel, we still had a hard time keeping up with the
amount of people stopping by to ask questions! My takeaway from this event is
that we still have work to do on explaining what Metal3 is and how it works. It
is quite uncommon that people know about baseboard management controllers (BMCs)
and this of course makes it harder to grasp what Metal3 is all about. However,
the interest is there, so we just need to get the information out there so that
people can learn! Another takeaway is that Cluster API in general seems to
really take off. Many people that came by our kiosk knew about Cluster API and
were interested in Metal3 because of the integration with have with it.</p>

<p>For those of you who couldn’t attend, I hope this post gives an idea about what
happened at KubeCon related to Metal3. Did you miss the contribfest? Maybe you
would like to contribute but don’t know where to start? Check out the
<a href="https://github.com/issues?page=1&amp;q=archived%3Afalse+user%3Ametal3-io+label%3A%22good+first+issue%22+is%3Aissue+sort%3Acreated-asc+is%3Aopen">good-first-issue’s</a>!
There are still plenty to choose from, and we will keep adding more.</p>]]></content><author><name>Lennart Jern</name></author><category term="metal3" /><category term="talk" /><category term="conference" /><category term="kubecon" /><summary type="html"><![CDATA[The Metal3 project was present at KubeCon EU 2024 with multiple maintainers, contributors and users! For many of us, this was the first time we met in the physical world, despite working together for years already. This was very valuable and appreciated by many of us, I am sure. We had time to casually discuss ideas and proposals, hack together on the ironic-standalone-operator and simply get to know each other.]]></summary></entry><entry><title type="html">How to run Metal3 website locally with Jekyll</title><link href="https://metal3.io/blog/2024/01/18/How_to_run_Metal3_website_locally_with_Jekyll.html" rel="alternate" type="text/html" title="How to run Metal3 website locally with Jekyll" /><published>2024-01-18T00:00:00-06:00</published><updated>2024-01-18T00:00:00-06:00</updated><id>https://metal3.io/blog/2024/01/18/How_to_run_Metal3_website_locally_with_Jekyll</id><content type="html" xml:base="https://metal3.io/blog/2024/01/18/How_to_run_Metal3_website_locally_with_Jekyll.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>If you’re a developer or contributor to the Metal3 project, you may need
to run the Metal3 website locally to test changes and ensure everything
looks as expected before deploying them. In this guide, we’ll walk you
through the process of setting up and running Metal3’s website locally
on your machine using Jekyll.</p>

<h2 id="prerequisites">Prerequisites</h2>

<p>Before we begin, make sure you have the following prerequisites
installed on your system:</p>

<ul>
  <li>
    <p>Ruby: Jekyll, the static site generator used by Metal3, is built with
Ruby. Install Ruby and its development tools by running the following
command in your terminal:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nb">sudo </span>apt <span class="nb">install </span>ruby-full
</code></pre></div>    </div>
  </li>
</ul>

<h2 id="setting-up-metal3s-website">Setting up Metal3’s Website</h2>

<p>Once Ruby is installed, we can proceed to set up Metal3’s website and
its dependencies. Follow these steps:</p>

<ul>
  <li>
    <p>Clone the Metal3 website repository from GitHub. Open your terminal
and navigate to the directory where you want to clone the repository,
then run the following command:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>git clone https://github.com/metal3-io/metal3-io.github.io.git
</code></pre></div>    </div>
  </li>
  <li>
    <p>Change to the cloned directory:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nb">cd </span>metal3-io.github.io
</code></pre></div>    </div>
  </li>
  <li>
    <p>Install the required gems and dependencies using Bundler. Run the
following command:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>bundle <span class="nb">install</span>
</code></pre></div>    </div>
  </li>
</ul>

<h2 id="running-the-metal3-website-locally">Running the Metal3 Website Locally</h2>

<p>With Metal3’s website and its dependencies installed, you can now start the local
development server to view and test the website. In the terminal, navigate to the
project’s root directory (<code class="language-plaintext highlighter-rouge">metal3-io.github.io</code>) and run the following command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>bundle <span class="nb">exec </span>jekyll serve
</code></pre></div></div>

<p>This command tells Jekyll to build the website and start a local server.
Once the server is running, you’ll see output indicating the local
address where the Metal3 website is being served, typically
<a href="http://localhost:4000">http://localhost:4000</a>.</p>

<p>Open your web browser and enter the provided address. Congratulations!
You should now see the Metal3 website running locally, allowing you to
preview your changes and ensure everything is working as expected.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Running Metal3’s website locally using Jekyll is a great way to test
changes and ensure the site functions properly before deploying them. By
following the steps outlined in this guide, you’ve successfully set up
and run Metal3’s website locally. Feel free to explore the Metal3
documentation and contribute to the project further.</p>]]></content><author><name>Salima Rabiu</name></author><category term="metal3" /><category term="baremetal" /><category term="metal3-dev-env" /><category term="documentation" /><category term="development" /><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Scaling to 1000 clusters - Part 2</title><link href="https://metal3.io/blog/2023/05/17/Scaling_part_2.html" rel="alternate" type="text/html" title="Scaling to 1000 clusters - Part 2" /><published>2023-05-17T00:00:00-05:00</published><updated>2023-05-17T00:00:00-05:00</updated><id>https://metal3.io/blog/2023/05/17/Scaling_part_2</id><content type="html" xml:base="https://metal3.io/blog/2023/05/17/Scaling_part_2.html"><![CDATA[<p>In <a href="/blog/2023/05/05/Scaling_part_1.html">part 1</a>, we introduced the Bare Metal Operator test mode and saw how it can be used to play with BareMetalHosts without Ironic and without any actual hosts.
Now we will take a look at the other end of the stack and how we can fake the workload cluster API’s.</p>

<h2 id="test-setup">Test setup</h2>

<p>The end goal is to have one management cluster where the Cluster API and Metal3 controllers run.
In this cluster we would generate BareMetalHosts and create Clusters, Metal3Clusters, etc to benchmark the controllers.
To give them a realistic test, we also need to fake the workload cluster API’s.
These will run separately in “backing” clusters to avoid interfering with the test (e.g. by using up all the resources in the management cluster).
Here is a diagram that describes the setup:</p>

<p><img src="/assets/2023-05-17-Scaling_part_2/scaling-fake-clusters.drawio.png" alt="diagram of test setup" /></p>

<p>How are we going to fake the workload cluster API’s then?
The most obvious solution is to just run the real deal, i.e. the <code class="language-plaintext highlighter-rouge">kube-apiserver</code>.
This is what would be run in a real workload cluster, together with the other components that make up the Kubernetes control plane.</p>

<p>If you want to follow along and try to set this up yourself, you will need at least the following tools installed:</p>

<ul>
  <li><a href="https://kind.sigs.k8s.io/docs/user/quick-start">kind</a></li>
  <li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl">kubectl</a></li>
  <li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">kubeadm</a></li>
  <li><a href="https://cluster-api.sigs.k8s.io/user/quick-start.html#install-clusterctl">clusterctl</a></li>
  <li><a href="https://github.com/openssl/openssl">openssl</a></li>
  <li><a href="https://curl.se/">curl</a></li>
  <li><a href="https://www.gnu.org/software/wget/">wget</a></li>
</ul>

<p>This has been tested with Kubernetes v1.25, kind v0.19 and clusterctl v1.4.2.
All script snippets are assumed to be for the <code class="language-plaintext highlighter-rouge">bash</code> shell.</p>

<h2 id="running-the-kubernetes-api-server">Running the Kubernetes API server</h2>

<p>There are many misconceptions, maybe even superstitions, about the Kubernetes control plane.
The fact is that it is in no way special.
It consists of a few programs that can be run in any way you want: in a container, as a systemd unit or directly executed at the command line.
They can run on a Node or outside of the cluster.
You can even run multiple instances on the same host as long as you avoid port collisions.</p>

<p>For our purposes we basically want to run as little as possible of the control plane components.
We just need the API to be available and possible for us to populate with data that the controllers expect to be there.
In other words, we need the API server and etcd.
The scheduler is not necessary since we won’t run any actual workload (we are just pretending the Nodes are there anyway) and the controller manager would just get in the way when we want to fake resources.
It would, for example, try to update the status of the (fake) Nodes that we want to create.</p>

<p>The API server will need an etcd instance to connect to.
It will also need some TLS configuration, both for connecting to etcd and for handling service accounts.
One simple way to generate the needed certificates is to use kubeadm.
But before we get there we need to think about how the configuration should look like.</p>

<p>For simplicity, we will simply run the API server and etcd in a kind cluster for now.
It would then be easy to run them in some other Kubernetes cluster later if needed.
Let’s create it right away:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>kind create cluster
<span class="c"># Note: This has been tested with node image</span>
<span class="c"># kindest/node:v1.26.3@sha256:61b92f38dff6ccc29969e7aa154d34e38b89443af1a2c14e6cfbd2df6419c66f</span>
</code></pre></div></div>

<p>To try to cut down on the resources required, we will also use a single multi-tenant etcd instance instead of one per API server.
We can rely on the internal service discovery so the API server can find etcd via an address like <code class="language-plaintext highlighter-rouge">etcd-server.etd-system.svc.cluster.local</code>, instead of using IP addresses.
Finally, we will need an endpoint where the API is exposed to the cluster where the controllers are running, but for now we can focus on just getting it up and running with <code class="language-plaintext highlighter-rouge">127.0.0.1:6443</code> as the endpoint.</p>

<p>Based on the above, we can create a <code class="language-plaintext highlighter-rouge">kubeadm-config.yaml</code> file like this:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubeadm.k8s.io/v1beta3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ClusterConfiguration</span>
<span class="na">apiServer</span><span class="pi">:</span>
  <span class="na">certSANs</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">127.0.0.1</span>
<span class="na">clusterName</span><span class="pi">:</span> <span class="s">test</span>
<span class="na">controlPlaneEndpoint</span><span class="pi">:</span> <span class="s">127.0.0.1:6443</span>
<span class="na">etcd</span><span class="pi">:</span>
  <span class="na">local</span><span class="pi">:</span>
    <span class="na">serverCertSANs</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">etcd-server.etcd-system.svc.cluster.local</span>
    <span class="na">peerCertSANs</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">etcd-0.etcd.etcd-system.svc.cluster.local</span>
<span class="na">kubernetesVersion</span><span class="pi">:</span> <span class="s">v1.25.3</span>
<span class="na">certificatesDir</span><span class="pi">:</span> <span class="s">/tmp/test/pki</span>
</code></pre></div></div>

<p>We can now use this to generate some certificates and upload them to the cluster:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Generate CA certificates</span>
kubeadm init phase certs etcd-ca <span class="nt">--config</span> kubeadm-config.yaml
kubeadm init phase certs ca <span class="nt">--config</span> kubeadm-config.yaml
<span class="c"># Generate etcd peer and server certificates</span>
kubeadm init phase certs etcd-peer <span class="nt">--config</span> kubeadm-config.yaml
kubeadm init phase certs etcd-server <span class="nt">--config</span> kubeadm-config.yaml

<span class="c"># Upload certificates</span>
kubectl create namespace etcd-system
kubectl <span class="nt">-n</span> etcd-system create secret tls test-etcd <span class="nt">--cert</span> /tmp/test/pki/etcd/ca.crt <span class="nt">--key</span> /tmp/test/pki/etcd/ca.key
kubectl <span class="nt">-n</span> etcd-system create secret tls etcd-peer <span class="nt">--cert</span> /tmp/test/pki/etcd/peer.crt <span class="nt">--key</span> /tmp/test/pki/etcd/peer.key
kubectl <span class="nt">-n</span> etcd-system create secret tls etcd-server <span class="nt">--cert</span> /tmp/test/pki/etcd/server.crt <span class="nt">--key</span> /tmp/test/pki/etcd/server.key
</code></pre></div></div>

<h3 id="deploying-a-multi-tenant-etcd-instance">Deploying a multi-tenant etcd instance</h3>

<p>Now it is time to deploy etcd!</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/manifests/v2/etcd.yaml <span class="se">\</span>
  | <span class="nb">sed</span> <span class="s2">"s/CLUSTER/test/g"</span> | kubectl <span class="nt">-n</span> etcd-system apply <span class="nt">-f</span> -
kubectl <span class="nt">-n</span> etcd-system <span class="nb">wait </span>sts/etcd <span class="nt">--for</span><span class="o">=</span><span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.status.availableReplicas}"</span><span class="o">=</span>1
</code></pre></div></div>

<p>As mentioned before, we want to create a <a href="https://etcd.io/docs/v3.5/op-guide/authentication/rbac/">multi-tenant etcd</a> that many API servers can share.
For this reason, we will need to create a root user and enable authentication for etcd:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Create root role</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  role add root
<span class="c"># Create root user</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  user add root <span class="nt">--new-user-password</span><span class="o">=</span><span class="s2">"rootpw"</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  user grant-role root root
<span class="c"># Enable authentication</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  auth <span class="nb">enable</span>
</code></pre></div></div>

<p>At this point we have a working etcd instance with authentication and TLS enabled.
Each client will need to have an etcd user to interact with this instance so we need to create an etcd user for the API server.
We already created a root user before so this should look familiar.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c">## Create etcd tenant</span>
<span class="c"># Create user</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="nt">--user</span> root:rootpw <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  user add <span class="nb">test</span> <span class="nt">--new-user-password</span><span class="o">=</span><span class="nb">test</span>
<span class="c"># Create role</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="nt">--user</span> root:rootpw <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  role add <span class="nb">test</span>
<span class="c"># Add read/write permissions for prefix to the role</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="nt">--user</span> root:rootpw <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  role grant-permission <span class="nb">test</span> <span class="nt">--prefix</span><span class="o">=</span><span class="nb">true </span>readwrite <span class="s2">"/test/"</span>
<span class="c"># Give the user permissions from the role</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="nt">--user</span> root:rootpw <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  user grant-role <span class="nb">test test</span>
</code></pre></div></div>

<p>From etcd’s point of view, everything is now ready.
The API server could theoretically use <code class="language-plaintext highlighter-rouge">etcdctl</code> and authenticate with the username and password that we created for it.
However, that is not how the API server works.
It expects to be able to authenticate using client certificates.
Luckily, etcd supports this so we just have to generate the certificates and sign them so that etcd trusts them.
The key thing is to set the common name in the certificate to the name of the user we want to authenticate as.</p>

<p>Since <code class="language-plaintext highlighter-rouge">kubeadm</code> always sets the same common name, we will here use <code class="language-plaintext highlighter-rouge">openssl</code> to generate the client certificates so that we get control over it.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Generate etcd client certificate</span>
openssl req <span class="nt">-newkey</span> rsa:2048 <span class="nt">-nodes</span> <span class="nt">-subj</span> <span class="s2">"/CN=test"</span> <span class="se">\</span>
 <span class="nt">-keyout</span> <span class="s2">"/tmp/test/pki/apiserver-etcd-client.key"</span> <span class="nt">-out</span> <span class="s2">"/tmp/test/pki/apiserver-etcd-client.csr"</span>
openssl x509 <span class="nt">-req</span> <span class="nt">-in</span> <span class="s2">"/tmp/test/pki/apiserver-etcd-client.csr"</span> <span class="se">\</span>
  <span class="nt">-CA</span> /tmp/test/pki/etcd/ca.crt <span class="nt">-CAkey</span> /tmp/test/pki/etcd/ca.key <span class="nt">-CAcreateserial</span> <span class="se">\</span>
  <span class="nt">-out</span> <span class="s2">"/tmp/test/pki/apiserver-etcd-client.crt"</span> <span class="nt">-days</span> 365
</code></pre></div></div>

<h3 id="deploying-the-api-server">Deploying the API server</h3>

<p>In order to deploy the API server, we will first need to generate some more certificates.
The client certificates for connecting to etcd are already ready, but it also needs certificates to secure the exposed API itself, and a few other things.
Then we will also need to create secrets from all of these certificates:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>kubeadm init phase certs ca <span class="nt">--config</span> kubeadm-config.yaml
kubeadm init phase certs apiserver <span class="nt">--config</span> kubeadm-config.yaml
kubeadm init phase certs sa <span class="nt">--cert-dir</span> /tmp/test/pki

kubectl create ns workload-api
kubectl <span class="nt">-n</span> workload-api create secret tls test-ca <span class="nt">--cert</span> /tmp/test/pki/ca.crt <span class="nt">--key</span> /tmp/test/pki/ca.key
kubectl <span class="nt">-n</span> workload-api create secret tls test-etcd <span class="nt">--cert</span> /tmp/test/pki/etcd/ca.crt <span class="nt">--key</span> /tmp/test/pki/etcd/ca.key
kubectl <span class="nt">-n</span> workload-api create secret tls <span class="s2">"test-apiserver-etcd-client"</span> <span class="se">\</span>
  <span class="nt">--cert</span> <span class="s2">"/tmp/test/pki/apiserver-etcd-client.crt"</span> <span class="se">\</span>
  <span class="nt">--key</span> <span class="s2">"/tmp/test/pki/apiserver-etcd-client.key"</span>
kubectl <span class="nt">-n</span> workload-api create secret tls apiserver <span class="se">\</span>
  <span class="nt">--cert</span> <span class="s2">"/tmp/test/pki/apiserver.crt"</span> <span class="se">\</span>
  <span class="nt">--key</span> <span class="s2">"/tmp/test/pki/apiserver.key"</span>
kubectl <span class="nt">-n</span> workload-api create secret generic test-sa <span class="se">\</span>
  <span class="nt">--from-file</span><span class="o">=</span>tls.crt<span class="o">=</span><span class="s2">"/tmp/test/pki/sa.pub"</span> <span class="se">\</span>
  <span class="nt">--from-file</span><span class="o">=</span>tls.key<span class="o">=</span><span class="s2">"/tmp/test/pki/sa.key"</span>
</code></pre></div></div>

<p>With all that out of the way, we can finally deploy the API server!
For this we will use a normal Deployment.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Deploy API server</span>
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/manifests/v2/kube-apiserver-deployment.yaml |
  <span class="nb">sed</span> <span class="s2">"s/CLUSTER/test/g"</span> | kubectl <span class="nt">-n</span> workload-api apply <span class="nt">-f</span> -
kubectl <span class="nt">-n</span> workload-api <span class="nb">wait</span> <span class="nt">--for</span><span class="o">=</span><span class="nv">condition</span><span class="o">=</span>Available deploy/test-kube-apiserver
</code></pre></div></div>

<p>Time to check if it worked!
We can use port-forwarding to access the API, but of course we will need some authentication method for it to be useful.
With kubeadm we can generate a kubeconfig based on the certificates we already have.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>kubeadm kubeconfig user <span class="nt">--client-name</span> kubernetes-admin <span class="nt">--org</span> system:masters <span class="se">\</span>
  <span class="nt">--config</span> kubeadm-config.yaml <span class="o">&gt;</span> kubeconfig.yaml
</code></pre></div></div>

<p>Now open another terminal and set up port-forwarding to the API server:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>kubectl <span class="nt">-n</span> workload-api port-forward svc/test-kube-apiserver 6443
</code></pre></div></div>

<p>Back in the original terminal, you should now be able to reach the workload API server:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>kubectl <span class="nt">--kubeconfig</span> kubeconfig.yaml cluster-info
</code></pre></div></div>

<p>Note that it won’t have any Nodes or Pods running.
It is completely empty since it is running on its own.
There is no kubelet that registered as a Node or applied static manifests, there is no scheduler or controller manager.
Exactly like we want it.</p>

<h2 id="faking-nodes-and-other-resources">Faking Nodes and other resources</h2>

<p>Let’s take a step back and think about what we have done so far.
We have deployed a Kubernetes API server and a multi-tenant etcd instance.
More API servers can be added in the same way, so it is straight forward to scale.
All of it runs in a kind cluster, which means that it is easy to set up and we can switch to any other Kubernetes cluster if needed later.
Through Kubernetes we also get an easy way to access the API servers by using port-forwarding, without exposing all of them separately.</p>

<p>The time has now come to think about what we need to put in the workload cluster API to convince the Cluster API and Metal3 controllers that it is healthy.
First of all they will expect to see Nodes that match the Machines and that they have a provider ID set.
Secondly, they will expect to see healthy control plane Pods.
Finally, they will try to check on the etcd cluster.</p>

<p>The final point is a problem, but we can work around it for now by configuring <a href="https://cluster-api.sigs.k8s.io/tasks/external-etcd.html">external etcd</a>.
It will lead to a different code path for the bootstrap and control plane controllers, but until we have something better it will be a good enough test.</p>

<p>Creating the Nodes and control plane Pods is really easy though.
We are just adding resources and there are no controllers or validating web hooks that can interfere.
Try it out!</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Create a Node</span>
kubectl <span class="nt">--kubeconfig</span><span class="o">=</span>kubeconfig.yaml create <span class="nt">-f</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/fake-node.yaml
<span class="c"># Check that it worked</span>
kubectl <span class="nt">--kubeconfig</span><span class="o">=</span>kubeconfig.yaml get nodes
<span class="c"># Maybe label it as part of the control plane?</span>
kubectl <span class="nt">--kubeconfig</span><span class="o">=</span>kubeconfig.yaml label node fake-node node-role.kubernetes.io/control-plane<span class="o">=</span><span class="s2">""</span>
</code></pre></div></div>

<p>Now add a Pod:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>kubectl <span class="nt">--kubeconfig</span><span class="o">=</span>kubeconfig.yaml create <span class="nt">-f</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-apiserver-pod.yaml
<span class="c"># Set status on the pods (it is not added when using create/apply).</span>
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-apiserver-pod-status.yaml |
  kubectl <span class="nt">--kubeconfig</span><span class="o">=</span>kubeconfig.yaml <span class="nt">-n</span> kube-system patch pod kube-apiserver-node-name <span class="se">\</span>
    <span class="nt">--subresource</span><span class="o">=</span>status <span class="nt">--patch-file</span><span class="o">=</span>/dev/stdin
</code></pre></div></div>

<p>You should be able to see something like this:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">$</span><span class="w"> </span>kubectl <span class="nt">--kubeconfig</span> kubeconfig.yaml get pods <span class="nt">-A</span>
<span class="go">NAMESPACE     NAME                       READY   STATUS    RESTARTS   AGE
kube-system   kube-apiserver-node-name   1/1     Running   0          16h
</span><span class="gp">$</span><span class="w"> </span>kubectl <span class="nt">--kubeconfig</span> kubeconfig.yaml get nodes
<span class="go">NAME        STATUS   ROLES    AGE   VERSION
</span><span class="gp">fake-node   Ready    &lt;none&gt;</span><span class="w">   </span>16h   v1.25.3
</code></pre></div></div>

<p>Now all we have to do is to ensure that the API returns information that the controllers expect.</p>

<h2 id="hooking-up-the-api-server-to-a-cluster-api-cluster">Hooking up the API server to a Cluster API cluster</h2>

<p>We will now set up a fresh cluster where we can run the Cluster API and Metal3 controllers.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Delete the previous cluster</span>
kind delete cluster
<span class="c"># Create a fresh new cluster</span>
kind create cluster
<span class="c"># Initialize Cluster API with Metal3</span>
clusterctl init <span class="nt">--infrastructure</span> metal3
<span class="c">## Deploy the Bare Metal Opearator</span>
<span class="c"># Create the namespace where it will run</span>
kubectl create ns baremetal-operator-system
<span class="c"># Deploy it in normal mode</span>
kubectl apply <span class="nt">-k</span> https://github.com/metal3-io/baremetal-operator/config/default
<span class="c"># Patch it to run in test mode</span>
kubectl patch <span class="nt">-n</span> baremetal-operator-system deploy baremetal-operator-controller-manager <span class="nt">--type</span><span class="o">=</span>json <span class="se">\</span>
  <span class="nt">-p</span><span class="o">=</span><span class="s1">'[{"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--test-mode"}]'</span>
</code></pre></div></div>

<p>You should now have a cluster with the Cluster API, Metal3 provider and Bare Metal Operator running.
Next, we will prepare some files that will come in handy later, namely a cluster template, BareMetalHost manifest and Kubeadm configuration file.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Download cluster-template</span>
<span class="nv">CLUSTER_TEMPLATE</span><span class="o">=</span>/tmp/cluster-template.yaml
<span class="c"># https://github.com/metal3-io/cluster-api-provider-metal3/blob/main/examples/clusterctl-templates/clusterctl-cluster.yaml</span>
<span class="nv">CLUSTER_TEMPLATE_URL</span><span class="o">=</span><span class="s2">"https://raw.githubusercontent.com/metal3-io/cluster-api-provider-metal3/main/examples/clusterctl-templates/clusterctl-cluster.yaml"</span>
wget <span class="nt">-O</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_TEMPLATE</span><span class="k">}</span><span class="s2">"</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_TEMPLATE_URL</span><span class="k">}</span><span class="s2">"</span>

<span class="c"># Save a manifest of a BareMetalHost</span>
<span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh"> &gt; /tmp/test-hosts.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: worker-1-bmc-secret
type: Opaque
data:
  username: YWRtaW4=
  password: cGFzc3dvcmQ=
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: worker-1
spec:
  online: true
  bmc:
    address: libvirt://192.168.122.1:6233/
    credentialsName: worker-1-bmc-secret
  bootMACAddress: "00:60:2F:10:E9:A7"
</span><span class="no">EOF

</span><span class="c"># Save a kubeadm config template</span>
<span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh"> &gt; /tmp/kubeadm-config-template.yaml
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs:
    - localhost
    - 127.0.0.1
    - 0.0.0.0
    - HOST
clusterName: test
controlPlaneEndpoint: HOST:6443
etcd:
  local:
    serverCertSANs:
      - etcd-server.etcd-system.svc.cluster.local
    peerCertSANs:
      - etcd-0.etcd.etcd-system.svc.cluster.local
kubernetesVersion: v1.25.3
certificatesDir: /tmp/CLUSTER/pki
</span><span class="no">EOF
</span></code></pre></div></div>

<p>With this we have enough to start creating the workload cluster.
First, we need to set up some certificates.
This should look very familiar from earlier when we created certificates for the Kubernetes API server and etcd.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nb">mkdir</span> <span class="nt">-p</span> /tmp/pki/etcd
<span class="nv">CLUSTER</span><span class="o">=</span><span class="s2">"test"</span>
<span class="nv">NAMESPACE</span><span class="o">=</span>etcd-system
<span class="nv">CLUSTER_APIENDPOINT_HOST</span><span class="o">=</span><span class="s2">"test-kube-apiserver.</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">.svc.cluster.local"</span>

<span class="nb">sed</span> <span class="nt">-e</span> <span class="s2">"s/NAMESPACE/</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">/g"</span> <span class="nt">-e</span> <span class="s2">"s/</span><span class="se">\/</span><span class="s2">CLUSTER//g"</span> <span class="nt">-e</span> <span class="s2">"s/HOST/</span><span class="k">${</span><span class="nv">CLUSTER_APIENDPOINT_HOST</span><span class="k">}</span><span class="s2">/g"</span> <span class="se">\</span>
  /tmp/kubeadm-config-template.yaml <span class="o">&gt;</span> <span class="s2">"/tmp/kubeadm-config-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span>

<span class="c"># Generate CA certificates</span>
kubeadm init phase certs etcd-ca <span class="nt">--config</span> <span class="s2">"/tmp/kubeadm-config-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span>
kubeadm init phase certs ca <span class="nt">--config</span> <span class="s2">"/tmp/kubeadm-config-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span>
<span class="c"># Generate etcd peer and server certificates</span>
kubeadm init phase certs etcd-peer <span class="nt">--config</span> <span class="s2">"/tmp/kubeadm-config-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span>
kubeadm init phase certs etcd-server <span class="nt">--config</span> <span class="s2">"/tmp/kubeadm-config-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span>
</code></pre></div></div>

<p>Next, we create the namespace, the BareMetalHost and secrets from the certificates:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nv">CLUSTER</span><span class="o">=</span>test-1
<span class="nv">NAMESPACE</span><span class="o">=</span>test-1
kubectl create namespace <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span>
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> apply <span class="nt">-f</span> /tmp/test-hosts.yaml
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">-etcd"</span> <span class="nt">--cert</span> /tmp/pki/etcd/ca.crt <span class="nt">--key</span> /tmp/pki/etcd/ca.key
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">-ca"</span> <span class="nt">--cert</span> /tmp/pki/ca.crt <span class="nt">--key</span> /tmp/pki/ca.key
</code></pre></div></div>

<p>We are now ready to create the cluster!
We just need a few variables for the template.
The important part here is the <code class="language-plaintext highlighter-rouge">CLUSTER_APIENDPOINT_HOST</code> and <code class="language-plaintext highlighter-rouge">CLUSTER_APIENDPOINT_PORT</code>, since this will be used by the controllers to connect to the workload cluster API.
You should set the IP to the private IP of the test machine or similar.
This way we can use port-forwarding to expose the API on this IP, which the controllers can then reach.
The port just have to be one not in use, and preferably something that is easy to remember and associate with the correct cluster.
For example, cluster 1 gets port 10001, cluster 2 gets 10002, etc.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nb">export </span><span class="nv">IMAGE_CHECKSUM</span><span class="o">=</span><span class="s2">"97830b21ed272a3d854615beb54cf004"</span>
<span class="nb">export </span><span class="nv">IMAGE_CHECKSUM_TYPE</span><span class="o">=</span><span class="s2">"md5"</span>
<span class="nb">export </span><span class="nv">IMAGE_FORMAT</span><span class="o">=</span><span class="s2">"raw"</span>
<span class="nb">export </span><span class="nv">IMAGE_URL</span><span class="o">=</span><span class="s2">"http://172.22.0.1/images/rhcos-ootpa-latest.qcow2"</span>
<span class="nb">export </span><span class="nv">KUBERNETES_VERSION</span><span class="o">=</span><span class="s2">"v1.25.3"</span>
<span class="nb">export </span><span class="nv">WORKERS_KUBEADM_EXTRA_CONFIG</span><span class="o">=</span><span class="s2">""</span>
<span class="nb">export </span><span class="nv">CLUSTER_APIENDPOINT_HOST</span><span class="o">=</span><span class="s2">"172.17.0.2"</span>
<span class="nb">export </span><span class="nv">CLUSTER_APIENDPOINT_PORT</span><span class="o">=</span><span class="s2">"10001"</span>
<span class="nb">export </span><span class="nv">CTLPLANE_KUBEADM_EXTRA_CONFIG</span><span class="o">=</span><span class="s2">"
    clusterConfiguration:
      controlPlaneEndpoint: </span><span class="k">${</span><span class="nv">CLUSTER_APIENDPOINT_HOST</span><span class="k">}</span><span class="s2">:</span><span class="k">${</span><span class="nv">CLUSTER_APIENDPOINT_PORT</span><span class="k">}</span><span class="s2">
      apiServer:
        certSANs:
        - localhost
        - 127.0.0.1
        - 0.0.0.0
        - </span><span class="k">${</span><span class="nv">CLUSTER_APIENDPOINT_HOST</span><span class="k">}</span><span class="s2">
      etcd:
        external:
          endpoints:
            - https://etcd-server:2379
          caFile: /etc/kubernetes/pki/etcd/ca.crt
          certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt
          keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key"</span>
</code></pre></div></div>

<p>Create the cluster!</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>clusterctl generate cluster <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="nt">--from</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_TEMPLATE</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="nt">--target-namespace</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> | kubectl apply <span class="nt">-f</span> -
</code></pre></div></div>

<p>This will give you a cluster and all the templates and other resources that are needed.
However, we will need to fill in for the non-existent hardware and create the workload cluster API server, like we practiced before.
This time it is slightly different, because some of the steps are handled by the Cluster API.
We just need to take care of what would happen on the node, plus the etcd part since we are using external etcd configuration.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nb">mkdir</span> <span class="nt">-p</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/etcd"</span>

<span class="c"># Generate etcd client certificate</span>
openssl req <span class="nt">-newkey</span> rsa:2048 <span class="nt">-nodes</span> <span class="nt">-subj</span> <span class="s2">"/CN=</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
 <span class="nt">-keyout</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver-etcd-client.key"</span> <span class="nt">-out</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver-etcd-client.csr"</span>
openssl x509 <span class="nt">-req</span> <span class="nt">-in</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver-etcd-client.csr"</span> <span class="se">\</span>
  <span class="nt">-CA</span> /tmp/pki/etcd/ca.crt <span class="nt">-CAkey</span> /tmp/pki/etcd/ca.key <span class="nt">-CAcreateserial</span> <span class="se">\</span>
  <span class="nt">-out</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver-etcd-client.crt"</span> <span class="nt">-days</span> 365

<span class="c"># Get the k8s ca certificate and key.</span>
<span class="c"># This is used by kubeadm to generate the api server certificates</span>
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> get secrets <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">-ca"</span> <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.data.tls</span><span class="se">\.</span><span class="s2">crt}"</span> | <span class="nb">base64</span> <span class="nt">-d</span> <span class="o">&gt;</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/ca.crt"</span>
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> get secrets <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">-ca"</span> <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.data.tls</span><span class="se">\.</span><span class="s2">key}"</span> | <span class="nb">base64</span> <span class="nt">-d</span> <span class="o">&gt;</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/ca.key"</span>

<span class="c"># Generate certificates</span>
<span class="nb">sed</span> <span class="nt">-e</span> <span class="s2">"s/NAMESPACE/</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">/g"</span> <span class="nt">-e</span> <span class="s2">"s/CLUSTER/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/g"</span> <span class="nt">-e</span> <span class="s2">"s/HOST/</span><span class="k">${</span><span class="nv">CLUSTER_APIENDPOINT_HOST</span><span class="k">}</span><span class="s2">/g"</span> <span class="se">\</span>
  /tmp/kubeadm-config-template.yaml <span class="o">&gt;</span> <span class="s2">"/tmp/kubeadm-config-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span>
kubeadm init phase certs apiserver <span class="nt">--config</span> <span class="s2">"/tmp/kubeadm-config-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span>

<span class="c"># Create secrets</span>
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">-apiserver-etcd-client"</span> <span class="nt">--cert</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver-etcd-client.crt"</span> <span class="nt">--key</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver-etcd-client.key"</span>
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls apiserver <span class="nt">--cert</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver.crt"</span> <span class="nt">--key</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver.key"</span>
</code></pre></div></div>

<p>Now we will need to set up the fake cluster resources.
For this we will create a second kind cluster and set up etcd, just like we did before.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Note: This will create a kubeconfig context named kind-backing-cluster-1,</span>
<span class="c"># i.e. "kind-" is prefixed to the name.</span>
kind create cluster <span class="nt">--name</span> backing-cluster-1

<span class="c"># Setup central etcd</span>
<span class="nv">CLUSTER</span><span class="o">=</span><span class="s2">"test"</span>
<span class="nv">NAMESPACE</span><span class="o">=</span>etcd-system
kubectl create namespace <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span>

<span class="c"># Upload certificates</span>
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">-etcd"</span> <span class="nt">--cert</span> /tmp/pki/etcd/ca.crt <span class="nt">--key</span> /tmp/pki/etcd/ca.key
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls etcd-peer <span class="nt">--cert</span> /tmp/pki/etcd/peer.crt <span class="nt">--key</span> /tmp/pki/etcd/peer.key
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls etcd-server <span class="nt">--cert</span> /tmp/pki/etcd/server.crt <span class="nt">--key</span> /tmp/pki/etcd/server.key

<span class="c"># Deploy ETCD</span>
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/manifests/v2/etcd.yaml <span class="se">\</span>
  | <span class="nb">sed</span> <span class="s2">"s/CLUSTER/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/g"</span> | kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> apply <span class="nt">-f</span> -
kubectl <span class="nt">-n</span> etcd-system <span class="nb">wait </span>sts/etcd <span class="nt">--for</span><span class="o">=</span><span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.status.availableReplicas}"</span><span class="o">=</span>1

<span class="c"># Create root role</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  role add root
<span class="c"># Create root user</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  user add root <span class="nt">--new-user-password</span><span class="o">=</span><span class="s2">"rootpw"</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  user grant-role root root
<span class="c"># Enable authentication</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  auth <span class="nb">enable</span>
</code></pre></div></div>

<p>Switch the context back to the first cluster with <code class="language-plaintext highlighter-rouge">kubectl config use-context kind-kind</code> so we don’t get confused about which is the main cluster.
We will now need to put all the expected certificates for the fake cluster in the <code class="language-plaintext highlighter-rouge">kind-backing-cluster-1</code> so that they can be used by the API server that we will deploy there.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nv">CLUSTER</span><span class="o">=</span>test-1
<span class="nv">NAMESPACE</span><span class="o">=</span>test-1
<span class="c"># Setup fake resources for cluster test-1</span>
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 create namespace <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span>
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">-etcd"</span> <span class="nt">--cert</span> /tmp/pki/etcd/ca.crt <span class="nt">--key</span> /tmp/pki/etcd/ca.key
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">-ca"</span> <span class="nt">--cert</span> /tmp/pki/ca.crt <span class="nt">--key</span> /tmp/pki/ca.key
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">-apiserver-etcd-client"</span> <span class="nt">--cert</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver-etcd-client.crt"</span> <span class="nt">--key</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver-etcd-client.key"</span>
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls apiserver <span class="nt">--cert</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver.crt"</span> <span class="nt">--key</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver.key"</span>

kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> get secrets <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">-sa"</span> <span class="nt">-o</span> yaml | kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 create <span class="nt">-f</span> -

<span class="c">## Create etcd tenant</span>
<span class="c"># Create user</span>
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="nt">--user</span> root:rootpw <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  user add <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--new-user-password</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">"</span>
<span class="c"># Create role</span>
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="nt">--user</span> root:rootpw <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  role add <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">"</span>
<span class="c"># Add read/write permissions for prefix to the role</span>
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="nt">--user</span> root:rootpw <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  role grant-permission <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--prefix</span><span class="o">=</span><span class="nb">true </span>readwrite <span class="s2">"/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/"</span>
<span class="c"># Give the user permissions from the role</span>
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="nt">--user</span> root:rootpw <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  user grant-role <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">"</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">"</span>
</code></pre></div></div>

<p>Check that the Metal3Machine is associated with a BareMetalHost.
Deploy the API server.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Deploy API server</span>
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/manifests/v2/kube-apiserver-deployment.yaml |
  <span class="nb">sed</span> <span class="nt">-e</span> <span class="s2">"s/CLUSTER/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/g"</span> | kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> apply <span class="nt">-f</span> -
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> <span class="nb">wait</span> <span class="nt">--for</span><span class="o">=</span><span class="nv">condition</span><span class="o">=</span>Available deploy/test-kube-apiserver

<span class="c"># Get kubeconfig</span>
clusterctl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> get kubeconfig <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">"</span> <span class="o">&gt;</span> <span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span>
<span class="c"># Edit kubeconfig to point to 127.0.0.1:${CLUSTER_APIENDPOINT_PORT}</span>
<span class="nb">sed</span> <span class="nt">-i</span> <span class="nt">-e</span> <span class="s2">"s/</span><span class="k">${</span><span class="nv">CLUSTER_APIENDPOINT_HOST</span><span class="k">}</span><span class="s2">/127.0.0.1/"</span> <span class="nt">-e</span> <span class="s2">"s/:6443/:</span><span class="k">${</span><span class="nv">CLUSTER_APIENDPOINT_PORT</span><span class="k">}</span><span class="s2">/"</span> <span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span>
<span class="c"># Port forward for accessing the API</span>
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> port-forward <span class="se">\</span>
      <span class="nt">--address</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_APIENDPOINT_HOST</span><span class="k">}</span><span class="s2">,127.0.0.1"</span> svc/test-kube-apiserver <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_APIENDPOINT_PORT</span><span class="k">}</span><span class="s2">"</span>:6443 &amp;
<span class="c"># Check that it is working</span>
kubectl <span class="nt">--kubeconfig</span><span class="o">=</span><span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span> cluster-info
</code></pre></div></div>

<p>Now that we have a working API for the workload cluster, the only remaining thing is to put everything that the controllers expect in it.
This includes adding a Node to match the Machine as well as static pods that Cluster API expects to be there.
Let’s start with the Node!
The Node must have the correct name and a label with the BareMetalHost UID so that the controllers can put the correct provider ID on it.
We have only created 1 BareMetalHost so it is easy to pick the correct one.
The name of the Node should be the same as the Machine, which is also only a single one.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nv">machine</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span>kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> get machine <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[0].metadata.name}"</span><span class="si">)</span><span class="s2">"</span>
<span class="nv">bmh_uid</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span>kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> get bmh <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[0].metadata.uid}"</span><span class="si">)</span><span class="s2">"</span>
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/fake-node.yaml |
  <span class="nb">sed</span> <span class="nt">-e</span> <span class="s2">"s/fake-node/</span><span class="k">${</span><span class="nv">machine</span><span class="k">}</span><span class="s2">/g"</span> <span class="nt">-e</span> <span class="s2">"s/fake-uuid/</span><span class="k">${</span><span class="nv">bmh_uid</span><span class="k">}</span><span class="s2">/g"</span> | <span class="se">\</span>
  kubectl <span class="nt">--kubeconfig</span><span class="o">=</span><span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span> create <span class="nt">-f</span> -
<span class="c"># Label it as control-plane since this is a control-plane node.</span>
kubectl <span class="nt">--kubeconfig</span><span class="o">=</span><span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span> label node <span class="s2">"</span><span class="k">${</span><span class="nv">machine</span><span class="k">}</span><span class="s2">"</span> node-role.kubernetes.io/control-plane<span class="o">=</span><span class="s2">""</span>
<span class="c"># Upload kubeadm config to configmap. This will mark the KCP as initialized.</span>
kubectl <span class="nt">--kubeconfig</span><span class="o">=</span><span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span> <span class="nt">-n</span> kube-system create cm kubeadm-config <span class="se">\</span>
  <span class="nt">--from-file</span><span class="o">=</span><span class="nv">ClusterConfiguration</span><span class="o">=</span><span class="s2">"/tmp/kubeadm-config-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span>
</code></pre></div></div>

<p>This should be enough to make the Machines healthy!
You should be able to see something similar to this:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">$</span><span class="w"> </span>clusterctl <span class="nt">-n</span> test-1 describe cluster test-1
<span class="go">NAME                                            READY  SEVERITY  REASON  SINCE  MESSAGE
Cluster/test-1                                  True                     46s
├─ClusterInfrastructure - Metal3Cluster/test-1  True                     114m
└─ControlPlane - KubeadmControlPlane/test-1     True                     46s
  └─Machine/test-1-f2nw2                        True                     47s
</span></code></pre></div></div>

<p>However, if you check the KubeadmControlPlane more carefully, you will notice that it is still complaining about control plane components.
This is because we have not created the static pods yet, and it is also unable to check the certificate expiration date for the Machine.
Let’s fix it:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Add static pods to make kubeadm control plane manager happy</span>
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-apiserver-pod.yaml |
  <span class="nb">sed</span> <span class="s2">"s/node-name/</span><span class="k">${</span><span class="nv">machine</span><span class="k">}</span><span class="s2">/g"</span> |
  kubectl <span class="nt">--kubeconfig</span><span class="o">=</span><span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span> create <span class="nt">-f</span> -
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-controller-manager-pod.yaml |
  <span class="nb">sed</span> <span class="s2">"s/node-name/</span><span class="k">${</span><span class="nv">machine</span><span class="k">}</span><span class="s2">/g"</span> |
  kubectl <span class="nt">--kubeconfig</span><span class="o">=</span><span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span> create <span class="nt">-f</span> -
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-scheduler-pod.yaml |
  <span class="nb">sed</span> <span class="s2">"s/node-name/</span><span class="k">${</span><span class="nv">machine</span><span class="k">}</span><span class="s2">/g"</span> |
  kubectl <span class="nt">--kubeconfig</span><span class="o">=</span><span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span> create <span class="nt">-f</span> -
<span class="c"># Set status on the pods (it is not added when using create/apply).</span>
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-apiserver-pod-status.yaml |
  kubectl <span class="nt">--kubeconfig</span><span class="o">=</span><span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span> <span class="nt">-n</span> kube-system patch pod <span class="s2">"kube-apiserver-</span><span class="k">${</span><span class="nv">machine</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="nt">--subresource</span><span class="o">=</span>status <span class="nt">--patch-file</span><span class="o">=</span>/dev/stdin
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-controller-manager-pod-status.yaml |
  kubectl <span class="nt">--kubeconfig</span><span class="o">=</span><span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span> <span class="nt">-n</span> kube-system patch pod <span class="s2">"kube-controller-manager-</span><span class="k">${</span><span class="nv">machine</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="nt">--subresource</span><span class="o">=</span>status <span class="nt">--patch-file</span><span class="o">=</span>/dev/stdin
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-scheduler-pod-status.yaml |
  kubectl <span class="nt">--kubeconfig</span><span class="o">=</span><span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span> <span class="nt">-n</span> kube-system patch pod <span class="s2">"kube-scheduler-</span><span class="k">${</span><span class="nv">machine</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="nt">--subresource</span><span class="o">=</span>status <span class="nt">--patch-file</span><span class="o">=</span>/dev/stdin

<span class="c"># Add certificate expiry annotations to make kubeadm control plane manager happy</span>
<span class="nv">CERT_EXPIRY_ANNOTATION</span><span class="o">=</span><span class="s2">"machine.cluster.x-k8s.io/certificates-expiry"</span>
<span class="nv">EXPIRY_TEXT</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span>kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> get secret apiserver <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.data.tls</span><span class="se">\.</span><span class="s2">crt}"</span> | <span class="nb">base64</span> <span class="nt">-d</span> | openssl x509 <span class="nt">-enddate</span> <span class="nt">-noout</span> | <span class="nb">cut</span> <span class="nt">-d</span><span class="o">=</span> <span class="nt">-f</span> 2<span class="si">)</span><span class="s2">"</span>
<span class="nv">EXPIRY</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span><span class="nb">date</span> <span class="nt">--date</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">EXPIRY_TEXT</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--iso-8601</span><span class="o">=</span>seconds<span class="si">)</span><span class="s2">"</span>
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> annotate machine <span class="s2">"</span><span class="k">${</span><span class="nv">machine</span><span class="k">}</span><span class="s2">"</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CERT_EXPIRY_ANNOTATION</span><span class="k">}</span><span class="s2">=</span><span class="k">${</span><span class="nv">EXPIRY</span><span class="k">}</span><span class="s2">"</span>
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> annotate kubeadmconfig <span class="nt">--all</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CERT_EXPIRY_ANNOTATION</span><span class="k">}</span><span class="s2">=</span><span class="k">${</span><span class="nv">EXPIRY</span><span class="k">}</span><span class="s2">"</span>
</code></pre></div></div>

<p>Now we finally have a completely healthy cluster as far as the controllers are concerned.</p>

<h2 id="conclusions-and-summary">Conclusions and summary</h2>

<p>We now have all the tools necessary to start experimenting.</p>

<ul>
  <li>With the BareMetal Operator running in test mode, we can skip Ironic and still work with BareMetalHosts that act like normal.</li>
  <li>We can set up separate “backing” clusters where we run etcd and multiple API servers to fake the workload cluster API’s.</li>
  <li>Fake Nodes and Pods can be easily added to the workload cluster API’s, and configured as we want.</li>
  <li>The workload cluster API’s can be exposed to the controllers in the test cluster using port-forwarding.</li>
</ul>

<p>In this post we have not automated all of this, but if you want to see a scripted setup, take a look at <a href="https://github.com/Nordix/metal3-clusterapi-docs/tree/main/metal3-scaling-experiments">this</a>.
It is what we used to scale to 1000 clusters.
Just remember that it may need some tweaking for your specific environment if you want to try it out!</p>

<p>Specifically we used 10 “backing” clusters, i.e. 10 separate cloud VMs with kind clusters where we run etcd and the workload cluster API’s.
Each one would hold 100 API servers.
The test cluster was on its own separate VM also running a kind cluster with all the controllers and all the Cluster objects, etc.</p>

<p>In the next and final blog post of this series we will take a look at the results of all this.
What issues did we run into along the way?
How did we fix or work around them?
We will also take a look at what is going on in the community related to this and discuss potential future work in the area.</p>]]></content><author><name>Lennart Jern</name></author><category term="metal3" /><category term="cluster API" /><category term="provider" /><category term="edge" /><summary type="html"><![CDATA[In part 1, we introduced the Bare Metal Operator test mode and saw how it can be used to play with BareMetalHosts without Ironic and without any actual hosts. Now we will take a look at the other end of the stack and how we can fake the workload cluster API’s.]]></summary></entry><entry><title type="html">Scaling to 1000 clusters - Part 1</title><link href="https://metal3.io/blog/2023/05/05/Scaling_part_1.html" rel="alternate" type="text/html" title="Scaling to 1000 clusters - Part 1" /><published>2023-05-05T00:00:00-05:00</published><updated>2023-05-05T00:00:00-05:00</updated><id>https://metal3.io/blog/2023/05/05/Scaling_part_1</id><content type="html" xml:base="https://metal3.io/blog/2023/05/05/Scaling_part_1.html"><![CDATA[<p>We want to ensure that Metal3 can scale to thousands of nodes and clusters.
However, running tests with thousands of real servers is expensive and we don’t have access to any such large environment in the project.
So instead we have been focusing on faking the hardware while trying to keep things as realistic as possible for the controllers.
In this first part we will take a look at the Bare Metal Operator and the <a href="https://github.com/metal3-io/baremetal-operator/blob/b76dde223937009cebb9da85e6f1793a544675e6/docs/dev-setup.md?plain=1#L62">test mode</a> it offers.
The next part will be about how to fake the Kubernetes API of the workload clusters.
In the final post we will take a look at the issues we ran into and what is being done in the community to address them so that we can keep scaling!</p>

<h2 id="some-background-on-how-to-fool-the-controllers">Some background on how to fool the controllers</h2>

<p>With the full Metal3 stack, from Ironic to Cluster API, we have the following controllers that operate on Kubernetes APIs:</p>

<ul>
  <li>Cluster API Kubeadm control plane controller</li>
  <li>Cluster API Kubeadm bootstrap controller</li>
  <li>Cluster API controller</li>
  <li>Cluster API provider for Metal3 controller</li>
  <li>IP address manager controller</li>
  <li>Bare Metal Operator controller</li>
</ul>

<p>We will first focus on the controllers that interact with Nodes, Machines, Metal3Machines and BareMetalHosts, i.e. objects related to actual physical machines that we need to fake.
In other words, we are skipping the IP address manager for now.</p>

<p>What do these controllers care about really?
What do we need to do to fool them?
At the Cluster API level, the controllers just care about the Kubernetes resources in the management cluster (e.g. Clusters and Machines) and some resources in the workload cluster (e.g. Nodes and the etcd Pods).
The controllers will try to connect to the workload clusters in order to check the status of the resources there, so if there is no real workload cluster, this is something we will need to fake if we want to fool the controllers.
When it comes to Cluster API provider for Metal3, it connects the abstract high level objects with the BareMetalHosts, so here we will need to make the BareMetalHosts to behave realistically in order to provide a good test.</p>

<p>This is where the Bare Metal Operator test mode comes in.
If we can fake the workload cluster API and the BareMetalHosts, then all the Cluster API controllers and the Metal3 provider will get a realistic test that we can use when working on scalability.</p>

<h2 id="bare-metal-operator-test-mode">Bare Metal Operator test mode</h2>

<p>The Bare Metal Operator has a test mode, in which it doesn’t talk to Ironic.
Instead it just pretends that everything is fine and all actions succeed.
In this mode the BareMetalHosts will move through the state diagram just like they normally would (but quite a bit faster).
To enable it, all you have to do is add the <code class="language-plaintext highlighter-rouge">-test-mode</code> flag when running the Bare Metal Operator controller.
For convenience there is also a make target (<code class="language-plaintext highlighter-rouge">make run-test-mode</code>) that will run the Bare Metal Operator directly on the host in test mode.</p>

<p>Here is an example of how to use it.
You will need kind and kubectl installed for this to work, but you don’t need the Bare Metal Operator repository cloned.</p>

<ol>
  <li>
    <p>Create a kind cluster and deploy cert-manager (needed for web hook certificates):</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>kind create cluster
<span class="c"># Install cert-manager</span>
kubectl apply <span class="nt">-f</span> https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/cert-manager.yaml
</code></pre></div>    </div>
  </li>
  <li>
    <p>Deploy the Bare Metal Operator in test mode:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Create the namespace where it will run</span>
kubectl create ns baremetal-operator-system
<span class="c"># Deploy it in normal mode</span>
kubectl apply <span class="nt">-k</span> https://github.com/metal3-io/baremetal-operator/config/default
<span class="c"># Patch it to run in test mode</span>
kubectl patch <span class="nt">-n</span> baremetal-operator-system deploy baremetal-operator-controller-manager <span class="nt">--type</span><span class="o">=</span>json <span class="se">\</span>
  <span class="nt">-p</span><span class="o">=</span><span class="s1">'[{"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--test-mode"}]'</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>In a separate terminal, create a BareMetalHost from the example manifests:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>kubectl apply <span class="nt">-f</span> https://github.com/metal3-io/baremetal-operator/raw/main/examples/example-host.yaml
</code></pre></div>    </div>
  </li>
</ol>

<p>After applying the BareMetalHost, it will quickly go through <code class="language-plaintext highlighter-rouge">registering</code> and become <code class="language-plaintext highlighter-rouge">available</code>.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">$</span><span class="w"> </span>kubectl get bmh
<span class="go">NAME                    STATE         CONSUMER   ONLINE   ERROR   AGE
example-baremetalhost   registering              true             2s
</span><span class="gp">$</span><span class="w"> </span>kubectl get bmh
<span class="go">NAME                    STATE       CONSUMER   ONLINE   ERROR   AGE
example-baremetalhost   available              true             6s
</span></code></pre></div></div>

<p>We can now provision the BareMetalHost, turn it off, deprovision, etc.
Just like normal, except that the machine doesn’t exist.
Let’s try provisioning it!</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>kubectl patch bmh example-baremetalhost <span class="nt">--type</span><span class="o">=</span>merge <span class="nt">--patch-file</span><span class="o">=</span>/dev/stdin <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
spec:
  image:
    url: "http://example.com/totally-fake-image.vmdk"
    checksum: "made-up-checksum"
    format: vmdk
</span><span class="no">EOF
</span></code></pre></div></div>

<p>You will see it go through <code class="language-plaintext highlighter-rouge">provisioning</code> and end up in <code class="language-plaintext highlighter-rouge">provisioned</code> state:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">$</span><span class="w"> </span>kubectl get bmh
<span class="go">NAME                    STATE          CONSUMER   ONLINE   ERROR   AGE
example-baremetalhost   provisioning              true             7m20s

</span><span class="gp">$</span><span class="w"> </span>kubectl get bmh
<span class="go">NAME                    STATE         CONSUMER   ONLINE   ERROR   AGE
example-baremetalhost   provisioned              true             7m22s
</span></code></pre></div></div>

<h2 id="wrapping-up">Wrapping up</h2>

<p>With Bare Metal Operator in test mode, we have the foundation for starting our scalability journey.
We can easily create BareMetalHost objects and they behave similar to what they would in a real scenario.
A simple bash script will at this point allow us to create as many BareMetalHosts as we would like.
To wrap things up, we will now do just that: put together a script and try generating a few BareMetalHosts.</p>

<p>The script will do the same thing we did before when creating the example BareMetalHost, but it will also give them different names so we don’t get naming collisions.
Here it is:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c">#!/usr/bin/env bash</span>

<span class="nb">set</span> <span class="nt">-eu</span>

create_bmhs<span class="o">()</span> <span class="o">{</span>
  <span class="nv">n</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">1</span><span class="k">}</span><span class="s2">"</span>
  <span class="k">for</span> <span class="o">((</span> i <span class="o">=</span> 1<span class="p">;</span> i &lt;<span class="o">=</span> n<span class="p">;</span> ++i <span class="o">))</span><span class="p">;</span> <span class="k">do
    </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh">
---
apiVersion: v1
kind: Secret
metadata:
  name: worker-</span><span class="nv">$i</span><span class="sh">-bmc-secret
type: Opaque
data:
  username: YWRtaW4=
  password: cGFzc3dvcmQ=
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: worker-</span><span class="nv">$i</span><span class="sh">
spec:
  online: true
  bmc:
    address: libvirt://192.168.122.</span><span class="nv">$i</span><span class="sh">:6233/
    credentialsName: worker-</span><span class="nv">$i</span><span class="sh">-bmc-secret
  bootMACAddress: "</span><span class="si">$(</span><span class="nb">printf</span> <span class="s1">'00:60:2F:%02X:%02X:%02X\n'</span> <span class="k">$((</span>RANDOM%256<span class="k">))</span> <span class="k">$((</span>RANDOM%256<span class="k">))</span> <span class="k">$((</span>RANDOM%256<span class="k">))</span><span class="si">)</span><span class="sh">"
</span><span class="no">EOF
</span>  <span class="k">done</span>
<span class="o">}</span>

<span class="nv">NUM</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">1</span><span class="k">:-</span><span class="nv">10</span><span class="k">}</span><span class="s2">"</span>

create_bmhs <span class="s2">"</span><span class="k">${</span><span class="nv">NUM</span><span class="k">}</span><span class="s2">"</span>
</code></pre></div></div>

<p>Save it as <code class="language-plaintext highlighter-rouge">produce-available-hosts.sh</code> and try it out:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">$</span><span class="w"> </span>./produce-available-hosts.sh 10 | kubectl apply <span class="nt">-f</span> -
<span class="go">secret/worker-1-bmc-secret created
baremetalhost.metal3.io/worker-1 created
secret/worker-2-bmc-secret created
baremetalhost.metal3.io/worker-2 created
secret/worker-3-bmc-secret created
baremetalhost.metal3.io/worker-3 created
secret/worker-4-bmc-secret created
baremetalhost.metal3.io/worker-4 created
secret/worker-5-bmc-secret created
baremetalhost.metal3.io/worker-5 created
secret/worker-6-bmc-secret created
baremetalhost.metal3.io/worker-6 created
secret/worker-7-bmc-secret created
baremetalhost.metal3.io/worker-7 created
secret/worker-8-bmc-secret created
baremetalhost.metal3.io/worker-8 created
secret/worker-9-bmc-secret created
baremetalhost.metal3.io/worker-9 created
secret/worker-10-bmc-secret created
baremetalhost.metal3.io/worker-10 created
</span><span class="gp">$</span><span class="w"> </span>kubectl get bmh
<span class="go">NAME        STATE         CONSUMER   ONLINE   ERROR   AGE
worker-1    registering              true             2s
worker-10   available                true             2s
worker-2    available                true             2s
worker-3    available                true             2s
worker-4    available                true             2s
worker-5    available                true             2s
worker-6    registering              true             2s
worker-7    available                true             2s
worker-8    available                true             2s
worker-9    available                true             2s
</span></code></pre></div></div>

<p>With this we conclude the first part of the scaling series.
In the next post, we will take a look at how to fake the other end of the stack: the workload cluster API.</p>]]></content><author><name>Lennart Jern</name></author><category term="metal3" /><category term="cluster API" /><category term="provider" /><category term="edge" /><summary type="html"><![CDATA[We want to ensure that Metal3 can scale to thousands of nodes and clusters. However, running tests with thousands of real servers is expensive and we don’t have access to any such large environment in the project. So instead we have been focusing on faking the hardware while trying to keep things as realistic as possible for the controllers. In this first part we will take a look at the Bare Metal Operator and the test mode it offers. The next part will be about how to fake the Kubernetes API of the workload clusters. In the final post we will take a look at the issues we ran into and what is being done in the community to address them so that we can keep scaling!]]></summary></entry><entry><title type="html">One cluster - multiple providers</title><link href="https://metal3.io/blog/2022/07/08/One_cluster_multiple_providers.html" rel="alternate" type="text/html" title="One cluster - multiple providers" /><published>2022-07-08T00:00:00-05:00</published><updated>2022-07-08T00:00:00-05:00</updated><id>https://metal3.io/blog/2022/07/08/One_cluster_multiple_providers</id><content type="html" xml:base="https://metal3.io/blog/2022/07/08/One_cluster_multiple_providers.html"><![CDATA[<p>Running on bare metal has both benefits and drawbacks. You can get the
best performance possible out of the hardware, but it can also be quite
expensive and maybe not necessary for <em>all</em> workloads. Perhaps a hybrid
cluster could give you the best of both? Raw power for the workload that
needs it, and cheap virtualized commodity for the rest. This blog post
will show how to set up a cluster like this using the Cluster API backed
by the Metal3 and BYOH providers.</p>

<h2 id="the-problem">The problem</h2>

<p>Imagine that you have some bare metal servers that you want to use for
some specific workload. Maybe the workload benefits from the specific
hardware or there are some requirements that make it necessary to run it
there. The rest of the organization already uses Kubernetes and the
cluster API everywhere so of course you want the same for this as well.
Perfect, grab Metal³ and start working!</p>

<p>But hold on, this would mean that you use some of the servers for
running the Kubernetes control plane and possibly all the cluster API
controllers. If there are enough servers this is probably not an issue,
but do you really want to “waste” these servers on such generic
workloads that could be running anywhere? This can become especially
painful if you need multiple control plane nodes. Each server is
probably powerful enough to run all the control planes and controllers,
but it would be a single point of failure…</p>

<p>What if there was a way to use a different cluster API infrastructure
provider for some nodes? For example, use the Openstack infrastructure
provider for the control plane and Metal³ for the workers. Let’s do an
experiment!</p>

<h2 id="setting-up-the-experiment-environment">Setting up the experiment environment</h2>

<p>This blog post will use the <a href="https://github.com/vmware-tanzu/cluster-api-provider-bringyourownhost">Bring your own
host</a>
(BYOH) provider together with Metal³ as a proof of concept to show what
is currently possible.</p>

<p>The BYOH provider was chosen as the second provider for two reasons:</p>

<ol>
  <li>Due to its design (you provision the host yourself), it is very easy
to adapt it to the test (e.g. use a VM in the same network that the
metal3-dev-env uses).</li>
  <li>It is one of the providers that is known to work when combining
multiple providers for a single cluster.</li>
</ol>

<p>We will be using the
<a href="https://github.com/metal3-io/metal3-dev-env">metal3-dev-env</a> on Ubuntu
as a starting point for this experiment. Note that it makes substantial
changes to the machine where it is running, so you may want to use a
dedicated lab machine instead of your laptop for this. If you have not
done so already, clone it and run <code class="language-plaintext highlighter-rouge">make</code>. This should give you a
management cluster with the Metal³ provider installed and two
BareMetalHosts ready for provisioning.</p>

<p>The next step is to add the BYOH provider and a ByoHost.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>clusterctl init <span class="nt">--infrastructure</span> byoh
</code></pre></div></div>

<p>For the ByoHost we will use Vagrant.
You can install it with <code class="language-plaintext highlighter-rouge">sudo apt install vagrant</code>.
Then copy the Vagrantfile below to a new folder and run <code class="language-plaintext highlighter-rouge">vagrant up</code>.</p>

<pre><code class="language-Vagrantfile"># -*- mode: ruby -*-
hosts = {
    "control-plane1" =&gt; { "memory" =&gt; 2048, "ip" =&gt; "192.168.10.10"},
    # "control-plane2" =&gt; { "memory" =&gt; 2048, "ip" =&gt; "192.168.10.11"},
    # "control-plane3" =&gt; { "memory" =&gt; 2048, "ip" =&gt; "192.168.10.12"},
}


Vagrant.configure("2") do |config|
    # Choose which box you want below
    config.vm.box = "generic/ubuntu2004"
    config.vm.synced_folder ".", "/vagrant", disabled: true
    config.vm.provider :libvirt do |libvirt|
      # QEMU system connection is required for private network configuration
      libvirt.qemu_use_session = false
    end


    # Loop over all machine names
    hosts.each_key do |host|
        config.vm.define host, primary: host == hosts.keys.first do |node|
            node.vm.hostname = host
            node.vm.network :private_network, ip: hosts[host]["ip"],
              libvirt__forward_mode: "route"
            node.vm.provider :libvirt do |lv|
                lv.memory = hosts[host]["memory"]
                lv.cpus = 2
            end
        end
    end
end
</code></pre>

<p>Vagrant should now have created a new VM to use as a ByoHost. Now we
just need to run the BYOH agent in the VM to make it register as a
ByoHost in the management cluster. The BYOH agent needs a kubeconfig
file to do this, so we start by copying it to the VM:</p>

<!-- markdownlint-disable MD013 -->

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nb">cp</span> ~/.kube/config ~/.kube/management-cluster.conf
<span class="c"># Ensure that the correct IP is used (not localhost)</span>
<span class="nb">export </span><span class="nv">KIND_IP</span><span class="o">=</span><span class="si">$(</span>docker inspect <span class="nt">-f</span> <span class="s1">'{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'</span> kind-control-plane<span class="si">)</span>
<span class="nb">sed</span> <span class="nt">-i</span> <span class="s1">'s/    server\:.*/    server\: https\:\/\/'</span><span class="s2">"</span><span class="nv">$KIND_IP</span><span class="s2">"</span><span class="s1">'\:6443/g'</span> ~/.kube/management-cluster.conf
scp <span class="nt">-i</span> .vagrant/machines/control-plane1/libvirt/private_key <span class="se">\</span>
  /home/ubuntu/.kube/management-cluster.conf vagrant@192.168.10.10:management-cluster.conf

</code></pre></div></div>

<!-- markdownlint-enable MD013 -->

<p>Next, install the prerequisites and host agent in the VM and run it.</p>

<!-- markdownlint-enable MD013 -->

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>vagrant ssh
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> socat ebtables ethtool conntrack
wget https://github.com/vmware-tanzu/cluster-api-provider-bringyourownhost/releases/download/v0.2.0/byoh-hostagent-linux-amd64
<span class="nb">mv </span>byoh-hostagent-linux-amd64 byoh-hostagent
<span class="nb">chmod</span> +x byoh-hostagent
<span class="nb">sudo</span> ./byoh-hostagent <span class="nt">--namespace</span> metal3 <span class="nt">--kubeconfig</span> management-cluster.conf
</code></pre></div></div>

<!-- markdownlint-disable MD013 -->

<p>You should now have a management cluster with both the Metal³ and BYOH
providers installed, as well as two BareMetalHosts and one ByoHost.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">$</span><span class="w"> </span>kubectl <span class="nt">-n</span> metal3 get baremetalhosts,byohosts
<span class="go">NAME                             STATE       CONSUMER   ONLINE   ERROR   AGE
baremetalhost.metal3.io/node-0   available              true             18m
baremetalhost.metal3.io/node-1   available              true             18m


NAME                                                     AGE
byohost.infrastructure.cluster.x-k8s.io/control-plane1   73s
</span></code></pre></div></div>

<h2 id="creating-a-multi-provider-cluster">Creating a multi-provider cluster</h2>

<p>The trick is to create both a Metal3Cluster and a ByoCluster that are
owned by one common Cluster. We will use the ByoCluster for the control
plane in this case. First the Cluster:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">cluster.x-k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Cluster</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">cni</span><span class="pi">:</span> <span class="s">mixed-cluster-crs-0</span>
    <span class="na">crs</span><span class="pi">:</span> <span class="s2">"</span><span class="s">true"</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">clusterNetwork</span><span class="pi">:</span>
    <span class="na">pods</span><span class="pi">:</span>
      <span class="na">cidrBlocks</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">192.168.0.0/16</span>
    <span class="na">serviceDomain</span><span class="pi">:</span> <span class="s">cluster.local</span>
    <span class="na">services</span><span class="pi">:</span>
      <span class="na">cidrBlocks</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">10.128.0.0/12</span>
  <span class="na">controlPlaneRef</span><span class="pi">:</span>
    <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">controlplane.cluster.x-k8s.io/v1beta1</span>
    <span class="na">kind</span><span class="pi">:</span> <span class="s">KubeadmControlPlane</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">mixed-cluster-control-plane</span>
  <span class="na">infrastructureRef</span><span class="pi">:</span>
    <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1beta1</span>
    <span class="na">kind</span><span class="pi">:</span> <span class="s">ByoCluster</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
</code></pre></div></div>

<p>Add the rest of the BYOH manifests to get a control plane.
The code is collapsed here for easier reading.
Please click on the line below to expand it.</p>

<!-- markdownlint-disable MD033 -->

<details>
  <summary>KubeadmControlPlane, ByoCluster and ByoMachineTemplate</summary>
  <!-- Enable markdown parsing of the content. -->
  <div>

    <!-- markdownlint-enable MD033 -->

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">controlplane.cluster.x-k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">KubeadmControlPlane</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">nodepool</span><span class="pi">:</span> <span class="s">pool0</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mixed-cluster-control-plane</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">kubeadmConfigSpec</span><span class="pi">:</span>
    <span class="na">clusterConfiguration</span><span class="pi">:</span>
      <span class="na">apiServer</span><span class="pi">:</span>
        <span class="na">certSANs</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="s">localhost</span>
          <span class="pi">-</span> <span class="s">127.0.0.1</span>
          <span class="pi">-</span> <span class="s">0.0.0.0</span>
          <span class="pi">-</span> <span class="s">host.docker.internal</span>
      <span class="na">controllerManager</span><span class="pi">:</span>
        <span class="na">extraArgs</span><span class="pi">:</span>
          <span class="na">enable-hostpath-provisioner</span><span class="pi">:</span> <span class="s2">"</span><span class="s">true"</span>
    <span class="na">files</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">content</span><span class="pi">:</span> <span class="pi">|</span>
          <span class="s">apiVersion: v1</span>
          <span class="s">kind: Pod</span>
          <span class="s">metadata:</span>
            <span class="s">creationTimestamp: null</span>
            <span class="s">name: kube-vip</span>
            <span class="s">namespace: kube-system</span>
          <span class="s">spec:</span>
            <span class="s">containers:</span>
            <span class="s">- args:</span>
              <span class="s">- start</span>
              <span class="s">env:</span>
              <span class="s">- name: vip_arp</span>
                <span class="s">value: "true"</span>
              <span class="s">- name: vip_leaderelection</span>
                <span class="s">value: "true"</span>
              <span class="s">- name: vip_address</span>
                <span class="s">value: 192.168.10.20</span>
              <span class="s">- name: vip_interface</span>
                <span class="s">value: {{ .DefaultNetworkInterfaceName }}</span>
              <span class="s">- name: vip_leaseduration</span>
                <span class="s">value: "15"</span>
              <span class="s">- name: vip_renewdeadline</span>
                <span class="s">value: "10"</span>
              <span class="s">- name: vip_retryperiod</span>
                <span class="s">value: "2"</span>
              <span class="s">image: ghcr.io/kube-vip/kube-vip:v0.3.5</span>
              <span class="s">imagePullPolicy: IfNotPresent</span>
              <span class="s">name: kube-vip</span>
              <span class="s">resources: {}</span>
              <span class="s">securityContext:</span>
                <span class="s">capabilities:</span>
                  <span class="s">add:</span>
                  <span class="s">- NET_ADMIN</span>
                  <span class="s">- SYS_TIME</span>
              <span class="s">volumeMounts:</span>
              <span class="s">- mountPath: /etc/kubernetes/admin.conf</span>
                <span class="s">name: kubeconfig</span>
            <span class="s">hostNetwork: true</span>
            <span class="s">volumes:</span>
            <span class="s">- hostPath:</span>
                <span class="s">path: /etc/kubernetes/admin.conf</span>
                <span class="s">type: FileOrCreate</span>
              <span class="s">name: kubeconfig</span>
          <span class="s">status: {}</span>
        <span class="na">owner</span><span class="pi">:</span> <span class="s">root:root</span>
        <span class="na">path</span><span class="pi">:</span> <span class="s">/etc/kubernetes/manifests/kube-vip.yaml</span>
    <span class="na">initConfiguration</span><span class="pi">:</span>
      <span class="na">nodeRegistration</span><span class="pi">:</span>
        <span class="na">criSocket</span><span class="pi">:</span> <span class="s">/var/run/containerd/containerd.sock</span>
        <span class="na">ignorePreflightErrors</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="s">Swap</span>
          <span class="pi">-</span> <span class="s">DirAvailable--etc-kubernetes-manifests</span>
          <span class="pi">-</span> <span class="s">FileAvailable--etc-kubernetes-kubelet.conf</span>
    <span class="na">joinConfiguration</span><span class="pi">:</span>
      <span class="na">nodeRegistration</span><span class="pi">:</span>
        <span class="na">criSocket</span><span class="pi">:</span> <span class="s">/var/run/containerd/containerd.sock</span>
        <span class="na">ignorePreflightErrors</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="s">Swap</span>
          <span class="pi">-</span> <span class="s">DirAvailable--etc-kubernetes-manifests</span>
          <span class="pi">-</span> <span class="s">FileAvailable--etc-kubernetes-kubelet.conf</span>
  <span class="na">machineTemplate</span><span class="pi">:</span>
    <span class="na">infrastructureRef</span><span class="pi">:</span>
      <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1beta1</span>
      <span class="na">kind</span><span class="pi">:</span> <span class="s">ByoMachineTemplate</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">mixed-cluster-control-plane</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">version</span><span class="pi">:</span> <span class="s">v1.23.5</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ByoCluster</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">bundleLookupBaseRegistry</span><span class="pi">:</span> <span class="s">projects.registry.vmware.com/cluster_api_provider_bringyourownhost</span>
  <span class="na">bundleLookupTag</span><span class="pi">:</span> <span class="s">v1.23.5</span>
  <span class="na">controlPlaneEndpoint</span><span class="pi">:</span>
    <span class="na">host</span><span class="pi">:</span> <span class="s">192.168.10.20</span>
    <span class="na">port</span><span class="pi">:</span> <span class="m">6443</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ByoMachineTemplate</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mixed-cluster-control-plane</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span> <span class="pi">{}</span>

</code></pre></div>    </div>

  </div>
</details>

<p>So far this is a “normal” Cluster backed by the BYOH provider. But now
it is time to do something different. Instead of adding more ByoHosts as
workers, we will add a Metal3Cluster and MachineDeployment backed by
BareMetalHosts! Note that the <code class="language-plaintext highlighter-rouge">controlPlaneEndpoint</code> of the
Metal3Cluster must point to the same endpoint that the ByoCluster is
using.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Metal3Cluster</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">controlPlaneEndpoint</span><span class="pi">:</span>
    <span class="na">host</span><span class="pi">:</span> <span class="s">192.168.10.20</span>
    <span class="na">port</span><span class="pi">:</span> <span class="m">6443</span>
  <span class="na">noCloudProvider</span><span class="pi">:</span> <span class="no">true</span>
</code></pre></div></div>

<!-- markdownlint-disable MD033 -->

<details>
  <summary>IPPools</summary>
  <div>

    <!-- markdownlint-enable MD033 -->

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">ipam.metal3.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">IPPool</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">provisioning-pool</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">clusterName</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
  <span class="na">namePrefix</span><span class="pi">:</span> <span class="s">test1-prov</span>
  <span class="na">pools</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">end</span><span class="pi">:</span> <span class="s">172.22.0.200</span>
      <span class="na">start</span><span class="pi">:</span> <span class="s">172.22.0.100</span>
  <span class="na">prefix</span><span class="pi">:</span> <span class="m">24</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">ipam.metal3.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">IPPool</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">baremetalv4-pool</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">clusterName</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
  <span class="na">gateway</span><span class="pi">:</span> <span class="s">192.168.111.1</span>
  <span class="na">namePrefix</span><span class="pi">:</span> <span class="s">test1-bmv4</span>
  <span class="na">pools</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">end</span><span class="pi">:</span> <span class="s">192.168.111.200</span>
      <span class="na">start</span><span class="pi">:</span> <span class="s">192.168.111.100</span>
  <span class="na">prefix</span><span class="pi">:</span> <span class="m">24</span>
</code></pre></div>    </div>

  </div>
</details>

<p>These manifests are quite large but they are just the same as would be
used by the metal3-dev-env with some name changes here and there. The
key thing to note is that all references to a Cluster are to the one we
defined above. Here is the MachineDeployment:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">cluster.x-k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">MachineDeployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">cluster.x-k8s.io/cluster-name</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
    <span class="na">nodepool</span><span class="pi">:</span> <span class="s">nodepool-0</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">test1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">clusterName</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">cluster.x-k8s.io/cluster-name</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
      <span class="na">nodepool</span><span class="pi">:</span> <span class="s">nodepool-0</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">cluster.x-k8s.io/cluster-name</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
        <span class="na">nodepool</span><span class="pi">:</span> <span class="s">nodepool-0</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">bootstrap</span><span class="pi">:</span>
        <span class="na">configRef</span><span class="pi">:</span>
          <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">bootstrap.cluster.x-k8s.io/v1beta1</span>
          <span class="na">kind</span><span class="pi">:</span> <span class="s">KubeadmConfigTemplate</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">test1-workers</span>
      <span class="na">clusterName</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
      <span class="na">infrastructureRef</span><span class="pi">:</span>
        <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1beta1</span>
        <span class="na">kind</span><span class="pi">:</span> <span class="s">Metal3MachineTemplate</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">test1-workers</span>
      <span class="na">nodeDrainTimeout</span><span class="pi">:</span> <span class="s">0s</span>
      <span class="na">version</span><span class="pi">:</span> <span class="s">v1.23.5</span>
</code></pre></div></div>

<p>Finally, we add the Metal3MachineTemplate, Metal3DataTemplate and
KubeadmConfigTemplate. Here you may want to add your public ssh key in
the KubeadmConfigTemplate (the last few lines).</p>

<!-- markdownlint-disable MD033 -->

<details>
  <summary>Metal3MachineTemplate, Metal3DataTemplate and KubeadmConfigTemplate</summary>
  <!-- Enable markdown parsing of the content. -->
  <div>

    <!-- markdownlint-enable MD033 -->

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Metal3MachineTemplate</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">test1-workers</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">dataTemplate</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">test1-workers-template</span>
      <span class="na">image</span><span class="pi">:</span>
        <span class="na">checksum</span><span class="pi">:</span> <span class="s">http://172.22.0.1/images/UBUNTU_22.04_NODE_IMAGE_K8S_v1.23.5-raw.img.md5sum</span>
        <span class="na">checksumType</span><span class="pi">:</span> <span class="s">md5</span>
        <span class="na">format</span><span class="pi">:</span> <span class="s">raw</span>
        <span class="na">url</span><span class="pi">:</span> <span class="s">http://172.22.0.1/images/UBUNTU_22.04_NODE_IMAGE_K8S_v1.23.5-raw.img</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Metal3DataTemplate</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">test1-workers-template</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">metal3</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">clusterName</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
  <span class="na">metaData</span><span class="pi">:</span>
    <span class="na">ipAddressesFromIPPool</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">provisioningIP</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">provisioning-pool</span>
    <span class="na">objectNames</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">name</span>
        <span class="na">object</span><span class="pi">:</span> <span class="s">machine</span>
      <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">local-hostname</span>
        <span class="na">object</span><span class="pi">:</span> <span class="s">machine</span>
      <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">local_hostname</span>
        <span class="na">object</span><span class="pi">:</span> <span class="s">machine</span>
    <span class="na">prefixesFromIPPool</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">provisioningCIDR</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">provisioning-pool</span>
  <span class="na">networkData</span><span class="pi">:</span>
    <span class="na">links</span><span class="pi">:</span>
      <span class="na">ethernets</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">id</span><span class="pi">:</span> <span class="s">enp1s0</span>
          <span class="na">macAddress</span><span class="pi">:</span>
            <span class="na">fromHostInterface</span><span class="pi">:</span> <span class="s">enp1s0</span>
          <span class="na">type</span><span class="pi">:</span> <span class="s">phy</span>
        <span class="pi">-</span> <span class="na">id</span><span class="pi">:</span> <span class="s">enp2s0</span>
          <span class="na">macAddress</span><span class="pi">:</span>
            <span class="na">fromHostInterface</span><span class="pi">:</span> <span class="s">enp2s0</span>
          <span class="na">type</span><span class="pi">:</span> <span class="s">phy</span>
    <span class="na">networks</span><span class="pi">:</span>
      <span class="na">ipv4</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">id</span><span class="pi">:</span> <span class="s">baremetalv4</span>
          <span class="na">ipAddressFromIPPool</span><span class="pi">:</span> <span class="s">baremetalv4-pool</span>
          <span class="na">link</span><span class="pi">:</span> <span class="s">enp2s0</span>
          <span class="na">routes</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">gateway</span><span class="pi">:</span>
                <span class="na">fromIPPool</span><span class="pi">:</span> <span class="s">baremetalv4-pool</span>
              <span class="na">network</span><span class="pi">:</span> <span class="s">0.0.0.0</span>
              <span class="na">prefix</span><span class="pi">:</span> <span class="m">0</span>
    <span class="na">services</span><span class="pi">:</span>
      <span class="na">dns</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">8.8.8.8</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">bootstrap.cluster.x-k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">KubeadmConfigTemplate</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">test1-workers</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">files</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">content</span><span class="pi">:</span> <span class="pi">|</span>
            <span class="s">network:</span>
              <span class="s">version: 2</span>
              <span class="s">renderer: networkd</span>
              <span class="s">bridges:</span>
                <span class="s">ironicendpoint:</span>
                  <span class="s">interfaces: [enp1s0]</span>
                  <span class="s">addresses:</span>
                  <span class="s">- {{ ds.meta_data.provisioningIP }}/{{ ds.meta_data.provisioningCIDR }}</span>
          <span class="na">owner</span><span class="pi">:</span> <span class="s">root:root</span>
          <span class="na">path</span><span class="pi">:</span> <span class="s">/etc/netplan/52-ironicendpoint.yaml</span>
          <span class="na">permissions</span><span class="pi">:</span> <span class="s2">"</span><span class="s">0644"</span>
        <span class="pi">-</span> <span class="na">content</span><span class="pi">:</span> <span class="pi">|</span>
            <span class="s">[registries.search]</span>
            <span class="s">registries = ['docker.io']</span>


            <span class="s">[registries.insecure]</span>
            <span class="s">registries = ['192.168.111.1:5000']</span>
          <span class="na">path</span><span class="pi">:</span> <span class="s">/etc/containers/registries.conf</span>
      <span class="na">joinConfiguration</span><span class="pi">:</span>
        <span class="na">nodeRegistration</span><span class="pi">:</span>
          <span class="na">kubeletExtraArgs</span><span class="pi">:</span>
            <span class="na">cgroup-driver</span><span class="pi">:</span> <span class="s">systemd</span>
            <span class="na">container-runtime</span><span class="pi">:</span> <span class="s">remote</span>
            <span class="na">container-runtime-endpoint</span><span class="pi">:</span> <span class="s">unix:///var/run/crio/crio.sock</span>
            <span class="na">feature-gates</span><span class="pi">:</span> <span class="s">AllAlpha=false</span>
            <span class="na">node-labels</span><span class="pi">:</span> <span class="s">metal3.io/uuid={{ ds.meta_data.uuid }}</span>
            <span class="na">provider-id</span><span class="pi">:</span> <span class="s">metal3://{{ ds.meta_data.uuid }}</span>
            <span class="na">runtime-request-timeout</span><span class="pi">:</span> <span class="s">5m</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s2">"</span><span class="s">{{</span><span class="nv"> </span><span class="s">ds.meta_data.name</span><span class="nv"> </span><span class="s">}}"</span>
      <span class="na">preKubeadmCommands</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">netplan apply</span>
        <span class="pi">-</span> <span class="s">systemctl enable --now crio kubelet</span>
      <span class="na">users</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">metal3</span>
          <span class="c1"># sshAuthorizedKeys:</span>
          <span class="c1"># - add your public key here for debugging</span>
          <span class="na">sudo</span><span class="pi">:</span> <span class="s">ALL=(ALL) NOPASSWD:ALL</span>

</code></pre></div>    </div>

  </div>
</details>

<p>The result of all this is a Cluster with two Machines, one from the
Metal³ provider and one from the BYOH provider.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">$</span><span class="w"> </span>k <span class="nt">-n</span> metal3 get machine
<span class="go">NAME                                CLUSTER         NODENAME                PROVIDERID                                      PHASE     AGE     VERSION
mixed-cluster-control-plane-48qmm   mixed-cluster   control-plane1          byoh://control-plane1/jf5uye                    Running   7m41s   v1.23.5
test1-8767dbccd-24cl5               mixed-cluster   test1-8767dbccd-24cl5   metal3://0642d832-3a7c-4ce9-833e-a629a60a455c   Running   7m18s   v1.23.5
</span></code></pre></div></div>

<p>Let’s also check that the workload cluster is functioning as expected.
Get the kubeconfig and add Calico as CNI.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>clusterctl get kubeconfig <span class="nt">-n</span> metal3 mixed-cluster <span class="o">&gt;</span> kubeconfig.yaml
<span class="nb">export </span><span class="nv">KUBECONFIG</span><span class="o">=</span>kubeconfig.yaml
kubectl apply <span class="nt">-f</span> https://docs.projectcalico.org/v3.20/manifests/calico.yaml
</code></pre></div></div>

<p>Now check the nodes.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">$</span><span class="w"> </span>kubectl get nodes
<span class="go">NAME                    STATUS   ROLES                  AGE   VERSION
control-plane1          Ready    control-plane,master   88m   v1.23.5
</span><span class="gp">test1-8767dbccd-24cl5   Ready    &lt;none&gt;</span><span class="w">                 </span>82m   v1.23.5
</code></pre></div></div>

<p>Going back to the management cluster, we can inspect the state of the
cluster API resources.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">$</span><span class="w"> </span>clusterctl <span class="nt">-n</span> metal3 describe cluster mixed-cluster
<span class="go">NAME                                                                        READY  SEVERITY  REASON  SINCE  MESSAGE
Cluster/mixed-cluster                                                       True                     13m
├─ClusterInfrastructure - ByoCluster/mixed-cluster
├─ControlPlane - KubeadmControlPlane/mixed-cluster-control-plane            True                     13m
│ └─Machine/mixed-cluster-control-plane-hp2fp                               True                     13m
│   └─MachineInfrastructure - ByoMachine/mixed-cluster-control-plane-vxft5
└─Workers
  └─MachineDeployment/test1                                                 True                     3m57s
    └─Machine/test1-7f77dfb7c8-j7x4q                                        True                     9m32s
</span></code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>

<p>As we have seen in this post, it is possible to combine at least some
infrastructure providers when creating a single cluster. This can be
useful for example if a provider has a high cost or limited resources.
Furthermore, the use case is not addressed by MachineDeployments since
they would all be from the same provider (even though they can have
different properties).</p>

<p>There is some room for development and improvement though. The most
obvious thing is perhaps that Clusters only have one
<code class="language-plaintext highlighter-rouge">infrastructureRef</code>. This means that the cluster API controllers are not
aware of the “secondary” infrastructure provider(s).</p>

<p>Another thing that may be less obvious is the reliance on Nodes and
Machines in the Kubeadm control plane provider. It is not an issue in
the example we have seen here since both Metal³ and BYOH creates Nodes.
However, there are some projects where Nodes are unnecessary. See for
example <a href="https://github.com/clastix/kamaji">Kamaji</a>, which aims to
integrate with the cluster API. The idea here is to run the control
plane components in the management cluster as Pods. Naturally, there
would not be any control plane Nodes or Machines in this case. (A second
provider would be used to add workers.) But the Kubeadm control plane
provider expects there to be both Machines and Nodes for the control
plane, so a new provider is likely needed to make this work as desired.</p>

<p>This issue can already be seen in the
<a href="https://github.com/loft-sh/cluster-api-provider-vcluster">vcluster</a>
provider, where the Cluster stays in <code class="language-plaintext highlighter-rouge">Provisioning</code> state because it is
“Waiting for the first control plane machine to have its
<code class="language-plaintext highlighter-rouge">status.nodeRef</code> set”. The idea with vcluster is to reuse the Nodes of
the management cluster but provide a separate control plane. This gives
users better isolation than just namespaces without the need for another
“real” cluster. It is for example possible to have different custom
resource definitions in each vcluster. But since vcluster runs all the
pods (including the control plane) in the management cluster, there will
never be a control plane Machine or <code class="language-plaintext highlighter-rouge">nodeRef</code>.</p>

<p>There is already one implementation of a control plane provider without
Nodes, i.e. the EKS provider. Perhaps this is the way forward. One
implementation for each specific case. It would be nice if it was
possible to do it in a more generic way though, similar to how the
Kubeadm control plane provider is used by almost all infrastructure
providers.</p>

<p>To summarize, there is already some support for mixed clusters with
multiple providers. However, there are some issues that make it
unnecessarily awkward. Two things that could be improved in the cluster
API would be the following:</p>

<ol>
  <li>Make the <code class="language-plaintext highlighter-rouge">cluster.infrastructureRef</code> into a list to allow multiple
infrastructure providers to be registered.</li>
  <li>Drop the assumption that there will always be control plane Machines
and Nodes (e.g. by implementing a new control plane provider).</li>
</ol>]]></content><author><name>Lennart Jern</name></author><category term="metal3" /><category term="cluster API" /><category term="provider" /><category term="hybrid" /><category term="edge" /><summary type="html"><![CDATA[Running on bare metal has both benefits and drawbacks. You can get the best performance possible out of the hardware, but it can also be quite expensive and maybe not necessary for all workloads. Perhaps a hybrid cluster could give you the best of both? Raw power for the workload that needs it, and cheap virtualized commodity for the rest. This blog post will show how to set up a cluster like this using the Cluster API backed by the Metal3 and BYOH providers.]]></summary></entry><entry><title type="html">Metal3 Introduces Pivoting</title><link href="https://metal3.io/blog/2021/05/05/Pivoting.html" rel="alternate" type="text/html" title="Metal3 Introduces Pivoting" /><published>2021-05-05T00:00:00-05:00</published><updated>2021-05-05T00:00:00-05:00</updated><id>https://metal3.io/blog/2021/05/05/Pivoting</id><content type="html" xml:base="https://metal3.io/blog/2021/05/05/Pivoting.html"><![CDATA[<p>Metal3 project has introduced pivoting in its CI workflow. The motivation for
pivoting is to move all the objects from the ephemeral/management
cluster to a target cluster. This blog post will briefly introduce the concept
of pivoting and the impact it has on the overall CI workflow. For the rest of
this blog, we refer ephemeral/management cluster as an ephemeral cluster.</p>

<h2 id="what-is-pivoting">What is Pivoting?</h2>

<p>In the context of Metal3 Provider, Pivoting is the process of moving
Cluster-API and Metal3 objects from the ephemeral k8s cluster to a target
cluster. In Metal3, this process is performed using the
<a href="https://cluster-api.sigs.k8s.io/clusterctl/overview.html">clusterctl</a> tool
provided by Cluster-API. clusterctl recognizes pivoting as a move. During the
pivot process, clusterctl pauses any reconciliation of Cluster-API objects and
this gets propagated to Cluster-api-provider-metal3 (CAPM3) objects as well.
Once all the objects are paused, the objects are created on the other side on
the target cluster and deleted from the ephemeral cluster.</p>

<h2 id="prerequisites">Prerequisites</h2>

<p>Prior to the actual pivot process, the target cluster should already have the
provider components, ironic containers and CNI installed and running. To perform
pivot outside metal3-dev-env, specifically, the following points need to be
addressed:</p>

<ul>
  <li>clusterctl is used to initialize both the ephemeral and target cluster.</li>
  <li>BMH objects have correct status annotation.</li>
  <li>Maintain connectivity towards the provisioning network.</li>
  <li>Baremetal Operator(BMO) is deployed as part of CAPM3.</li>
  <li>Objects should have a proper owner reference chain.</li>
</ul>

<p>For a detailed explanation of the above-mentioned prerequisites please read the
<a href="https://book.metal3.io/capm3/pivoting">pivoting documentation</a>.</p>

<h2 id="pivoting-workflow-in-ci">Pivoting workflow in CI</h2>

<p>The Metal3 CI currently includes pivoting as part of the deployment
process both for Ubuntu and CentOS-based jobs. This essentially means all
the PRs that go in, are tested through the pivoting workflow. Here is the
CI deployment workflow:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">make</code> the <a href="https://github.com/metal3-io/metal3-dev-env.git">metal3-dev-env</a>.
It gives us the ephemeral cluster with all the necessary controllers running
within it. The corresponding metal3-dev-env command is <code class="language-plaintext highlighter-rouge">make</code></li>
  <li><code class="language-plaintext highlighter-rouge">provision</code> target cluster. For normal integration tests, this step deploys
a control-plane node and a worker in the target cluster. For, <code class="language-plaintext highlighter-rouge">feature-test</code>
and <code class="language-plaintext highlighter-rouge">feature-test-upgrade</code> the provision step deploys three control-planes and
a worker. The corresponding metal3-dev-env commands are (normal integration
test workflow):</li>
</ul>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="syntax"><code>./scripts/provision/cluster.sh
./scripts/provision/controlplane.sh
./scripts/provision/worker.sh
</code></pre></div></div>

<ul>
  <li>Initialize the provider components on the target cluster. This installs all
the controllers and associated components related to cluster-api ,
cluster-api-provider-metal3, baremetal-operator and ironic. Since it is
necessary to have only one set of ironic deployment/containers in the picture,
this step also deletes the ironic deployment/containers from
ephemeral cluster.</li>
  <li><code class="language-plaintext highlighter-rouge">Move</code> all the objects from ephemeral to the target cluster.</li>
  <li>Check the status of the objects to verify whether the objects are being
reconciled correctly by the controllers in the target cluster. This step
verifies and finalizes the pivoting process. The corresponding metal3-dev-env
the command that performs this and the previous two steps is :</li>
</ul>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="syntax"><code>./scripts/feature_tests/pivoting/pivot.sh
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Move</code> the objects back to the ephemeral cluster. This step also
removes the ironic deployment from the target cluster and reinstates the
ironic deployment/containers in the ephemeral cluster. Since we do
not delete the provider components in the ephemeral cluster,
installing them again is not necessary. The corresponding metal3-dev-env command
that performs this step is :</li>
</ul>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="syntax"><code>./scripts/feature_tests/pivoting/repivot.sh
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">De-provision</code> the BMHs and delete the target cluster. The corresponding
metal3-dev-env commands to de-provision worker, controlplane and the cluster
is as follows:</li>
</ul>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="syntax"><code>./scripts/deprovision/worker.sh
./scripts/deprovision/controlplane.sh
./scripts/deprovision/cluster.sh
</code></pre></div></div>

<p>Note that, if we de-provision cluster, that would de-provision worker and
controlplane automatically.</p>

<h2 id="pivoting-in-metal3">Pivoting in Metal3</h2>

<p>The pivoting process described above is realized in <code class="language-plaintext highlighter-rouge">ansible</code> scripts
<a href="https://github.com/metal3-io/metal3-dev-env/blob/main/tests/roles/run_tests/tasks/move.yml">move.yml</a>
and
<a href="https://github.com/metal3-io/metal3-dev-env/blob/main/tests/roles/run_tests/tasks/move_back.yml">move_back.yml</a>.
Under the hood, pivoting uses the <code class="language-plaintext highlighter-rouge">move</code> command from
<a href="https://cluster-api.sigs.k8s.io/clusterctl/commands/move.html">clusterctl</a>
provided by Cluster-API.</p>

<p>As stated earlier, all the PRs that go into any Metal3 repository where the
integration tests are run, the code change introduced in the PR is verified with
pivoting also in the integration tests now. Moreover, the upgrade workflow in
Metal3 performs all the upgrade operations in Metal3 after pivoting to the
target cluster.</p>]]></content><author><name>Kashif Nizam Khan</name></author><category term="metal3" /><category term="baremetal" /><category term="Pivoting" /><category term="Move" /><summary type="html"><![CDATA[Metal3 project has introduced pivoting in its CI workflow. The motivation for pivoting is to move all the objects from the ephemeral/management cluster to a target cluster. This blog post will briefly introduce the concept of pivoting and the impact it has on the overall CI workflow. For the rest of this blog, we refer ephemeral/management cluster as an ephemeral cluster.]]></summary></entry></feed>